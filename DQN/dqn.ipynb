{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Logo\"](https://github.com/Fortuz/rl_education/blob/main/assets/logo.png?raw=1)\n",
    "\n",
    "Created by **Zoltán Barta**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/10.%20Off-policy%20Control/dqn.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks (DQN) and Its Variants\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement Learning (RL) has seen significant advancements with the introduction of Deep Q-Networks (DQN). Originally proposed by **Mnih et al. (2013)**, DQN combines deep learning with Q-learning to approximate optimal action-value functions in complex environments. This approach has been instrumental in enabling agents to play Atari games at superhuman levels.\n",
    "\n",
    "While DQN has demonstrated impressive performance, it suffers from instability and overestimation issues. To address these challenges, several improved versions have been proposed:\n",
    "\n",
    "- **Double DQN**: Mitigates overestimation bias in Q-learning by using a separate target network for action selection and evaluation.\n",
    "- **Dueling DQN**: Introduces a novel architecture that separately estimates state-value and advantage functions to improve learning efficiency.\n",
    "- **Prioritized Experience Replay (PER)**: Enhances sampling efficiency by giving more importance to valuable experiences.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook explores the implementation and evaluation of these DQN variants. Specifically, it aims to:\n",
    "\n",
    "- Implement a baseline **DQN** agent.\n",
    "- Extend it with **Double DQN** to reduce overestimation.\n",
    "- Incorporate **Dueling DQN** for better action-value estimation.\n",
    "- Evaluate enhancements like **PER**\n",
    "\n",
    "The experiments will be conducted using **Gymnasium**'s CartPole-v1 environment, and the models will be implemented using **PyTorch**.\n",
    "\n",
    "## References\n",
    "\n",
    "- [DQN Paper (2013)](https://arxiv.org/abs/1312.5602)\n",
    "- [Double DQN (2015)](https://arxiv.org/abs/1509.06461)\n",
    "- [Dueling DQN (2015)](https://arxiv.org/abs/1511.06581)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Homework 3**\n",
    "In this assignment, you will implement **Dueling DQN**, an improved version of the Deep Q-Network (DQN) that separates **state-value estimation** from **action advantage estimation**. Dueling DQN is particularly useful in environments where some actions are redundant in certain states.\n",
    "\n",
    "Your task is to:\n",
    "- Implement the **forward pass** of the dueling architecture.  \n",
    "- Train and evaluate the agent on an **Atari** or **Gym environment**.  \n",
    "- For details, check Cell #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py) \n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In reinforcement learning, an agent interacts with an environment by taking actions and receiving rewards. However, learning directly from consecutive experiences can lead to inefficiencies and instability. **Experience Replay** is a crucial technique introduced to improve sample efficiency and stability in **Deep Q-Networks (DQN)**.\n",
    "\n",
    "Originally proposed by **Mnih et al. (2013)**, Experience Replay involves storing past transitions in a memory buffer and randomly sampling batches of experiences for training. This helps break the correlation between consecutive observations, leading to more stable learning.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The core idea behind Experience Replay is to maintain a **Replay Buffer** that stores transitions of the form:\n",
    "\n",
    "$$ (s_t, a_t, r_t, s_{t+1}) $$\n",
    "\n",
    "where:\n",
    "- $s_t$ is the current state,\n",
    "- $a_t$ is the action taken,\n",
    "- $r_t$ is the reward received,\n",
    "- $s_{t+1}$ is the next state.\n",
    "\n",
    "At each training step:\n",
    "1. The agent **samples** a mini-batch of transitions from the replay buffer.\n",
    "2. It updates the Q-network using these past experiences rather than relying only on the latest transition.\n",
    "3. This process **reduces correlation** between updates and leads to more efficient learning.\n",
    "\n",
    "### Benefits of Experience Replay\n",
    "\n",
    "- **Breaks Correlation:** Sampling from a replay buffer prevents the network from learning directly from consecutive states, reducing correlation in training updates.\n",
    "- **Improves Sample Efficiency:** By reusing past experiences, the agent learns more efficiently compared to using only the most recent transitions.\n",
    "- **Enables Off-Policy Learning:** Experience Replay allows learning from past experiences, making it possible to use older transitions even if the current policy has changed.\n",
    "- **Stabilizes Training:** Helps smooth out learning by reducing variance in updates.\n",
    "\n",
    "\n",
    "### Implementation in DQN\n",
    "\n",
    "A **ReplayMemory** class is typically used to store and manage the experience replay buffer. The key methods include:\n",
    "\n",
    "- **push()** – Stores a transition in the buffer.\n",
    "- **sample()** – Retrieves a random mini-batch of experiences for training.\n",
    "- **__len__()** – Returns the number of stored experiences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"        \n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **epsilon-greedy action selection** strategy balances **exploration** and **exploitation** by choosing the action with the highest estimated Q-value with probability **$1 - \\epsilon$** and selecting a random action with probability **$\\epsilon$**, where **$\\epsilon$** is often annealed over time to transition from exploration to exploitation. More on it in the [K-armed bandit notebook.](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/1.%20K-armed%20Bandit/k_armed_bandit_solution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyActionSelection():\n",
    "\n",
    "    def __init__(self, epsilon_start, epsilon_end, epsilon_decay,n_actions):\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.n_actions = n_actions\n",
    "    \n",
    "    def get_epsilon(self, step):\n",
    "        return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * math.exp(-1. * step / self.epsilon_decay)\n",
    "    \n",
    "    def select_action(self, policy_net, state, step):\n",
    "        sample = random.random()\n",
    "        epsilon = self.get_epsilon(step)\n",
    "        if sample > epsilon:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Policy\n",
    "\n",
    "### What is the MLP Policy?\n",
    "\n",
    "The **MLP (Multi-Layer Perceptron) Policy** is a **feedforward neural network** used as the function approximator in Deep Q-Networks (DQN). It maps **observations (state representations)** to **Q-values for each action**, enabling the agent to make decisions based on learned value estimates.\n",
    "\n",
    "### Why Are We Using It?\n",
    "\n",
    "We use an **MLP-based policy** because:\n",
    "- It can learn **non-linear representations** of the environment, making it suitable for complex state spaces.\n",
    "- It is computationally efficient and can be trained using **gradient-based optimization**.\n",
    "- It generalizes well across similar states, improving sample efficiency.\n",
    "\n",
    "### When Is It Worth Using?\n",
    "\n",
    "- **Tabular Q-learning becomes infeasible**: When the state space is large or continuous, using a neural network as a function approximator is necessary.\n",
    "- **Environments with structured features**: The MLP policy can extract meaningful patterns from structured state representations.\n",
    "- **Computational efficiency matters**: MLPs are simpler and faster to train compared to deeper or more complex architectures like CNNs or RNNs.\n",
    "\n",
    "In our implementation, the **MLP consists of three fully connected layers** with **ReLU activation**, ensuring non-linearity and effective gradient flow.\n",
    "\n",
    "## MLP Size for Atari Environments\n",
    "\n",
    "For solving **Atari** environments with Deep Q-Networks (DQN), using a simple **Multi-Layer Perceptron (MLP) policy** is generally **not sufficient** due to the high-dimensional **pixel-based** state representation. Instead, **Convolutional Neural Networks (CNNs)** are commonly used for feature extraction.\n",
    "\n",
    "However, if **low-dimensional features** (such as RAM states) are used instead of raw pixels, an **MLP policy** can still be effective. The required size depends on:\n",
    "\n",
    "1. **State Space Dimensionality**: \n",
    "   - If using raw pixels ($84 \\times 84 \\times 4$ stacked frames), a deep CNN is preferred.\n",
    "   - If using **RAM states** (128-dimensional), an **MLP with 2–3 hidden layers of 256–512 neurons** is generally effective.\n",
    "\n",
    "2. **Action Space Complexity**:\n",
    "   - More complex environments with **many discrete actions** might require larger networks to capture nuanced decision-making.\n",
    "\n",
    "3. **Computational Constraints**:\n",
    "   - A larger network improves representation learning but increases training time.\n",
    "   - Standard DQN implementations for **Atari** use **three convolutional layers followed by a fully connected MLP with 512 neurons**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpPolicy(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions,num_hidden=128):\n",
    "        super(MlpPolicy, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, num_hidden)\n",
    "        self.layer2 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.layer3 = nn.Linear(num_hidden, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling Policy\n",
    "\n",
    "### What is the Dueling Architecture?\n",
    "\n",
    "The **Dueling DQN** architecture is an improvement over standard Deep Q-Networks (DQN) that helps the agent **better differentiate** between valuable states and valuable actions. Introduced by **Wang et al. (2015)**, the **dueling network architecture** modifies the Q-value function by separately estimating:\n",
    "\n",
    "1. **State Value Function**: $V(s)$ measures how valuable a given state is, regardless of the action taken.\n",
    "2. **Advantage Function**: $A(s, a)$ captures the relative importance of each action within a given state.\n",
    "\n",
    "Instead of directly computing Q-values, the network learns:\n",
    "\n",
    "$$\n",
    "Q(s, a) = V(s) + A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a')\n",
    "$$\n",
    "\n",
    "where the subtraction ensures that the **advantage function is centered**, preventing redundant representations.\n",
    "\n",
    "### Why Use a Dueling Architecture?\n",
    "\n",
    "- **Better State Evaluation**: The network can learn to **evaluate states independently of actions**, which is useful when some actions have little impact.\n",
    "- **More Efficient Learning**: Helps **generalize learning** across actions, leading to faster convergence.\n",
    "- **Improved Stability**: Reduces unnecessary action-value variations in states where action choice doesn’t matter.\n",
    "\n",
    "### When Is It Useful?\n",
    "\n",
    "- **Large or Discrete Action Spaces**: The dueling structure helps **identify important states** without needing to compare all actions explicitly.\n",
    "- **Environments with Redundant Actions**: When **some actions have no effect** in certain states, the architecture ensures learning is focused on meaningful actions.\n",
    "- **Atari & Complex RL Tasks**: Many **Atari games** benefit from this architecture due to the complexity of state-action relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Homework**\n",
    "\n",
    "- **Create a shared feature extraction layer** for state representations.\n",
    "- **Defining two separate branches:**\n",
    "  1. **State-Value Stream** $V(s)$ – Computes how good a state is, independent of actions.\n",
    "  2. **Advantage Stream** $A(s, a)$ – Computes how much better or worse each action is relative to the state value.\n",
    "- **Merging these two streams using the dueling Q-value formula:**\n",
    "\n",
    "$$\n",
    "Q(s, a) = V(s) + \\left(A(s, a) - \\frac{1}{|A|} \\sum_{a{\\prime}} A(s, a{\\prime})\\right)\n",
    "$$\n",
    "\n",
    "![Dueling Architecture](Dueling_NN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingMlpPolicy(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling Network architecture for DQN.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_observations, n_actions,num_hidden=128):\n",
    "        super(DuelingMlpPolicy, self).__init__()\n",
    "        ###############CODE HERE################\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        ########################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###############CODE HERE################\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        ########################################\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent\n",
    "\n",
    "### What is the DQNAgent?\n",
    "\n",
    "The **DQNAgent** is the core component responsible for training and decision-making in a **Deep Q-Network (DQN)** setup. It integrates key reinforcement learning concepts, including **experience replay, target networks, and action selection**, to improve the stability and efficiency of learning.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Experience Replay**:\n",
    "   - Stores transitions in a **Replay Buffer** to break correlation between consecutive experiences.\n",
    "   - Uses **mini-batch sampling** for training to improve data efficiency.\n",
    "\n",
    "2. **Policy and Target Networks**:\n",
    "   - Maintains two networks:\n",
    "     - **Policy Network**: The main neural network used for action selection.\n",
    "     - **Target Network**: A delayed copy of the policy network, used for stabilizing training.\n",
    "   - The **target network is updated periodically** using a soft update mechanism controlled by **$\\tau$**.\n",
    "\n",
    "3. **Epsilon-Greedy Action Selection**:\n",
    "   - Uses an **action selection strategy** to balance exploration vs. exploitation.\n",
    "\n",
    "4. **Optimization Process**:\n",
    "   - Trains the network using **Smooth L1 Loss (Huber loss)** to handle noisy rewards.\n",
    "   - Uses the **Adam optimizer** for gradient-based learning.\n",
    "   - **Gradient clipping** prevents instability in training.\n",
    "\n",
    "5. **Target Network Hard Update**:\n",
    "   - Copy the parameters of policy net to the target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, n_observations, n_actions, action_selection,policy_net,memory_capacity=10000,\n",
    "                 optimization_iterations = 15 ,\n",
    "                 batch_size=128,\n",
    "                 target_update_interval=10, \n",
    "                 gamma=0.999,\n",
    "                 learning_rate=0.001):\n",
    "        self.n_observations = n_observations\n",
    "        self.n_actions = n_actions\n",
    "        self.memory = ReplayMemory(memory_capacity)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.optimization_iterations = optimization_iterations\n",
    "        self.target_net_update_interval = target_update_interval\n",
    "\n",
    "        self.policy_net = policy_net.to(device)\n",
    "        self.target_net = copy.deepcopy(self.policy_net)  # Exact copy of the object\n",
    "        self.target_net.to(device)\n",
    "        self.target_net.eval()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(),\n",
    "                                    lr=learning_rate)\n",
    "                                    \n",
    "        self.steps_done = 0\n",
    "\n",
    "        self.action_selection = action_selection\n",
    "\n",
    "    def __call__(self, state):\n",
    "        self.steps_done += 1\n",
    "        return self.action_selection.select_action(self.policy_net, state, self.steps_done)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.push(*args)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        \n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        for i in range(self.optimization_iterations):\n",
    "            transitions = self.memory.sample(self.batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "    \n",
    "            non_final_mask = torch.tensor(\n",
    "                tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                device=device,\n",
    "                dtype=torch.bool\n",
    "            )\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "            state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "            next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "            expected_state_action_values = reward_batch + self.gamma * next_state_values\n",
    "    \n",
    "            loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for param in self.policy_net.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def update_target_net(self,step):\n",
    "        if step % self.target_net_update_interval == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN Agent\n",
    "\n",
    "### What is Double DQN?\n",
    "\n",
    "The **Double DQN (DDQN)** agent is an enhancement over standard **Deep Q-Networks (DQN)** that mitigates **overestimation bias** in Q-learning. Introduced by **van Hasselt et al. (2015)**, **Double Q-learning** improves the stability of learning by decoupling the **action selection** and **action evaluation** steps.\n",
    "\n",
    "### Key Difference from DQN\n",
    "\n",
    "In **DQN**, the target Q-values are computed as:\n",
    "\n",
    "$$\n",
    "Q_{\\text{target}}(s, a) = r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')\n",
    "$$\n",
    "\n",
    "This approach often **overestimates Q-values**, leading to instability. \n",
    "\n",
    "**Double DQN fixes this by using two networks differently:**\n",
    "1. **The policy network selects the next action**:\n",
    "   $$\n",
    "   a^* = \\arg\\max_{a'} Q_{\\text{policy}}(s', a')\n",
    "   $$\n",
    "2. **The target network evaluates the chosen action**:\n",
    "   $$\n",
    "   Q_{\\text{target}}(s, a) = r + \\gamma Q_{\\text{target}}(s', a^*)\n",
    "   $$\n",
    "\n",
    "This reduces overestimation because the **same network is not used for both selection and evaluation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"\n",
    "    Double DQN Agent:\n",
    "    - Overrides the optimize_model() to do the Double Q-learning target.\n",
    "    \"\"\"\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        for _ in range(self.optimization_iterations):\n",
    "            transitions = self.memory.sample(self.batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            non_final_mask = torch.tensor([s is not None for s in batch.next_state], device=device, dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "            # Current Q estimates\n",
    "            q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "            next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "\n",
    "            # 1) Choose best action in next_state via policy_net\n",
    "            if non_final_mask.any():\n",
    "                next_actions = self.policy_net(non_final_next_states).argmax(1).unsqueeze(1)\n",
    "\n",
    "                # 2) Evaluate that action using target_net\n",
    "                next_q = self.target_net(non_final_next_states).gather(1, next_actions).squeeze(1)\n",
    "\n",
    "                next_state_values[non_final_mask] = next_q.detach()\n",
    "\n",
    "            # Compute the target\n",
    "            expected_q_values = reward_batch + (self.gamma * next_state_values)\n",
    "\n",
    "            # Loss and update\n",
    "            loss = F.smooth_l1_loss(q_values, expected_q_values.unsqueeze(1))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_value_(self.policy_net.parameters(), 1.0)\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns(episode_returns, show_result=False,ma = 25):\n",
    "    \"\"\"\n",
    "    Plots the total return (sum of rewards per episode) for an Atari Pong agent.\n",
    "\n",
    "    Args:\n",
    "        episode_returns (list): A list of total rewards per episode.\n",
    "        show_result (bool): Whether to show the final result or continue updating during training.\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    returns_t = torch.tensor(episode_returns, dtype=torch.float)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title('Final Training Results')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training Progress - Pong Agent')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Return')\n",
    "    plt.plot(returns_t.numpy(), label=\"Episode Returns\")\n",
    "\n",
    "    if len(returns_t) >= ma:\n",
    "        means = returns_t.unfold(0, ma, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(ma-1), means))\n",
    "        plt.plot(means.numpy(), linestyle='dashed', label=f\"{ma}-Episode Moving Avg\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)  # Pause a bit to update the plot\n",
    "\n",
    "    # Display in Jupyter Notebook properly\n",
    "    if 'is_ipython' in globals() and is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script for the DQN Agent\n",
    "\n",
    "### Overview\n",
    "\n",
    "The following script is responsible for training a **DQN-based agent** in a reinforcement learning environment. It follows the **standard reinforcement learning loop**, where the agent interacts with the environment, collects experience, and updates its policy over multiple episodes.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "1. **Initialize Training Parameters**:\n",
    "   - The number of **training episodes** is set based on the availability of a **GPU (CUDA/MPS)**.\n",
    "   - Stores **returns (total rewards per episode)** and **episode lengths** for performance tracking.\n",
    "\n",
    "2. **Episode Loop**:\n",
    "   - The environment is **reset** at the start of each episode.\n",
    "   - The initial **state is converted to a tensor** for processing by the neural network.\n",
    "\n",
    "3. **Agent-Environment Interaction**:\n",
    "   - The agent **selects an action** based on the current state.\n",
    "   - The action is **executed in the environment**, producing:\n",
    "     - **New observation** (next state).\n",
    "     - **Reward** received for the action.\n",
    "     - **Termination signal** (whether the episode has ended).\n",
    "   - The transition **(state, action, next_state, reward)** is stored in the agent's **Replay Buffer**.\n",
    "\n",
    "4. **Optimization Step**:\n",
    "   - The agent **samples experiences from the buffer** and **updates the policy network**.\n",
    "   - **Target network updates** ensure stable learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2500\n",
    "LR = 5e-5\n",
    "SAVE_PATH = \"dqn_pong.pth\"\n",
    "SAVE_INTERVAL = 10\n",
    "MEMORY_CAPACITY = 50_000\n",
    "OPTIMIZATION_ITERATIONS = 5\n",
    "TARGET_UPDATE_INTERVAL = 10\n",
    "EPISODES = 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/Pong-ram-v5\",)\n",
    "n_observation = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DuelingMlpPolicy(n_observation, n_actions, num_hidden=256)\n",
    "\n",
    "# Load a pretrained model if needed by uncommenting the line below\n",
    "# policy.load_state_dict(torch.load(SAVE_PATH))\n",
    "\n",
    "policy.to(device)\n",
    "action_selection = EpsilonGreedyActionSelection(EPS_START, EPS_END, EPS_DECAY,n_actions)\n",
    "agent = DoubleDQNAgent(n_observation, n_actions, action_selection, policy, memory_capacity=MEMORY_CAPACITY,\n",
    "                 optimization_iterations=OPTIMIZATION_ITERATIONS, batch_size=BATCH_SIZE,\n",
    "                 target_update_interval=TARGET_UPDATE_INTERVAL, gamma=GAMMA,learning_rate=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "training_data = {\n",
    "    'returns': [],\n",
    "    'episode_lengths': []\n",
    "}\n",
    "total_steps = 0\n",
    "\n",
    "for i_episode in range(EPISODES):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).div_(255.0).unsqueeze(0)  # Normalize before passing\n",
    "    episode_return = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = agent(state)\n",
    "\n",
    "    \n",
    "        next_observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        # Normalize observation & convert to tensor\n",
    "        # We apply normalization here to avoid unstable gradients\n",
    "        # The state representation is 128 integer vector, ranging from 0 to 255\n",
    "        # We normalize it to 0-1 range by dividing by 255\n",
    "        next_state = torch.tensor(next_observation, dtype=torch.float32, device=device).div_(255.0).unsqueeze(0) if not terminated else None\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "       \n",
    "        episode_return += reward.item()\n",
    "        agent.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        agent.optimize_model()\n",
    "        agent.update_target_net(total_steps)\n",
    "\n",
    "        total_steps += 1\n",
    "\n",
    "        if terminated or truncated:\n",
    "            training_data['returns'].append(episode_return)\n",
    "            training_data['episode_lengths'].append(t + 1)\n",
    "            plot_returns(training_data['returns'])\n",
    "            break\n",
    "\n",
    "    # Save model checkpoint periodically\n",
    "    if i_episode % SAVE_INTERVAL == 0:\n",
    "        torch.save(policy.state_dict(), SAVE_PATH)\n",
    "\n",
    "print('Training complete')\n",
    "plot_returns(training_data['returns'], show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "env = gym.make(\"ALE/Pong-ram-v5\",render_mode='human')\n",
    "NUM_EPISODES = 3\n",
    "RENDER_DELAY = 0.01\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).div_(255.0).unsqueeze(0)  # Normalize\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        time.sleep(RENDER_DELAY)  \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = agent.policy_net(state).argmax(dim=1).item()  \n",
    "        \n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "       \n",
    "        state = torch.tensor(next_state, dtype=torch.float32, device=device).div_(255.0).unsqueeze(0)\n",
    "\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{NUM_EPISODES} - Total Reward: {total_reward}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
