{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da821e0-a7d9-48b1-a92a-696c1b6bc491",
   "metadata": {},
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/9.%20On-policy%20Control/mountain_car.ipynb)\n",
    "\n",
    "# Mountain Car\n",
    "\n",
    "The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach the goal state on top of the right hill. In order to achieve this,\n",
    "the car must first climb up a little on the oppisite side of the hill, otherwise it won't be able to build up enough acceleration to reach the goal.\n",
    "\n",
    "<img src=\"assets/mountain_car.gif\" width=\"500\"/>\n",
    "\n",
    "The state space of this environment is continuous. The state consists of two floats:\n",
    "\n",
    "- The position of the car along the x-axis $([-1.2, 0.6])$\n",
    "- The velocity of the car $([-0.07, 0.07])$\n",
    "\n",
    "The starting position is picked randomly from the $[-0.6 , -0.4]$ range, and the starting velocity is always $0$.\n",
    "There are 3 discrete actions: left, right, and no acceleration. The reward is -1 on each time step, encouraging the agent to reach the goal as quickly as possible.\n",
    "Termination occurs when the car reaches the goal on the right, or the episode gets truncated after 200 steps.\n",
    "\n",
    "We will use *tile coding* as our function approximator, in the form of a \"Tiled Q-table\". This is adapted from the \"Tiled value-table\" from earlier, but instead of the $V$-function,\n",
    "it approximates the $Q$-function. The learning algorithm is going to be *Episodic Semi-gradient Sarsa*, an on-policy method for continuous problems.\n",
    "\n",
    "- Documentation for the Mountain Car environment: https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
    "- This notebook is based on Chapter 10 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272f003-3e8b-47dd-869a-196c221bb7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install gymnasium==0.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8515e-60e6-48a9-b98a-6d13980f492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "import pickle\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a45b7-7150-483e-8548-67e185e4b347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_EPISODES = 1_000  # Number of training episodes\n",
    "N_TILINGS = 8  # Number of tilings\n",
    "N_BINS = 8  # Number of bins per dimension per tiling\n",
    "ALPHA = 0.1  # Learning rate\n",
    "GAMMA = 1  # Discount rate\n",
    "EPSILON = 0.1  # Exploration rate\n",
    "N_RECORDINGS = 3  # Number of episodes to record\n",
    "REC_EPISODES = np.linspace(0, N_EPISODES-1, num=N_RECORDINGS, dtype=int)  # Episodes to record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a188b4-c7ed-4990-b6f8-09f6e9bfa302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create environment\n",
    "base_env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "# Wrap environment to record videos throughout the learning process \n",
    "trigger = lambda ep: ep in REC_EPISODES\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524b277-2b42-4154-afb7-b453d3fcefb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOW = env.observation_space.low\n",
    "HIGH = env.observation_space.high\n",
    "N_ACTIONS = env.action_space.n\n",
    "\n",
    "print(f'N_ACTIONS: {N_ACTIONS}\\n')\n",
    "print('\\tPos.  Vel.')\n",
    "print(f'LOW:\\t{LOW}')\n",
    "print(f'HIGH:\\t{HIGH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78cf2e-c93f-4a08-8a59-e689cdf5ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax function that breaks ties randomly\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7abbe-a52c-41c8-9d7b-62a6cc77a611",
   "metadata": {},
   "source": [
    "## The Tiled Q-Table\n",
    "\n",
    "We can easily adapt the tiled value-table from earlier to approximate the $Q$-function. The number of actions is concatenated to the shape of the table,\n",
    "so we have separate action-values for each tile in each tiling. The argument of the `__getitem__` and `__setitem__` methods is now a tuple of `(state, action)`,\n",
    "and instead of a state-value, they return a state-action value.\n",
    "Finally, in order to be able select the optimal action for each state, an `argmax` function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c6132-a024-4343-8455-53fcfe4e63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiledQTable:\n",
    "    def __init__(self, n_dims, low, high, offsets, n_actions, n_bins):\n",
    "        self.tilings = []\n",
    "        self.n_tilings = len(offsets)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Create tilings\n",
    "        for offset in offsets:\n",
    "            tiling = [np.linspace(low[dim], high[dim], n_bins + 1)[1:-1] + offset[dim] for dim in range(n_dims)]\n",
    "            self.tilings.append(tiling)\n",
    "\n",
    "        # Initialize Q-table\n",
    "        shape = (self.n_tilings, ) + (n_bins, ) * n_dims + (n_actions, )\n",
    "        self.q_table = np.zeros(shape)\n",
    "\n",
    "    def __getitem__(self, args):\n",
    "        obs = args[0]\n",
    "        action = args[1]\n",
    "        val = 0\n",
    "\n",
    "        # Sum up values in all tilings\n",
    "        for i, tiling in enumerate(self.tilings):\n",
    "            ind = [i]\n",
    "            for dim in range(len(obs)):\n",
    "                ind.append(np.searchsorted(tiling[dim], obs[dim]))\n",
    "            ind.append(action)\n",
    "            val += self.q_table[tuple(ind)]\n",
    "\n",
    "        return val\n",
    "\n",
    "    def __setitem__(self, args, new):\n",
    "        old = self.__getitem__(args)\n",
    "        obs = args[0]\n",
    "        action = args[1]\n",
    "\n",
    "        # Shift values in all tilings\n",
    "        for i, tiling in enumerate(self.tilings):\n",
    "            ind = [i]\n",
    "            for dim in range(len(obs)):\n",
    "                ind.append(np.searchsorted(tiling[dim], obs[dim]))\n",
    "            ind.append(action)\n",
    "            self.q_table[tuple(ind)] += (new - old) / self.n_tilings\n",
    "\n",
    "    def argmax(self, obs):\n",
    "        action_values = np.zeros(self.n_actions)       \n",
    "\n",
    "        # Sum up the values for each action in all tilings\n",
    "        for i, tiling in enumerate(self.tilings):\n",
    "            ind = [i]\n",
    "            for dim in range(len(obs)):\n",
    "                ind.append(np.searchsorted(tiling[dim], obs[dim]))\n",
    "            action_values += self.q_table[tuple(ind)]\n",
    "            \n",
    "        return argmax(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db294a-261e-4b7f-9e90-fd5a90383e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set asymmetrical offsets to avoid artifacts in\n",
    "# generalization; see Sutton & Barto pg. 218-220 for details\n",
    "tile_width = (HIGH - LOW) / N_BINS\n",
    "unit = tile_width / N_TILINGS\n",
    "offsets = [(unit[0] * i, 3 * unit[1] * i) for i in range(N_TILINGS)]\n",
    "\n",
    "# Initialize tiled Q-table\n",
    "tqt = TiledQTable(2, LOW, HIGH, offsets, N_ACTIONS, N_BINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491745a-40b4-4b83-be70-8ae6dbb5fef5",
   "metadata": {},
   "source": [
    "## Episodic Semi-gradient Sarsa\n",
    "\n",
    "Episodic Semi-gradient Sarsa is a simple learning method, similar to Q-learning. The update rule is defined as:\n",
    "\n",
    "$$\\textbf{w} \\leftarrow \\textbf{w} + \\alpha \\left[R+\\gamma\\hat q(S',A',\\textbf{w}) - \\hat q(S,A,\\textbf{w})\\right]\\nabla \\hat q(S,A,\\textbf{w})$$\n",
    "\n",
    "Where $\\hat q$ is the approximate $Q$-function, with $\\textbf{w}$ being a vector of its weights.\n",
    "In our case, $\\hat q$ is the tiled Q-table, and its weight vector consists of each of the tile-action-values.\n",
    "Mathematically, this can be defined as\n",
    "\n",
    "$$\\hat q(s, a, \\textbf{w}) := \\textbf{w}^\\top\\textbf{x}(s, a)=\n",
    "\\sum_{i=1}^d w_i\\cdot x_i(s,a)$$\n",
    "\n",
    "Where $\\textbf{x}(s,a)$ is a *feature vector*, a mapping of state-action pairs to a vector. The weight vector and the feature vectors have equal sizes,\n",
    "in this case, it is \"$\\text{number of tiles across all tilings}\\times\\text{number of actions}$\". The value of $x_i(s,a)$ will be 1 for all $i$ where\n",
    "$w_i$ is a tile-action-value of action $a$ and a tile that $s$ falls in, and 0 otherwise. For any $s, a$, there will be exactly as many $x_i(s,a)$\n",
    "of value 1 as many tilings exist.\n",
    "\n",
    "But what is the gradient? The partial derivatives w.r.t. each of the $w_i$-s are simply $\\frac{\\partial \\hat q(s, a, \\textbf{w})}{\\partial w_i} = x_i(s,a)$,\n",
    "so the gradient is basically a zero-vector except for the few positions corresponding to the current state-action pair. Intuitively, this means that we only\n",
    "shift the weights related to $(s, a)$, and leave the others alone.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement this algorithm! Pseudocode is shown in the box below.\n",
    "\n",
    "<img src=\"assets/ep_sg_sarsa.png\" width=\"700\"/>\n",
    "\n",
    "*Pseudocode from page 244 of the Sutton & Barto book*\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- By using the Tiled Q-Table implementation, you don't have to explictly update weight vectors or calculate gradients. Think about how the update rule in the pseudocode translates to the TQT!\n",
    "- The `tqt.argmax` function can be used to find the action with the highest value in a state.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce23eb1-bf19-4b60-b183-91e3d812d9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Re-initialize environment and TQT\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n",
    "tqt = TiledQTable(2, LOW, HIGH, offsets, N_ACTIONS, N_BINS)\n",
    "\n",
    "# Training\n",
    "for episode in trange(N_EPISODES):\n",
    "\n",
    "    ############## CODE HERE ###################\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    \n",
    "# Save the tiled Q-table\n",
    "with open('tqt.bin', 'wb') as f:\n",
    "    pickle.dump(tqt, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d52bd4-3072-431e-a052-70d376038067",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "You can watch the videos recorded throughout the training process here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eecfef1-13d6-403f-b0f1-4777d81f621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recordings\n",
    "children = [widgets.Video.from_file(f'./videos/rl-video-episode-{episode}.mp4', autoplay=False, loop=False, width=500) for episode in REC_EPISODES]\n",
    "tab = widgets.Tab()\n",
    "tab.children = children\n",
    "titles = tuple([f'Episode {episode + 1:,}' for episode in REC_EPISODES])\n",
    "for i in range(len(children)):\n",
    "    tab.set_title(i, titles[i])\n",
    "display(tab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
