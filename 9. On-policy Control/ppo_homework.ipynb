{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45046d37",
   "metadata": {},
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by  **Zolt√°n Barta**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/9.%20On-policy%20Control/ppo_homework.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae6bad",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "\n",
    "Proximal Policy Optimization (PPO) is a popular policy gradient algorithm in reinforcement learning. It is especially effective in continuous control tasks. PPO simplifies the trust-region idea from TRPO by using a clipped surrogate objective, allowing for more stable and efficient training without requiring complex optimization techniques.\n",
    "\n",
    "## What is PPO?\n",
    "\n",
    "PPO is an on-policy, actor-critic algorithm with the following features:\n",
    "\n",
    "1. **Actor-Critic Structure**:  \n",
    "   - Actor network: policy $\\pi_\\theta(a|s)$ that selects actions  \n",
    "   - Critic network: value function $V_\\phi(s)$ that evaluates states\n",
    "\n",
    "2. **On-Policy Learning**:  \n",
    "   - Data is collected using the current policy only.\n",
    "\n",
    "3. **Clipped Surrogate Objective**:  \n",
    "   - Avoids large policy updates that destabilize learning.\n",
    "\n",
    "4. **Multiple Epochs per Batch**:  \n",
    "   - Improves sample efficiency by reusing collected data.\n",
    "\n",
    "## PPO Objective Function\n",
    "\n",
    "### Clipped Surrogate Objective\n",
    "\n",
    "Given:\n",
    "\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$\n",
    "- $\\hat{A}_t$: estimated advantage\n",
    "\n",
    "The clipped objective is:\n",
    "\n",
    "$$\n",
    "L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t,\\ \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon)\\hat{A}_t \\right) \\right]\n",
    "$$\n",
    "\n",
    "This prevents the policy from changing too much in a single update step.\n",
    "\n",
    "### Value Function Loss\n",
    "\n",
    "The critic is trained to minimize the squared error:\n",
    "\n",
    "$$\n",
    "L^{VF}(\\phi) = \\mathbb{E}_t \\left[ \\left( V_\\phi(s_t) - V_t^{\\text{target}} \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "### Entropy Bonus\n",
    "\n",
    "Encourages exploration:\n",
    "\n",
    "$$\n",
    "S[\\pi_\\theta](s_t) = \\mathbb{E}_{a \\sim \\pi_\\theta} [-\\log \\pi_\\theta(a|s_t)]\n",
    "$$\n",
    "\n",
    "### Combined PPO Loss\n",
    "\n",
    "The full loss combines the components:\n",
    "\n",
    "$$\n",
    "L^{PPO} = \\mathbb{E}_t \\left[ L^{CLIP}(\\theta) - c_1 L^{VF}(\\phi) + c_2 S[\\pi_\\theta](s_t) \\right]\n",
    "$$\n",
    "\n",
    "## PPO Algorithm Steps\n",
    "\n",
    "1. Initialize policy and value networks and hyperparameters\n",
    "2. Collect trajectories using the current policy\n",
    "3. Estimate advantages (typically using GAE)\n",
    "4. For several epochs:\n",
    "   - Divide data into mini-batches\n",
    "   - Compute clipped loss, value loss, entropy bonus\n",
    "   - Update network parameters using gradient descent\n",
    "5. Repeat\n",
    "\n",
    "## Generalized Advantage Estimation (GAE)\n",
    "\n",
    "GAE reduces variance in advantage estimation:\n",
    "\n",
    "$$\n",
    "\\hat{A}^{GAE}_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}, \\quad \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
    "$$\n",
    "\n",
    "\n",
    "## Why PPO?\n",
    "\n",
    "- Simpler than TRPO to implement\n",
    "- More stable than vanilla policy gradients\n",
    "- Supports multiple training epochs per batch\n",
    "- Scales well to large models and complex tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f41f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import random\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "obs,info = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b6452",
   "metadata": {},
   "source": [
    "### Task: Implement a Policy Network for PPO (Continuous Action Space)\n",
    "\n",
    "In this task, you will implement a multi-layer perceptron (MLP) policy network for a PPO agent operating in a continuous action space.\n",
    "\n",
    "The goal is for the network to output the parameters of a Normal (Gaussian) distribution from which actions can be sampled.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "- Subclass `nn.Module`\n",
    "- Use two hidden layers with ReLU activations\n",
    "- The final layer should output the **mean** of the action distribution\n",
    "- Define a learnable parameter `log_std` to represent the log standard deviation\n",
    "- In the `forward()` method, return a `torch.distributions.Normal(mean, std)` object\n",
    "\n",
    "---\n",
    "\n",
    "### Network Structure:\n",
    "\n",
    "- Input: `n_observations` (number of input features)\n",
    "- Hidden Layer 1: 128 units + ReLU\n",
    "- Hidden Layer 2: 128 units + ReLU\n",
    "- Output Layer: `n_actions` units (mean of the Gaussian)\n",
    "- Standard deviation: computed using `torch.exp(log_std)`, where `log_std` is a learnable `nn.Parameter`\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Notes:\n",
    "\n",
    "- Use `torch.nn.Parameter` for `log_std` so that it can be learned during training\n",
    "- In the `forward()` method, make sure the input is a float tensor on the correct device\n",
    "- If the input is a 1D tensor (single observation), add a batch dimension with `unsqueeze(0)`\n",
    "- Return a `Normal(mean, std)` distribution from `torch.distributions`\n",
    "\n",
    "---\n",
    "\n",
    "This network will allow you to sample continuous actions for a PPO agent and compute log-probabilities needed for training. Make sure to test the output distribution to verify it behaves as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463121bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\" MLP Actor network for PPO with continuous action space \"\"\"\n",
    "    def __init__(self, n_observations: int, n_actions: int):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        ###################CODE HERE###################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ################################################\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass, returns a Normal (Gaussian) distribution over actions.\n",
    "        \"\"\"\n",
    "        ###################CODE HERE###################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3de96",
   "metadata": {},
   "source": [
    "### Task: Implement a Value Network for PPO (Critic)\n",
    "\n",
    "In this task, you will build a multi-layer perceptron (MLP) value network that serves as the **critic** in a PPO setup. The value network estimates the **state value** for a given observation, which is used in advantage estimation and value loss computation.\n",
    "\n",
    "---\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "- Subclass `nn.Module`\n",
    "- Use two hidden layers with ReLU activations\n",
    "- The final layer should output a single scalar value per input state\n",
    "- In the `forward()` method, return the estimated value as a tensor\n",
    "\n",
    "---\n",
    "\n",
    "### Network Structure:\n",
    "\n",
    "- Input: `n_observations` (state dimension)\n",
    "- Hidden Layer 1: 128 units + ReLU\n",
    "- Hidden Layer 2: 128 units + ReLU\n",
    "- Output Layer: 1 unit (scalar value)\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Notes:\n",
    "\n",
    "- Convert input to `torch.float32` if needed\n",
    "- If input is 1D (a single state), add a batch dimension with `unsqueeze(0)`\n",
    "- Use `F.relu()` as the activation function after each hidden layer\n",
    "- The final layer should not apply any activation\n",
    "\n",
    "---\n",
    "\n",
    "This network is used to approximate the expected return (value) of a given state. It will be trained by minimizing the squared difference between predicted values and target returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc42c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\" MLP Critic network for PPO \"\"\"\n",
    "    def __init__(self, n_observations: int):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        ###################CODE HERE###################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ################################################\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass, returns the estimated state value.\n",
    "        \"\"\"\n",
    "        ###################CODE HERE###################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315aaccb",
   "metadata": {},
   "source": [
    "### Task: Implement Generalized Advantage Estimation (GAE)\n",
    "\n",
    "In this task, you will implement the **Generalized Advantage Estimation (GAE)** function. GAE is used in PPO to compute a low-variance and smoother estimate of the advantage function, which guides policy updates.\n",
    "\n",
    "---\n",
    "\n",
    "### Inputs:\n",
    "\n",
    "- `rewards`: Tensor of rewards collected from the environment.\n",
    "- `values`: Estimated state values from the value network at each timestep.\n",
    "- `next_values`: Value predictions for the next states.\n",
    "- `dones`: Tensor indicating episode terminations (1 if done, 0 otherwise).\n",
    "- `gamma`: Discount factor (typically around 0.99).\n",
    "- `lambda_gae`: GAE lambda parameter (typically around 0.95).\n",
    "- `standardize` (optional): Whether to normalize the advantages.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **TD Residual** (`delta`):  \n",
    "   $$\n",
    "   \\delta_t = r_t + \\gamma \\cdot V_{t+1} \\cdot (1 - \\text{done}_t) - V_t\n",
    "   $$\n",
    "\n",
    "2. **Recursive Advantage Calculation**:  \n",
    "   Starting from the end of the trajectory and moving backward:\n",
    "   $$\n",
    "   A_t = \\delta_t + \\gamma \\lambda \\cdot A_{t+1} \\cdot (1 - \\text{done}_t)\n",
    "   $$\n",
    "\n",
    "3. **Standardization** (optional):  \n",
    "   Normalize advantages to have zero mean and unit variance to improve training stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards: torch.Tensor, \n",
    "                values: torch.Tensor, \n",
    "                next_values: torch.Tensor, \n",
    "                dones: torch.Tensor, \n",
    "                gamma: float, \n",
    "                lambda_gae: float, \n",
    "                standardize: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes Generalized Advantage Estimation (GAE).\n",
    "    \"\"\"\n",
    "    ###################CODE HERE###################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    ################################################\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7769fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env, policy_net, max_steps):\n",
    "    observations, actions, rewards, log_probs, dones = [], [], [], [], []\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(max_steps):\n",
    "        obs_tensor = torch.tensor(obs.flatten(), dtype=torch.float32, device=device)\n",
    "        \n",
    "        dist = policy_net(obs_tensor)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(-1)\n",
    "    \n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action.cpu().numpy())\n",
    "        done = terminated or truncated\n",
    "    \n",
    "        observations.append(obs_tensor.squeeze(0))  # to keep consistent shape [n]\n",
    "        actions.append(action)\n",
    "        rewards.append(torch.tensor(reward, dtype=torch.float32, device=device))\n",
    "        log_probs.append(log_prob.detach())\n",
    "        dones.append(torch.tensor(done, dtype=torch.float32, device=device))\n",
    "    \n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    observations = torch.stack(observations)\n",
    "    actions = torch.stack(actions)\n",
    "    rewards = torch.stack(rewards)\n",
    "    log_probs = torch.stack(log_probs)\n",
    "    dones = torch.stack(dones)\n",
    "\n",
    "    return observations, actions, log_probs, rewards, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ccdd56",
   "metadata": {},
   "source": [
    "### Task: Implement the training loop\n",
    "Implement the main training loop for the Proximal Policy Optimization (PPO) algorithm. This loop should train a policy network and a value network using data collected from interaction with an environment.\n",
    "\n",
    "---\n",
    "### What the Function Should Do\n",
    "\n",
    "1. **Collect Experience**  \n",
    "   Interact with the environment for a fixed number of steps. Record:\n",
    "   - Observations\n",
    "   - Actions\n",
    "   - Rewards\n",
    "   - Log-probabilities of actions (under the current policy)\n",
    "   - Done flags\n",
    "\n",
    "2. **Estimate Values and Compute Advantages**  \n",
    "   Use the value network to estimate state values. Then, compute advantages using Generalized Advantage Estimation (GAE). Optionally standardize the advantages for numerical stability.\n",
    "\n",
    "3. **Compute Returns**  \n",
    "   Add the computed advantages to the estimated values to get the target returns for value function learning.\n",
    "\n",
    "4. **Optimize Policy and Value Networks**  \n",
    "   For a number of epochs:\n",
    "   - Shuffle and divide the data into mini-batches.\n",
    "   - For each batch:\n",
    "     - Compute the policy loss using the clipped surrogate PPO objective.\n",
    "     - Compute the value loss using mean squared error between predicted and target returns.\n",
    "     - Backpropagate and update both networks using their respective optimizers.\n",
    "\n",
    "5. **Log Progress**  \n",
    "   After each iteration, print or store:\n",
    "   - Total reward collected\n",
    "   - Average policy loss\n",
    "   - Average value loss\n",
    "   - Mean and standard deviation of advantages\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Detach tensors where appropriate to avoid reusing computation graphs.\n",
    "- Make sure log-probabilities from the old policy are detached before being used in the surrogate loss.\n",
    "- The value targets (`returns`) should not have gradients.\n",
    "\n",
    "This loop should repeat for a specified number of iterations to progressively improve the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1056e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_training_loop(env, policy_net, value_net, optimizer_policy, optimizer_value,\n",
    "                      iterations, max_steps_per_iter, gamma=0.99, lambda_gae=0.95,\n",
    "                      epsilon_clip=0.2, epochs=10, batch_size=64):\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # --- Collect data ---\n",
    "        observations, actions, log_probs_old, rewards, dones = collect_data(env, policy_net, max_steps_per_iter)\n",
    "        ###################CODE HERE###################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ################################################\n",
    "        avg_policy_loss = 0.0\n",
    "        avg_value_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            indices = torch.randperm(dataset_size)\n",
    "\n",
    "            for start in range(0, dataset_size, batch_size):\n",
    "                end = start + batch_size\n",
    "                batch_indices = indices[start:end]\n",
    "                ###################CODE HERE###################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ################################################\n",
    "\n",
    "                avg_policy_loss += policy_loss.item()\n",
    "                avg_value_loss += value_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        avg_policy_loss /= num_batches\n",
    "        avg_value_loss /= num_batches\n",
    "        total_reward = rewards.sum().item()\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}/{iterations} | \"\n",
    "              f\"Total Reward: {total_reward:.2f} | \"\n",
    "              f\"Avg Policy Loss: {avg_policy_loss:.4f} | \"\n",
    "              f\"Avg Value Loss: {avg_value_loss:.4f} | \"\n",
    "              f\"Advantage Mean: {advantages.mean().item():.4f} | Std: {advantages.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "n_observations = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "policy_net = PolicyNetwork(n_observations, n_actions).to(device)\n",
    "value_net = ValueNetwork(n_observations).to(device)\n",
    "\n",
    "optimizer_policy = Adam(policy_net.parameters(), lr=3e-4)\n",
    "optimizer_value = Adam(value_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_training_loop(\n",
    "    env=env,\n",
    "    policy_net=policy_net,\n",
    "    value_net=value_net,\n",
    "    optimizer_policy=optimizer_policy,\n",
    "    optimizer_value=optimizer_value,\n",
    "    iterations=50,              \n",
    "    max_steps_per_iter=2048,    \n",
    "    gamma=0.99,\n",
    "    lambda_gae=0.95,\n",
    "    epsilon_clip=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
