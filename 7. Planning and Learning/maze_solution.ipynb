{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6b7a47-954c-47e7-b711-799901326092",
   "metadata": {},
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/7.%20Planning%20and%20Learning/maze_solution.ipynb)\n",
    "\n",
    "# Maze (solution)\n",
    "\n",
    "In this notebook, we consider a simple maze-solving problem where the agent has to find the shortest path from the start (top left corner) to the exit (bottom right corner).\n",
    "\n",
    "<img src=\"assets/maze.gif\" width=\"400\"/>\n",
    "\n",
    "This maze environment was originally made with the old `gym` library, but we apply an API compatibility layer, so it behaves exactly like\n",
    "a `gymnasium` environment. The states are the x, y coordinates of the agent (which we transform to be respresented by a single integer), and the actions are the 4 directions: 'N', 'S', 'E' and 'W'. The reward\n",
    "is -0.1/(number of cells) for each step, and a reward of +1 is received for reaching the goal.\n",
    "\n",
    "The maze is randomly generated each time the environment is created. This notebook uses *prioritized sweeping* to approximate the optimal policy in the (third-party) `maze-random-10x10-v0` environment.\n",
    "\n",
    "- This notebook is based on Chapter 8 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html\n",
    "- Documentation for the Maze environment: https://github.com/MattChanTK/gym-maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40b9c5e-45ff-409b-9a2e-ac13fd69b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install gymnasium==0.29.0\n",
    "    !pip install setuptools==58.2.0 \n",
    "    !pip install shimmy[gym-v26]\n",
    "    !git clone https://github.com/MattChanTK/gym-maze gym-maze\n",
    "    %cd gym-maze\n",
    "    !python3 setup.py install\n",
    "    %cd ..\n",
    "    !rm -r gym* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217b9e4d-1854-405c-9266-803b34915fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_maze\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from gymnasium.wrappers import TransformObservation, RecordVideo\n",
    "import ipywidgets as widgets\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b306b4-6004-406c-90e7-0daa1f17ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_EPISODES = 5  # Number of training episodes\n",
    "N_UPDATES_PER_STEP = 200  # Number of planning updates per interaction with the environment\n",
    "EPSILON = 0.01  # Exploration rate\n",
    "ALPHA = 0.7  # Learning rate\n",
    "GAMMA = 1  # Discount factor\n",
    "THETA = 0.01  # Priority treshold\n",
    "N_RECORDINGS = 3  # Number of episodes to record\n",
    "REC_EPISODES = np.linspace(1, N_EPISODES-1, num=N_RECORDINGS, dtype=int)  # Episodes to record (the first episode is not recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabb6faf-27bb-44e4-98c4-e136f0e4dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize gym environment with gymnasium compatibility\n",
    "base_env = gym.make(\"maze-random-10x10-v0\", apply_api_compatibility=True, render_mode='rgb_array')\n",
    "# Transform observation representation from array to int: e.g. [3, 4] -> 43\n",
    "base_env = TransformObservation(base_env, lambda obs: int(obs[1] * (base_env.observation_space.high + 1)[0] + obs[0]))\n",
    "# Wrap environment to record videos throughout the learning process \n",
    "trigger = lambda ep: ep in REC_EPISODES\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f93420e-b1af-4142-b48c-15bb04474f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "action_space_size = env.action_space.n\n",
    "observation_space_size = (env.observation_space.high + 1)[0] * \\\n",
    "    (env.observation_space.high + 1)[1]\n",
    "q_table_shape = observation_space_size, action_space_size\n",
    "q_table = np.zeros(q_table_shape)\n",
    "\n",
    "# Initialize priority 'queue'\n",
    "priorities = np.zeros(q_table_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36954da-d9e2-4a34-832f-f7133beee555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax function that breaks ties randomly\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c13ee-8ee9-4e3f-8de8-2f5a6b66ae22",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "In addition to the Q-table, the agent also learns a model of the environment. Since the maze is deterministic, the model is pretty simple:\n",
    "for each state-action pair, the model stores the next state and reward: $\\text{Model}(S_t, A_t) = (R_{t+1}, S_{t+1})$. The `add` method\n",
    "is used to add new information to the model, while the `get` method returns the reward and next state for a given state-action pair.\n",
    "The `get_leading` method returns all state-action pairs that lead to a given state: $\\text{get} \\textunderscore \\text{leading}(S_t) = \\{(s, a)\\in\\mathcal{S}\\times\\mathcal{A}\\ |\\ Model(s, a)_2 = S_t\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a59e5c6-abd4-44a6-8363-020a9bb84e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class representing a model of the environment\n",
    "class Model:\n",
    "    def __init__(self, shape):\n",
    "        self.transitions = np.zeros(shape, dtype=int) - 1\n",
    "        self.rewards = np.zeros(shape)\n",
    "        self.transitions_reverse = {}\n",
    "\n",
    "    # Add new information to the model:\n",
    "    # Taking 'action' in 'obs' produces 'reward'\n",
    "    # and transfers the agent to 'new_obs'\n",
    "    def add(self, obs, action, reward, new_obs):\n",
    "        self.transitions[obs, action] = new_obs\n",
    "        self.rewards[obs, action] = reward\n",
    "\n",
    "        if new_obs in self.transitions_reverse:\n",
    "            if not (obs, action) in self.transitions_reverse[new_obs]:\n",
    "                self.transitions_reverse[new_obs].append((obs, action))\n",
    "        else:\n",
    "            self.transitions_reverse[new_obs] = [(obs, action)]\n",
    "\n",
    "    # Get information from the model:\n",
    "    def get(self, obs, action):\n",
    "        new_obs = self.transitions[obs, action]\n",
    "        reward = self.rewards[obs, action]\n",
    "        return new_obs, reward\n",
    "\n",
    "    # Get all obs-action pairs that lead the agent to 'obs'\n",
    "    def get_leading(self, obs):\n",
    "        return self.transitions_reverse[obs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307c573-e7b8-40c0-b234-6e3b52cd1327",
   "metadata": {},
   "source": [
    "## Prioritized Sweeping\n",
    "\n",
    "Prioritized sweeping works similarly to Q-learning, but in addition to updates based on real experience (*learning*) it also utilizes updates based on simulated\n",
    "experience (*planning*). To be able to do this, the algorithm records each state transition, forming a model of the environment.\n",
    "\n",
    "A simpler, similar algorithm is called *Dyna-Q*: it is essentially Q-learning with state transitions being recorded to a model, and after each step, randomly selected\n",
    "Q-values are being updated using information from the model.\n",
    "Prioritized sweeping improves on Dyna-Q by focusing on updating state-action pairs with higher temporal-difference error values more frequently. By prioritizing updates based on the magnitude of the error, it accelerates the learning process by directing attention to the most critical areas of the environment, where value estimates need refinement, leading to quicker convergence.\n",
    "\n",
    "Since state-action pairs that lead to a state with a high error are likely to have a high TD error themselves, after each update, the errors of state-action pairs that lead to the newly updated state are recalculated. This leads to a quick backpropagation of rewards: for example, in the maze environment, after the goal state is reached, prioritized sweeping will\n",
    "first update the state-action pair leading to the goal state, then the state-action pairs leading to the state just before the goal state, and so on, spreading backwards from the\n",
    "goal to the starting state.\n",
    "\n",
    "In this example, the agent learns to solve the maze in only a few episodes, with 200 planning updates after each step. Due to the nature of this environment, the agent will wander around the maze aimlessly in the first episode until it stumbles upon the goal. From that moment, the newly gained information propagates backwards rapidly, and the agent's performance increases marginally in the second episode.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement this algorithm! The block below only contains code necessary for logging the episode length and queue size after each episode. The algorithm itself is up to you! Pseudocode for this algorithm is shown in the box below.\n",
    "\n",
    "<img src=\"assets/prioritized_sweeping.png\" width=\"700\"/>\n",
    "\n",
    "*Pseudocode from page 170 of the Sutton & Barto book*\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- For simplicity and readability, it's recommended to separate the \"planning\" phase (*(g)* in the pseudocode) to its own function.\n",
    "- Instead of 0-3, the actions in this environment are the strings 'N', 'S', 'E' and 'W'.\n",
    "- The priority queue is represented by a NumPy array (`priorities`), as Python's built-in implementations don't allow modifying the priority of an element. \n",
    "- Truncation is not needed for this environment.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ca6b72-6124-4a22-bda1-071e88fadd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planning function\n",
    "def plan():\n",
    "    for _ in range(N_UPDATES_PER_STEP):\n",
    "        # Get state-action pair with highest priority\n",
    "        obs, action = np.unravel_index(np.argmax(priorities), priorities.shape)\n",
    "        \n",
    "        # If the highest priority is 0, there is nothing to do\n",
    "        if priorities[obs, action] == 0:\n",
    "            break\n",
    "\n",
    "        # Reset priority\n",
    "        priorities[obs, action] = 0\n",
    "\n",
    "        # Get new state and reward from model\n",
    "        new_obs, reward = model.get(obs, action)\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[obs, action] += ALPHA * (reward + GAMMA * np.max(q_table[new_obs]) - q_table[obs, action])\n",
    "\n",
    "        # Add leading states to queue\n",
    "        for prev_obs, prev_action in model.get_leading(obs):\n",
    "            _, prev_reward = model.get(prev_obs, prev_action)\n",
    "            priority = abs(prev_reward + GAMMA * np.max(q_table[obs] - q_table[prev_obs, prev_action]))\n",
    "\n",
    "            if priorities[prev_obs, prev_action] < priority and priority > THETA:\n",
    "                priorities[prev_obs, prev_action] = priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfcb921-5953-4911-af60-5042349d20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1:\n",
      "\tSteps: 1,498\n",
      "\tQueue size: 59\n",
      "Episode 2:\n",
      "\tSteps: 23\n",
      "\tQueue size: 92\n",
      "Episode 3:\n",
      "\tSteps: 22\n",
      "\tQueue size: 31\n",
      "Episode 4:\n",
      "\tSteps: 22\n",
      "\tQueue size: 2\n",
      "Episode 5:\n",
      "\tSteps: 22\n",
      "\tQueue size: 0\n"
     ]
    }
   ],
   "source": [
    "# Initalize environment, Q-table, model and priority queue\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n",
    "q_table = np.zeros(q_table_shape)\n",
    "model = Model(q_table_shape)\n",
    "priorities = np.zeros(q_table_shape)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(N_EPISODES):\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    n_steps = 0\n",
    "\n",
    "    while not terminated:\n",
    "        n_steps += 1\n",
    "        ############## CODE HERE ###################\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() > EPSILON:\n",
    "            action = argmax(q_table[obs])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take selected action\n",
    "        new_obs, reward, terminated, truncated, info = env.step(['N', 'S', 'E', 'W'][action])\n",
    "        \n",
    "        # Add information to the model\n",
    "        model.add(obs, action, reward, new_obs)\n",
    "\n",
    "        # Add state-action pair to queue\n",
    "        priority = abs(reward + GAMMA * np.max(q_table[new_obs] - q_table[obs, action]))\n",
    "        if priorities[obs, action] < priority and priority > THETA:\n",
    "            priorities[obs, action] = priority\n",
    "\n",
    "        # Store new state\n",
    "        obs = new_obs\n",
    "\n",
    "        # Start a planning phase\n",
    "        plan()\n",
    "\n",
    "        ############################################\n",
    "        \n",
    "    # Log results\n",
    "    print(f'Episode {episode+1:,}:\\n\\tSteps: {n_steps:,}\\n\\tQueue size: {np.count_nonzero(priorities):,}')\n",
    "    \n",
    "# Save Q-table\n",
    "with open('q_table.bin', 'wb') as f:\n",
    "    pickle.dump(q_table, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbd403-025f-47c2-a80a-97de99d3a5df",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "You can watch the videos recorded throughout the training process here:\n",
    "\n",
    "*(Note that the first episode is not recorded due to its excessive length)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e23cef3b-d417-4fe0-9861-b857cad7ae1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc18efbb22994bc68f6ec17cb9482286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free...', aut…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display recordings\n",
    "children = [widgets.Video.from_file(f'./videos/rl-video-episode-{episode}.mp4', autoplay=False, loop=False, width=500) for episode in REC_EPISODES]\n",
    "tab = widgets.Tab()\n",
    "tab.children = children\n",
    "titles = tuple([f'Episode {episode + 1:,}' for episode in REC_EPISODES])\n",
    "for i in range(len(children)):\n",
    "    tab.set_title(i, titles[i])\n",
    "display.display(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282f72d-86d6-44f2-b364-356ed0aff67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
