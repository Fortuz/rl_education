{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6b7a47-954c-47e7-b711-799901326092",
   "metadata": {},
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/7.%20Planning%20and%20Learning/maze_solution.ipynb)\n",
    "\n",
    "# Maze (solution)\n",
    "\n",
    "In this notebook we consider the problem of solving a maze in only a few episodes.\n",
    "\n",
    "![Example](assets/maze.gif)\n",
    "\n",
    "This maze environment was originally made with the old `gym` library, but we apply an API compatibility layer, so it behaves exactly like\n",
    "a `gymnasium` environment. The states are the x, y coordinates of the agent (which we transform to be respresented by a single integer), and the actions are the 4 directions: 'N', 'S', 'E' and 'W'. The reward\n",
    "is -0.1/(number of cells) for each step, and for reaching the goal, a reward of +1 is received.\n",
    "\n",
    "The maze is randomly generated every time the environment is created. To find the optimal path in only a small number of episodes, we are going to use\n",
    "*Prioritized Sweeping*.\n",
    "\n",
    "- This notebook is based on Chapter 8 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html\n",
    "- Documentation for the Maze environment: https://github.com/MattChanTK/gym-maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a40b9c5e-45ff-409b-9a2e-ac13fd69b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install gymnasium==0.29.0\n",
    "    !git clone https://github.com/MattChanTK/gym-maze\n",
    "    !cd gym-maze\n",
    "    !python3 setup.py install\n",
    "    !rm -r gym-maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217b9e4d-1854-405c-9266-803b34915fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_maze\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from gymnasium.wrappers import TransformObservation\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b306b4-6004-406c-90e7-0daa1f17ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_EPISODES = 3  # Number of training episodes\n",
    "N_UPDATES_PER_STEP = 200  # Number of planning updates per interaction with the environment\n",
    "EPSILON_MAX = 1  # Initial exploration\n",
    "EPSILON_MIN = 0.001  # Final exploration\n",
    "EPSILON_DECAY = 2 * EPSILON_MAX / N_EPISODES  # Exploration decay rate\n",
    "ALPHA = 0.7  # Learning rate\n",
    "GAMMA = 1  # Discount factor\n",
    "THETA = 0.01  # Priority treshold\n",
    "N_RECORDINGS = 3  # Number of episodes to record\n",
    "REC_EPISODES = np.linspace(0, N_EPISODES-1, num=N_RECORDINGS, dtype=int)  # Episodes to record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabb6faf-27bb-44e4-98c4-e136f0e4dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize gym environment with gymnasium compatibility\n",
    "base_env = gym.make(\"maze-random-10x10-v0\", apply_api_compatibility=True)\n",
    "# Transform observation representation from array to int: e.g. [3, 4] -> 43\n",
    "base_env = TransformObservation(base_env, lambda obs: int(obs[1] * (env.observation_space.high + 1)[0] + obs[0]))\n",
    "# Wrap environment to record videos throughout the learning process \n",
    "trigger = lambda ep: ep in REC_EPISODES\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f93420e-b1af-4142-b48c-15bb04474f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "action_space_size = env.action_space.n\n",
    "observation_space_size = (env.observation_space.high + 1)[0] * \\\n",
    "    (env.observation_space.high + 1)[1]\n",
    "q_table_shape = observation_space_size, action_space_size\n",
    "q_table = np.zeros(q_table_shape)\n",
    "\n",
    "# Initialize priority 'queue'\n",
    "priorities = np.zeros(q_table_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b36954da-d9e2-4a34-832f-f7133beee555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax function that breaks ties randomly\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c13ee-8ee9-4e3f-8de8-2f5a6b66ae22",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "In addition to the Q-table, the agent also learns a model of the environment. Since the maze is deterministic, the model is pretty simple:\n",
    "for each state-action pair, the model stores the next state and reward: $\\text{Model}(S_t, A_t) = (R_{t+1}, S_{t+1})$. The `add` method\n",
    "is used to add new information to the model, while the `get` method returns the reward and next state for a given state-action pair.\n",
    "The `get_leading` method returns all state-action pairs that lead to a given state: $\\text{get\\_leading}(S_t) = \\{(s, a)\\in\\mathcal{S}\\times\\mathcal{A}\\ |\\ Model(s, a)_2 = S_t\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a59e5c6-abd4-44a6-8363-020a9bb84e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class representing a model of the environment\n",
    "class Model:\n",
    "    def __init__(self, shape):\n",
    "        self.states = np.zeros(shape, dtype=int) - 1\n",
    "        self.rewards = np.zeros(shape)\n",
    "        self.states_reverse = {}\n",
    "\n",
    "    # Add new information to the model:\n",
    "    # Taking 'action' in 'state' produces 'reward'\n",
    "    # and transfers the agent to 'new_state'\n",
    "    def add(self, state, action, reward, new_state):\n",
    "        self.states[state, action] = new_state\n",
    "        self.rewards[state, action] = reward\n",
    "\n",
    "        if new_state in self.states_reverse:\n",
    "            if not (state, action) in self.states_reverse[new_state]:\n",
    "                self.states_reverse[new_state].append((state, action))\n",
    "        else:\n",
    "            self.states_reverse[new_state] = [(state, action)]\n",
    "\n",
    "    # Get information from the model:\n",
    "    def get(self, state, action):\n",
    "        new_state = self.states[state, action]\n",
    "        reward = self.rewards[state, action]\n",
    "        return new_state, reward\n",
    "\n",
    "    # Get all state-action pairs that lead the agent to 'state'\n",
    "    def get_leading(self, state):\n",
    "        return self.states_reverse[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307c573-e7b8-40c0-b234-6e3b52cd1327",
   "metadata": {},
   "source": [
    "## Prioritized Sweeping\n",
    "\n",
    "Prioritized sweeping works similarly to Q-learning, but in addition to updates based on real experience (*learning*) it also utilizes updates based on simulated\n",
    "experience (*planning*). To be able to do this, the algorithm records each state transition, forming a model of the environment.\n",
    "\n",
    "A simpler, similar algorithm is called *Dyna-Q*: it is essentially Q-learning with state transitions being recorded to a model, and after each step, randomly selected\n",
    "Q-values are being updated using information from the model.\n",
    "Prioritized sweeping improves on Dyna-Q by focusing on updating state-action pairs with higher temporal-difference error values more frequently. By prioritizing updates based on the magnitude of the error, it accelerates the learning process by directing attention to the most critical areas of the environment, where value estimates need refinement, leading to quicker convergence.\n",
    "\n",
    "Since state-action pairs that lead to a state with a high error are likely to have a high TD error themselves, after each update, the errors of state-action pairs that lead to the newly updated state are recalculated. This leads to a quick backpropagation of rewards: for example, in the maze environment, after the goal state is reached, prioritized sweeping will\n",
    "first update the state-action pair leading to the goal state, then the state-action pairs leading to the state just before the goal state, and so on, spreading backwards from the\n",
    "goal to the starting state.\n",
    "\n",
    "In this example, the agent learns to solve the maze in only 3 episodes, with 200 planning updates after each step. In real-world applications, learning and planning are usually parallelized: the agent will react instantly to new information while continuously planning in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8ca6b72-6124-4a22-bda1-071e88fadd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan():\n",
    "    for _ in range(N_UPDATES_PER_STEP):\n",
    "        # Get state-action pair with highest priority\n",
    "        state, action = np.unravel_index(argmax(priorities), priorities.shape)\n",
    "        # If the highest priority is 0, there is nothing to do\n",
    "        if priorities[state, action] == 0:\n",
    "            break\n",
    "\n",
    "        # Reset priority\n",
    "        priorities[state, action] = 0\n",
    "\n",
    "        # Get new state and reward from model\n",
    "        new_state, reward = model.get(state, action)\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[state, action] += ALPHA * (reward + GAMMA * np.max(q_table[new_state]) - q_table[obs, action])\n",
    "\n",
    "        # Add leading states to queue\n",
    "        for prev_state, prev_action in model.get_leading(state):\n",
    "            _, prev_reward = model.get(prev_state, prev_action)\n",
    "            priority = abs(prev_reward + GAMMA * np.max(q_table[state] - q_table[prev_state, prev_action]))\n",
    "\n",
    "            if priorities[prev_state, prev_action] < priority and priority > THETA:\n",
    "                priorities[prev_state, prev_action] = priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bfcb921-5953-4911-af60-5042349d20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0:\n",
      "\tSteps: 3205\n",
      "\tQueue size: 65\n",
      "Episode 1:\n",
      "\tSteps: 39\n",
      "\tQueue size: 100\n",
      "Episode 2:\n",
      "\tSteps: 36\n",
      "\tQueue size: 45\n"
     ]
    }
   ],
   "source": [
    "model = Model(q_table_shape)\n",
    "\n",
    "for episode in range(N_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    n = 0\n",
    "    selections = np.zeros(shape)\n",
    "\n",
    "    while not done:\n",
    "        n += 1\n",
    "\n",
    "        # UCB action selection\n",
    "        UCB_estimations = q_table[state, :] + C * np.sqrt(np.log(n) / (selections[state, :] + 1e-5))\n",
    "        action = argmax(UCB_estimations)\n",
    "\n",
    "        selections[state, action] += 1\n",
    "\n",
    "        # take the selected action\n",
    "        new_state, reward, done, truncated, info = env.step(['N', 'S', 'E', 'W'][action])\n",
    "        # add information to the model\n",
    "        model.add(state, action, reward, new_state)\n",
    "\n",
    "        # add state-action pair to queue\n",
    "        priority = abs(reward + GAMMA *\n",
    "                       np.max(q_table[new_state, :] - q_table[state, action]))\n",
    "\n",
    "        if priorities[state, action] < priority and priority > THETA:\n",
    "            priorities[state, action] = priority\n",
    "\n",
    "        state = new_state\n",
    "        plan()\n",
    "\n",
    "    print(f'Episode {episode}:\\n\\tSteps: {n}\\n\\tQueue size: {np.count_nonzero(priorities)}')\n",
    "    \n",
    "# Save the q-table\n",
    "with open('q_table.bin', 'wb') as f:\n",
    "    pickle.dump(q_table, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbd403-025f-47c2-a80a-97de99d3a5df",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "You can watch the videos recorded throughout the training process here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23cef3b-d417-4fe0-9861-b857cad7ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recordings\n",
    "children = [widgets.Video.from_file(f'./videos/rl-video-episode-{episode}.mp4', autoplay=False, loop=False, width=500) for episode in REC_EPISODES]\n",
    "tab = widgets.Tab()\n",
    "tab.children = children\n",
    "titles = tuple([f'Episode {episode + 1:,}' for episode in REC_EPISODES])\n",
    "for i in range(len(children)):\n",
    "    tab.set_title(i, titles[i])\n",
    "display(tab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
