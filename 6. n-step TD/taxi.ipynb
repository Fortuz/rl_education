{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3713bd-52b4-490d-a4ab-edeef02e03d7",
   "metadata": {},
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Created by: **Domonkos Nagy**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/6.%20n-step%20TD/taxi.ipynb)\n",
    "\n",
    "# Taxi\n",
    "\n",
    "The Taxi Problem involves navigating to passengers in a grid world, picking them up and dropping them off at one of four locations.\n",
    "\n",
    "<img src=\"assets/taxi.gif\" width=\"500\"/>\n",
    "\n",
    "There are four designated pick-up and drop-off locations (Red, Green, Yellow and Blue) in the 5x5 grid world. The taxi starts off at a random square and the passenger at one of the designated locations.\n",
    "The goal is move the taxi to the passenger’s location, pick up the passenger, move to the passenger’s desired destination, and drop off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "There are 6 possible actions: movement in any of the 4 directions (up, down, right, left) and picking up and dropping off the passenger. The problem has 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "\n",
    "The rewards are:\n",
    "\n",
    "- -1 per step unless other reward is triggered,\n",
    "\n",
    "- +20 for delivering passenger,\n",
    "\n",
    "- -10 for executing “pickup” and “drop-off” actions illegally.\n",
    "\n",
    "This notebook uses the *$n$-step tree backup algorithm* to approximate the optimal policy in the `Taxi-v3` Gymnasium environment.\n",
    "\n",
    "- This notebook is based on Chapter 7 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html\n",
    "- Documentation for the Taxi environment: https://gymnasium.farama.org/environments/toy_text/taxi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91cbaad-483f-4325-99b1-1bbbdacca010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install gymnasium==0.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0a99fc-88d6-4d26-b123-52a9760705fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03f555-bf45-4196-9951-e67bc60d40dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_EPISODES = 10_000  # Number of training episodes\n",
    "MAX_STEPS_PER_EPISODE = 200  # Number of steps before truncation (there is no truncation in this env by default)\n",
    "N_STEPS = 2  # Number of steps to sample from\n",
    "EPSILON_MAX = 1  # Initial exploration\n",
    "EPSILON_MIN = 0.1  # Final exploration\n",
    "EPSILON_DECAY = 2 * EPSILON_MAX / N_EPISODES  # Exploration decay rate\n",
    "ALPHA = 0.4  # Learning rate\n",
    "GAMMA = 1  # Discount factor\n",
    "N_RECORDINGS = 3  # Number of episodes to record\n",
    "REC_EPISODES = np.linspace(0, N_EPISODES-1, num=N_RECORDINGS, dtype=int)  # Episodes to record\n",
    "LOG_FREQ = N_EPISODES / 10  # Progress log frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b603c9-7a65-4e3c-bba8-590dbe3f5434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create environment\n",
    "base_env = gym.make('Taxi-v3', render_mode='rgb_array')\n",
    "# Wrap environment to record videos throughout the learning process \n",
    "trigger = lambda ep: ep in REC_EPISODES\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7cfb3-bd95-4414-aff7-583ab93cff6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Q-table shape\n",
    "action_space_size = env.action_space.n\n",
    "observation_space_size = env.observation_space.n\n",
    "q_table_shape = observation_space_size, action_space_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7958c-53ad-4654-a625-e76f3ad11a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax function that breaks ties randomly\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])\n",
    "\n",
    "# Epsilon-greedy action selection\n",
    "def select_action(q_table, epsilon, obs):\n",
    "    if np.random.rand() > epsilon:\n",
    "        return argmax(q_table[obs])\n",
    "    else:\n",
    "        return env.action_space.sample()\n",
    "\n",
    "# Convert action-values to state-values\n",
    "def state_value(q_table, obs):\n",
    "    return np.max(q_table[obs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc288b31-1dc4-49a0-ac08-4b826bbe86de",
   "metadata": {},
   "source": [
    "## $n$-step Bootstrapping\n",
    "\n",
    "$n$-step bootstrapping can be seen as a generalization of MC and TD methods. Both of these methods have the same update rule:\n",
    "\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha\\left[G - Q(S_t, A_t)\\right]$$\n",
    "\n",
    "The difference is in $G$, often called the *target* of the update. The target of a MC update at time $t$ is the discounted return\n",
    "from the state $S_t$ up to the terminal state: $G_t^{(MC)} := R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... + \\gamma^{T-t-1} R_{T}$. In the case\n",
    "of TD learning, the target is $G_t^{(TD)} := R_{t+1} + V(S_{t+1})$. While the MC target consists of only sample rewards from the currently simulated\n",
    "episode, the TD target only takes 1 sample reward into account, and then it *bootstraps*, which means that it uses the estimate of the next state to update the estimate\n",
    "of the current one.\n",
    "\n",
    "$n$-step bootstrapping combines these two ideas together: the target for this update is made up by $n$ sample rewards plus the estimate of the successive state's value:\n",
    "\n",
    "$$G_{t:t+n} := R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n})$$\n",
    "\n",
    "### The $n$-step Tree Backup Algorithm\n",
    "\n",
    "An off-policy version of $n$-step bootstrapping that does not utilize importance sampling is called the $n$*-step tree backup algorithm*. This algorithm utilizes\n",
    "a mix of expected updates and sample updates: for all non-selected actions, the estimated values are taken into account (just like an expected update), but for\n",
    "the selected action, the actual next target is added:\n",
    "\n",
    "$$G_{t:t+n} := R_{t+1} + \\gamma \\sum_{a\\ne A_{t+1}} \\pi(a|S_{t+1})Q(S_{t+1},a) + \\gamma \\pi(A_{t+1} | S_{t+1}) G_{t+1:t+n} \\; (\\text{where} \\; G_{T-1:t+n} := R_T)$$\n",
    "\n",
    "The backup diagram for the $n=3$ case provides a nice visual representation of this update rule:\n",
    "\n",
    "<img src=\"assets/backup_diagram.png\" width=\"100\"/>\n",
    "\n",
    "*Backup diagram from page 152 of the Sutton & Barto book*\n",
    "\n",
    "### When $\\pi$ is Deterministic\n",
    "\n",
    "In our case, the target policy $\\pi$ is deterministic, so $\\pi(a|s)\\in\\{0,1\\}$. This means that the update rule can be simplified:\n",
    "\n",
    "$$\n",
    "  G_{t:t+n} :=\n",
    "    \\begin{cases}\n",
    "      R_{t+1} + \\gamma G_{t+1:t+n} & \\text{if $\\pi(S_{t+1}) = A_{t+1}$}\\\\\n",
    "      R_{t+1} + \\gamma Q(S_{t+1},\\pi(S_{t+1})) & \\text{if $\\pi(S_{t+1}) \\ne A_{t+1}$}\n",
    "    \\end{cases},\n",
    "$$\n",
    "\n",
    "where $\\pi(s)$ is the action $a$ for which $\\pi(a|s) = 1$ in state $s$.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement this algorithm! The block below only contains code necessary for logging the average reward at every `LOG_FREQ` iterations. The algorithm itself is up to you! Pseudocode for this algorithm is shown in the box below.\n",
    "\n",
    "<img src=\"assets/n-step_tree_backup.png\" width=\"700\"/>\n",
    "\n",
    "*Pseudocode from page 154 of the Sutton & Barto book*\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- If a hyperparameter is given as an argument, use that, otherwise use the appropriate global constant. This will be important a few blocks below, where this function will be called with different parameters for comparison.\n",
    "- The three helper functions defined above might be useful.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e138c5-33a3-4d0f-9363-5b163656971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(alpha=ALPHA, n_steps=N_STEPS,\n",
    "          n_episodes=N_EPISODES, epsilon_decay=EPSILON_DECAY,\n",
    "          log=True, record=True):\n",
    "    \n",
    "    # Select environment and initalize Q-table\n",
    "    env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True) if record else base_env\n",
    "    q_table = np.zeros(q_table_shape)\n",
    "    episodes = trange(n_episodes) if log else range(n_episodes)\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Training loop\n",
    "    for episode in episodes:\n",
    "        \n",
    "        ############## CODE HERE ###################\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "\n",
    "        # Log results\n",
    "        if log and (episode + 1) % LOG_FREQ == 0:\n",
    "            print(f'Episode {episode + 1} : avg={sum_rewards / LOG_FREQ}')\n",
    "            sum_rewards = 0\n",
    "\n",
    "    # Close environment\n",
    "    env.close()\n",
    "\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053cb08-4331-4afb-b858-27fe3b36ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "q_table = train()\n",
    "\n",
    "# Save Q-table\n",
    "with open('q_table.bin', 'wb') as f:\n",
    "    pickle.dump(q_table, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675063b9-a385-4e0e-ad11-cf37b340ca07",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "You can watch the videos recorded throughout the training process here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97bb505-ca09-4a1f-8857-cd35d87e88bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recordings\n",
    "children = [widgets.Video.from_file(f'./videos/rl-video-episode-{episode}.mp4', autoplay=False, loop=False, width=500) for episode in REC_EPISODES]\n",
    "tab = widgets.Tab()\n",
    "tab.children = children\n",
    "titles = tuple([f'Episode {episode + 1:,}' for episode in REC_EPISODES])\n",
    "for i in range(len(children)):\n",
    "    tab.set_title(i, titles[i])\n",
    "display.display(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ae4e2-03a5-418d-9826-4328d5cd3ea8",
   "metadata": {},
   "source": [
    "## Finding the Optimal $n$\n",
    "\n",
    "As you might expect, there is no universal best value for $n$, it will always depends on the problem. The method will eventually converge to the optimal policy and value function\n",
    "regardless of the choice of $n$, but the speed of convergence can vary greatly depending on this choice. The plot below compares different values of $n$ and $\\alpha$ at different levels\n",
    "of training.\n",
    "\n",
    "**Note**: The process below can take a long time to finish, as it involves repeating the training process many times with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaeb93a-1fb4-4d64-bc1d-a656ac099c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Play n_episodes episodes using a greedy policy w.r.t. the argument Q-table,\n",
    "# and return the average reward\n",
    "def test(q_table, n_episodes=N_EPISODES):\n",
    "    sum_reward = 0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = base_env.reset()\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            # Select action (greedy)\n",
    "            action = argmax(q_table[obs])\n",
    "\n",
    "            # Take step\n",
    "            new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or step >= MAX_STEPS_PER_EPISODE  # Truncating manually\n",
    "\n",
    "            # Store reward and new state\n",
    "            sum_reward += reward\n",
    "            obs = new_obs\n",
    "            step += 1\n",
    "\n",
    "    return sum_reward / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783ba93-5e79-4604-9bfe-e647a050a12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters to compare\n",
    "episode_counts = [100, 500, 1000, 5000]\n",
    "step_counts = [1, 2, 4]\n",
    "alphas = np.linspace(0.1, 1, 4)\n",
    "n_trials = 5\n",
    "\n",
    "# Compare parameters\n",
    "comparison_results = np.zeros((len(episode_counts), len(step_counts), len(alphas)))\n",
    "for t in range(n_trials):\n",
    "    display.clear_output(wait=True)\n",
    "    print(f'Trial {t+1}/{n_trials}')\n",
    "\n",
    "    # For all episode counts\n",
    "    for i, n_episodes in enumerate(tqdm(episode_counts)):\n",
    "        # For all values of n\n",
    "        for j, n_steps in enumerate(tqdm(step_counts)):\n",
    "            # For all values of alpha\n",
    "            for k, alpha in enumerate(alphas):\n",
    "                # Train\n",
    "                epsilon_decay = 2 * EPSILON_MAX / n_episodes\n",
    "                q_table = train(alpha=alpha, n_steps=n_steps,\n",
    "                                n_episodes=n_episodes, epsilon_decay=epsilon_decay,\n",
    "                                log=False, record=False)\n",
    "\n",
    "                # Test\n",
    "                comparison_results[i, j, k] += test(q_table, 100)\n",
    "\n",
    "# Average performance over n_trials\n",
    "comparison_results = comparison_results / n_trials\n",
    "\n",
    "# Save results\n",
    "with open('comparison_results.bin', 'wb') as f:\n",
    "    pickle.dump(comparison_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd108d7b-029f-46d1-9aa6-fc60d1ec9f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open('comparison_results.bin', 'rb') as f:\n",
    "    comparison_results = pickle.load(f)\n",
    "\n",
    "# Set up plot\n",
    "nrows = math.ceil(len(step_counts) / 2)\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=2, figsize=(10, 3*nrows))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot results\n",
    "for i, n_episodes in enumerate(episode_counts):\n",
    "    ax = axs[i]\n",
    "    ax.set_title(f'Trained on {n_episodes:,} episodes')\n",
    "\n",
    "    for j, n_steps in enumerate(step_counts):\n",
    "        ax.plot(alphas, comparison_results[i, j, :], label=f'$n={n_steps}$',\n",
    "                zorder=2)\n",
    "\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylabel('Avg. reward')\n",
    "    ax.set_xlabel('$\\\\alpha$')\n",
    "    ax.set_xticks(alphas)\n",
    "    ax.grid(alpha=0.8, linestyle=':', zorder=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
