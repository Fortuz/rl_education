{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchrl\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchrl.envs import GymEnv\n",
    "from torchrl.modules import MLP, ProbabilisticActor, ValueOperator\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "from torchrl.objectives import DQNLoss, ClipPPOLoss\n",
    "from torchrl.trainers import make_trainer\n",
    "\n",
    "# Introduction to TorchRL\n",
    "print(\"TorchRL Version:\", torchrl.__version__)\n",
    "\n",
    "# Environment setup\n",
    "env_name = \"CartPole-v1\"\n",
    "env = GymEnv(env_name)\n",
    "state_dim = env.observation_spec.shape[0]\n",
    "action_dim = env.action_spec.shape[0] if env.action_spec.ndimension() > 0 else env.action_spec.shape\n",
    "\n",
    "# Exploring the environment\n",
    "def explore_env(env, episodes=5):\n",
    "    \"\"\" Run a few episodes to explore the environment and observe its behavior.\"\"\"\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            action = env.action_spec.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        print(f\"Episode {episode+1}: Total Reward: {total_reward}\")\n",
    "\n",
    "explore_env(env)\n",
    "\n",
    "# Define Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc = MLP(in_features=state_dim, hidden_sizes=[hidden_size, hidden_size], out_features=action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = self.fc(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "# Define Value Network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc = MLP(in_features=state_dim, hidden_sizes=[hidden_size, hidden_size], out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize Networks\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "value_net = ValueNetwork(state_dim)\n",
    "\n",
    "# Create actor and critic operators\n",
    "actor = ProbabilisticActor(policy, action_spec=env.action_spec, distribution_class=torch.distributions.Normal)\n",
    "critic = ValueOperator(value_net)\n",
    "\n",
    "# Define Loss Functions\n",
    "ppo_loss = ClipPPOLoss(actor, critic, clip_epsilon=0.2, entropy_bonus=True)\n",
    "dqn_loss = DQNLoss(critic, gamma=0.99)\n",
    "\n",
    "# Set up Optimizers\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=3e-4)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Set up Replay Buffer\n",
    "buffer = TensorDictReplayBuffer(size=100000)\n",
    "\n",
    "# Define Trainer for PPO\n",
    "ppo_trainer = make_trainer(\n",
    "    loss_module=ppo_loss,\n",
    "    env=env,\n",
    "    buffer=buffer,\n",
    "    optimizer=[policy_optimizer, value_optimizer],\n",
    "    batch_size=64,\n",
    "    max_epochs=500,\n",
    ")\n",
    "\n",
    "# Train PPO Model\n",
    "ppo_trainer.train()\n",
    "\n",
    "# Evaluating the trained model\n",
    "def evaluate_agent(env, policy, episodes=10):\n",
    "    \"\"\" Evaluate the trained policy in the environment.\"\"\"\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            mean, _ = policy(state_tensor)\n",
    "            action = mean.detach().numpy()\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Evaluation Episode {episode+1}: Reward = {episode_reward}\")\n",
    "    print(f\"Average Reward: {np.mean(total_rewards)}\")\n",
    "\n",
    "evaluate_agent(env, policy)\n",
    "\n",
    "# Plot Training Results\n",
    "plt.plot(ppo_trainer.episode_rewards)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"PPO Training Performance with TorchRL\")\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"TorchRL provides a modular and efficient way to implement RL algorithms, integrating seamlessly with PyTorch.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
