{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "# Actor-Critic Network Definition\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor_fc = nn.Linear(hidden_size, action_dim)\n",
    "        self.actor_softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic_fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        action_probs = self.actor_softmax(self.actor_fc(x))\n",
    "        state_value = self.critic_fc(x)\n",
    "        return action_probs, state_value\n",
    "\n",
    "# A2C Algorithm Implementation\n",
    "class A2C:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.01, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.actor_critic = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        action_probs, _ = self.actor_critic(state)\n",
    "        action_dist = torch.distributions.Categorical(action_probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item(), action_dist.log_prob(action)\n",
    "    \n",
    "    def update_policy(self, rewards, log_probs, state_values):\n",
    "        G = 0\n",
    "        policy_loss = []\n",
    "        value_loss = []\n",
    "        for log_prob, value, reward in zip(reversed(log_probs), reversed(state_values), reversed(rewards)):\n",
    "            G = reward + self.gamma * G\n",
    "            advantage = G - value.item()\n",
    "            policy_loss.append(-log_prob * advantage)\n",
    "            value_loss.append(nn.functional.mse_loss(value, torch.tensor([G], dtype=torch.float32)))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Train A2C on CartPole-v1\n",
    "def train_a2c(env_name='CartPole-v1', episodes=1000):\n",
    "    env = gym.make(env_name)\n",
    "    agent = A2C(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n)\n",
    "    reward_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        state_values = []\n",
    "        \n",
    "        for t in range(200):\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            _, state_value = agent.actor_critic(torch.tensor(state, dtype=torch.float32))\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            state_values.append(state_value)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.update_policy(rewards, log_probs, state_values)\n",
    "        reward_history.append(episode_reward)\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "    return reward_history\n",
    "\n",
    "# Run training and plot results\n",
    "reward_history = train_a2c()\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"A2C Training Performance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.multiprocessing as mp\n",
    "from collections import deque\n",
    "\n",
    "# Actor-Critic Network Definition\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor_fc = nn.Linear(hidden_size, action_dim)\n",
    "        self.actor_softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic_fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        action_probs = self.actor_softmax(self.actor_fc(x))\n",
    "        state_value = self.critic_fc(x)\n",
    "        return action_probs, state_value\n",
    "\n",
    "# Worker process for A3C\n",
    "def worker(worker_id, global_model, optimizer, env_name, gamma, episodes, results):\n",
    "    env = gym.make(env_name)\n",
    "    local_model = ActorCritic(env.observation_space.shape[0], env.action_space.n)\n",
    "    local_model.load_state_dict(global_model.state_dict())\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        state_values = []\n",
    "        \n",
    "        for t in range(200):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            action_probs, state_value = local_model(state_tensor)\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            log_probs.append(action_dist.log_prob(action))\n",
    "            state_values.append(state_value)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Compute returns and update global model\n",
    "        G = 0\n",
    "        policy_loss = []\n",
    "        value_loss = []\n",
    "        for log_prob, value, reward in zip(reversed(log_probs), reversed(state_values), reversed(rewards)):\n",
    "            G = reward + gamma * G\n",
    "            advantage = G - value.item()\n",
    "            policy_loss.append(-log_prob * advantage)\n",
    "            value_loss.append(nn.functional.mse_loss(value, torch.tensor([G], dtype=torch.float32)))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "        loss.backward()\n",
    "        for global_param, local_param in zip(global_model.parameters(), local_model.parameters()):\n",
    "            global_param.grad = local_param.grad\n",
    "        optimizer.step()\n",
    "        \n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        results[worker_id].append(episode_reward)\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Train A3C on CartPole-v1\n",
    "def train_a3c(env_name='CartPole-v1', episodes=1000, num_workers=4):\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    env.close()\n",
    "    \n",
    "    global_model = ActorCritic(state_dim, action_dim)\n",
    "    global_model.share_memory()\n",
    "    optimizer = optim.Adam(global_model.parameters(), lr=0.001)\n",
    "    gamma = 0.99\n",
    "    \n",
    "    results = [mp.Manager().list() for _ in range(num_workers)]\n",
    "    processes = []\n",
    "    for worker_id in range(num_workers):\n",
    "        p = mp.Process(target=worker, args=(worker_id, global_model, optimizer, env_name, gamma, episodes // num_workers, results))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    "    reward_history = [reward for worker_rewards in results for reward in worker_rewards]\n",
    "    return reward_history\n",
    "\n",
    "# Run training and plot results\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method('spawn')\n",
    "    reward_history = train_a3c()\n",
    "    plt.plot(reward_history)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"A3C Training Performance\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
