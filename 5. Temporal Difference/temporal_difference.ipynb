{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Zoltán Barta**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/5.%20Temporal%20Difference/temporal_difference.ipynb)\n",
    "\n",
    "- This notebook is based on Chapter 6 and 7 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning: TD(N), SARSA, and Q-learning\n",
    "\n",
    "## Introduction\n",
    "Reinforcement Learning (RL) is a fundamental approach for training intelligent agents to interact with an environment and learn optimal behaviors. In this notebook, we will explore **Temporal Difference (TD) learning**, a key concept in RL that allows agents to learn **value functions** without needing a complete model of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "import random\n",
    "import gym\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "from collections import defaultdict,deque\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# Check Gym version\n",
    "print(f\"Using Gym version: {gym.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some utility functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(\n",
    "    data_dict: Dict[str, List[float]], \n",
    "    window_size: int, \n",
    "    show_original: bool = True, \n",
    "    clip_value: float = -500\n",
    ") -> None:\n",
    "    \"\"\"Computes and plots the moving average for multiple algorithms, with optional clipping.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict (Dict[str, List[float]]): A dictionary where keys are algorithm names and values are lists of rewards.\n",
    "    - window_size (int): Number of samples for the moving average.\n",
    "    - show_original (bool, optional): Whether to display the original data (dotted line). Default is True.\n",
    "    - clip_value (float, optional): Minimum value to clip the data to. Default is -500.\n",
    "    \"\"\"\n",
    "    if window_size < 1:\n",
    "        raise ValueError(\"Window size must be at least 1\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for algo, data in data_dict.items():\n",
    "        data = np.array(data)\n",
    "\n",
    "        # Clip values at the specified threshold\n",
    "        data = np.clip(data, clip_value, None)\n",
    "\n",
    "        # Compute moving average\n",
    "        moving_avg = np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "        # Plot original data if enabled\n",
    "        if show_original:\n",
    "            plt.plot(data, linestyle='dotted', alpha=0.4, label=f\"{algo} (Original)\")\n",
    "\n",
    "        # Plot moving average\n",
    "        plt.plot(range(window_size - 1, len(data)), moving_avg, label=f\"{algo}\")\n",
    "\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.title(f\"Moving Average ({window_size}-Point) with Clipping at {clip_value}\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state: int | Iterable) -> Tuple:\n",
    "    \"\"\"Converts a given state into a tuple format.\n",
    "\n",
    "    Parameters:\n",
    "    - state (int | Iterable): The state to be discretized.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple: The discretized state representation.\n",
    "    \"\"\"\n",
    "    if isinstance(state, int):\n",
    "        return (state,)\n",
    "    else:\n",
    "        return tuple(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(values: List[float]) -> int:\n",
    "    \"\"\"Returns the index of the maximum value in the list.\n",
    "    If multiple values have the same maximum, randomly selects one.\n",
    "\n",
    "    Parameters:\n",
    "    - values (List[float]): A list of values.\n",
    "\n",
    "    Returns:\n",
    "    - int: The index of the maximum value.\n",
    "    \"\"\"\n",
    "    max_val = max(values)\n",
    "    return np.random.choice([a for a, v in enumerate(values) if v == max_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cliff Walking: A Classic Reinforcement Learning Problem**  \n",
    "\n",
    "#### **Introduction**  \n",
    "Cliff Walking is a widely used **grid-world** reinforcement learning environment, often used to demonstrate the performance of different RL algorithms like **SARSA**, **Q-learning**, and **Monte Carlo methods**. The problem is inspired by **the windy gridworld**, where an agent must navigate through a grid while avoiding a deadly cliff.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Environment Setup**  \n",
    "The environment consists of a **4x12 grid**, where:\n",
    "- **Start State**: The agent begins at the bottom-left corner `(3,0)`.\n",
    "- **Goal State**: The agent's objective is to reach the bottom-right corner `(3,11)`.\n",
    "- **Cliff Region**: The entire row between `(3,1)` and `(3,10)` is considered a cliff.\n",
    "- **Actions**: The agent can move in **four directions**: **left, right, up, or down**.\n",
    "- **Rewards**:  \n",
    "  - Each step incurs a **reward of -1**.\n",
    "  - If the agent falls off the cliff, it receives a **reward of -100** and is sent back to the start.\n",
    "  - If the agent reaches the goal, the episode ends.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Learning Objective**  \n",
    "- The agent must learn the **optimal policy** that minimizes total negative reward while avoiding the cliff.  \n",
    "- **Exploration vs. Exploitation**: Since the cliff gives a high penalty, the agent needs to balance trying new paths (**exploration**) and sticking to learned safe routes (**exploitation**).  \n",
    "- **Algorithmic Comparison**:  \n",
    "  - **SARSA (On-policy control)**: Learns a **conservative** policy that avoids the cliff but is suboptimal.  \n",
    "  - **Q-learning (Off-policy control)**: Learns the **optimal policy** but can take risky steps near the cliff.  \n",
    "  - **Monte Carlo methods**: Requires full episodes but converges to a stable policy over time.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Policy for Cliff Walking\n",
    "\n",
    "Monte Carlo reinforcement learning methods estimate optimal policies by averaging returns over complete episodes without relying on a model of the environment. The `MonteCarloCliffWalking` class implements **Monte Carlo policy iteration** for the **CliffWalking** environment, using an **ε-soft policy** to ensure sufficient exploration. The algorithm collects full episodes, updates the **Q-value estimates** using first-visit Monte Carlo, and gradually improves the action-value function based on observed returns. This approach is particularly effective for **episodic tasks**, where learning is based on complete trajectories rather than step-by-step updates. MC methods rely on **sampling whole episodes** to approximate expected returns.  \n",
    "\n",
    "\n",
    "![MC](assets/MC.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloCliffWalking:\n",
    "    \"\"\"Implements Monte Carlo policy iteration for CliffWalking.\"\"\"\n",
    "\n",
    "    def __init__(self, env, gamma: float = 0.9, epsilon: float = 0.1, episodes: int = 5000):\n",
    "        \"\"\"\n",
    "        Initializes Monte Carlo policy iteration for CliffWalking.\n",
    "\n",
    "        Parameters:\n",
    "        - env: OpenAI Gym CliffWalking-v0 environment.\n",
    "        - gamma (float): Discount factor.\n",
    "        - epsilon (float): Exploration rate for ε-greedy policy.\n",
    "        - episodes (int): Number of training episodes.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.episodes = episodes\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.q_table = defaultdict(lambda: np.zeros(self.n_actions))  # Q(s, a)\n",
    "        self.returns = defaultdict(list)  # Stores all returns per (state, action)\n",
    "\n",
    "    def generate_episode(self, policy) -> Tuple[List[Tuple[int, int, float]], float]:\n",
    "        \"\"\"Generates an episode using the current policy and returns the sequence.\n",
    "\n",
    "        Parameters:\n",
    "        - policy: The policy function.\n",
    "\n",
    "        Returns:\n",
    "        - List[Tuple[int, int, float]]: The episode as (state, action, reward) tuples.\n",
    "        - float: The total reward collected in the episode.\n",
    "        \"\"\"\n",
    "        episode = []\n",
    "        state = self.env.reset()[0]  # Get initial state\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return episode, total_reward\n",
    "\n",
    "    def update_q_values(self, episode: List[Tuple[int, int, float]]) -> None:\n",
    "        \"\"\"Performs first-visit Monte Carlo update for Q-table.\n",
    "\n",
    "        Parameters:\n",
    "        - episode (List[Tuple[int, int, float]]): List of (state, action, reward) transitions.\n",
    "        \"\"\"\n",
    "        G = 0  # Return (discounted sum of rewards)\n",
    "        visited = set()  # Track first visit\n",
    "\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = reward + self.gamma * G  # Discounted return\n",
    "\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                self.returns[(state, action)].append(G)\n",
    "                self.q_table[state][action] = np.mean(self.returns[(state, action)])  # Update Q\n",
    "\n",
    "    def policy(self, state: int) -> int:\n",
    "        \"\"\"ε-soft policy for action selection.\n",
    "    \n",
    "        Parameters:\n",
    "        - state (int): The current state.\n",
    "    \n",
    "        Returns:\n",
    "        - int: The action to take.\n",
    "        \"\"\"\n",
    "        action_probabilities = np.ones(self.n_actions) * (self.epsilon / self.n_actions)  # Base probability\n",
    "        best_action = np.argmax(self.q_table[state])\n",
    "        action_probabilities[best_action] += (1 - self.epsilon)  # Assign extra probability to the best action\n",
    "        return np.random.choice(np.arange(self.n_actions), p=action_probabilities)\n",
    "\n",
    "    def train(self) -> List[float]:\n",
    "        \"\"\"Trains the Monte Carlo agent and returns training rewards.\n",
    "\n",
    "        Returns:\n",
    "        - List[float]: The rewards per episode.\n",
    "        \"\"\"\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        for _ in range(self.episodes):\n",
    "            episode, total_reward = self.generate_episode(self.policy)\n",
    "            self.update_q_values(episode)\n",
    "            rewards_per_episode.append(total_reward)  # Store episode reward\n",
    "        print(\"Training done!\")\n",
    "        return rewards_per_episode\n",
    "\n",
    "    def get_optimal_policy(self) -> Dict[int, int]:\n",
    "        \"\"\"Extracts the optimal policy from the Q-table.\n",
    "\n",
    "        Returns:\n",
    "        - Dict[int, int]: Mapping of state to best action.\n",
    "        \"\"\"\n",
    "        policy = {}\n",
    "        for state in self.q_table.keys():\n",
    "            policy[state] = np.argmax(self.q_table[state])\n",
    "        return policy\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Monte-Carlo (ε={self.epsilon}, γ={self.gamma})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Monte Carlo agent\n",
    "mc_agent = MonteCarloCliffWalking(env, gamma=0.9, epsilon=0.1, episodes=5000)\n",
    "\n",
    "# Train the agent and collect rewards\n",
    "training_rewards = mc_agent.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average({\"Monte Carlo\": training_rewards}, window_size=20, show_original=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Monte Carlo Methods  \n",
    "While Monte Carlo methods are effective in **episodic tasks**, they suffer from a major limitation:  \n",
    "- **Delayed Learning**: Monte Carlo methods **only update value estimates at the end of an episode**. This makes them **inefficient** in environments where episodes are long or rarely terminate.  \n",
    "- **No Online Learning**: Since updates occur only after an episode concludes, MC methods **cannot adapt immediately** to new experiences, making them slow to respond to dynamic environments.  \n",
    "- **High Variance**: Because each episode may vary significantly, Monte Carlo estimates often have **high variance**, leading to instability in learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How could we improve the efficiency?\n",
    "\n",
    "To address these issues, **Temporal Difference (TD) Learning** introduces an alternative approach:  \n",
    "- **TD(0) Learning** updates value estimates **after each step** rather than waiting for the full episode to end.  \n",
    "- This allows **online learning**, where the agent improves its estimates continuously during interaction with the environment.  \n",
    "- TD methods strike a balance between **Monte Carlo** (which waits until the episode ends) and **Dynamic Programming** (which requires a full model of the environment).  \n",
    "\n",
    "TD(0) is the simplest form of temporal difference learning and serves as the foundation for more advanced algorithms like **SARSA** and **Q-learning**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(0) Update Rule\n",
    "\n",
    "The Temporal Difference (TD(0)) update rule is defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ r_{t+1} + \\gamma V(s_{t+1}) - V(s_t) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $V(s_t)$: The current estimate of the value function for state $s_t$.\n",
    "- $\\alpha$: The learning rate, controlling the step size of updates.\n",
    "- $r_{t+1}$: The reward received after transitioning from $s_t$ to $s_{t+1}$.\n",
    "- $\\gamma$: The discount factor, weighting the importance of future rewards.\n",
    "- $V(s_{t+1})$: The estimated value of the next state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"width:50%; height:auto; overflow:hidden; position:relative;\">\n",
    "    <img src=\"assets/backup_diagram.png\" style=\"position:relative; left:-50%; width:200%;\">\n",
    "</div>\n",
    "\n",
    "[Source](https://www.researchgate.net/publication/366838727_An_intelligent_resource_management_method_in_SDN_based_fog_computing_using_reinforcement_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"\n",
    "    An epsilon-greedy policy for reinforcement learning.\n",
    "\n",
    "    This policy selects actions using an ε-greedy approach, balancing exploration and exploitation.\n",
    "    It maintains a Q-table for action-value estimates and allows updating and selecting actions.\n",
    "\n",
    "    Attributes:\n",
    "    - epsilon (float): The probability of selecting a random action for exploration.\n",
    "    - action_space_size (int): The total number of possible actions.\n",
    "    - q_table (Dict[Tuple, List[float]]): A dictionary mapping states to action-value estimates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space_size: int, epsilon: float):\n",
    "        \"\"\"\n",
    "        Initializes the epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        - action_space_size (int): The number of available actions.\n",
    "        - epsilon (float): The probability of selecting a random action (exploration rate).\n",
    "        \"\"\"\n",
    "        self.epsilon: float = epsilon \n",
    "        self.action_space_size: int = action_space_size\n",
    "        self.q_table: Dict[Tuple, List[float]] = defaultdict(lambda: np.zeros(action_space_size))\n",
    "\n",
    "    def __call__(self, state) -> int:\n",
    "        \"\"\"\n",
    "        Selects an action using the epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - int: The action to take.\n",
    "        \"\"\"\n",
    "        state = discretize_state(state)\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.randint(0, self.action_space_size - 1)  # Explore\n",
    "        else:\n",
    "            return argmax(self.q_table[state])  # Exploit\n",
    "\n",
    "    def update_and_select_action(self, state: int | Iterable, action: int, reward: float, next_state: int | Iterable,done:bool) -> int:\n",
    "        \"\"\"\n",
    "        Updates the policy (if necessary) and selects an action for the next state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (int | Iterable): The current state.\n",
    "        - action (int): The action taken.\n",
    "        - reward (float): The reward received for the action.\n",
    "        - next_state (int | Iterable): The next state after taking the action.\n",
    "\n",
    "        Returns:\n",
    "        - int: The next action to take.\n",
    "        \"\"\"\n",
    "        return self(next_state)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns a string representation of the policy.\n",
    "\n",
    "        Returns:\n",
    "        - str: A description of the policy.\n",
    "        \"\"\"\n",
    "        return \"Random Policy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA: On-Policy Temporal Difference Learning\n",
    "\n",
    "## Overview  \n",
    "SARSA (State-Action-Reward-State-Action) is an **on-policy** reinforcement learning algorithm that updates the Q-value of a state-action pair based on the action taken according to the current policy. Unlike **Q-learning**, which uses the maximum possible future reward (off-policy), SARSA follows the policy's actual behavior when updating values.\n",
    "\n",
    "## Update rule\n",
    "At each time step \\( t \\), SARSA updates the Q-value using the following rule:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q(s_t, a_t)$ is the estimated value of taking action $a_t$ in state $s_t$.\n",
    "- $\\alpha$ is the learning rate, controlling the step size of updates.\n",
    "- $r_{t+1}$ is the reward received after taking action $a_t$.\n",
    "- $\\gamma$ is the discount factor, weighting the importance of future rewards.\n",
    "- $Q(s_{t+1}, a_{t+1})$ is the value of the next state-action pair, chosen by the current policy.\n",
    "\n",
    "## Key Characteristics  \n",
    "- **On-Policy Learning:** The update is based on the action actually taken by the agent, making SARSA more conservative compared to Q-learning.\n",
    "- **Exploration Sensitivity:** Since it follows its own policy, SARSA naturally integrates exploration strategies like **ε-greedy**.\n",
    "- **Smooth Learning Curve:** SARSA tends to learn safer policies in environments with high penalties, as it accounts for exploratory actions during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SARSA](assets/SARSA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(Policy):\n",
    "    \"\"\"\n",
    "    Implements the SARSA (State-Action-Reward-State-Action) reinforcement learning algorithm.\n",
    "\n",
    "    This class inherits from the `Policy` class and updates the Q-values using the SARSA update rule.\n",
    "    It follows an epsilon-greedy policy for action selection and updates Q-values based on the expected\n",
    "    return of the next state-action pair.\n",
    "\n",
    "    Attributes:\n",
    "    - alpha (float): The learning rate, controlling the step size for Q-value updates.\n",
    "    - gamma (float): The discount factor, representing the importance of future rewards.\n",
    "    - epsilon (float): The probability of selecting a random action (exploration rate).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space_size: int, epsilon: float = 0.1, alpha: float = 0.1, gamma: float = 0.9):\n",
    "        \"\"\"\n",
    "        Initializes the SARSA algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - action_space_size (int): The number of available actions.\n",
    "        - epsilon (float): The probability of selecting a random action (exploration rate).\n",
    "        - alpha (float): The learning rate for Q-value updates.\n",
    "        - gamma (float): The discount factor for future rewards.\n",
    "        \"\"\"\n",
    "        super().__init__(action_space_size, epsilon=epsilon)\n",
    "        self.alpha: float = alpha\n",
    "        self.gamma: float = gamma\n",
    "        self.epsilon: float = epsilon\n",
    "\n",
    "    def update_and_select_action(self, state: int | Iterable, action: int, reward: float, next_state: int | Iterable,done:bool) -> int:\n",
    "        \"\"\"\n",
    "        Updates the Q-table using the SARSA update rule and selects the next action.\n",
    "\n",
    "        Parameters:\n",
    "        - state (int | Iterable): The current state.\n",
    "        - action (int): The action taken.\n",
    "        - reward (float): The reward received for the action.\n",
    "        - next_state (int | Iterable): The next state after taking the action.\n",
    "\n",
    "        Returns:\n",
    "        - int: The next action to take based on the updated Q-values.\n",
    "        \"\"\"\n",
    "        state = discretize_state(state)\n",
    "        next_state = discretize_state(next_state)\n",
    "        next_action = self(next_state)\n",
    "\n",
    "        td_target = reward + self.gamma * self.q_table[next_state][next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.alpha * td_error\n",
    "\n",
    "        return next_action\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns a string representation of the SARSA algorithm.\n",
    "\n",
    "        Returns:\n",
    "        - str: A formatted string showing the SARSA parameters.\n",
    "        \"\"\"\n",
    "        return f\"SARSA (ε={self.epsilon}, α={self.alpha}, γ={self.gamma})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected SARSA: An Improvement Over SARSA\n",
    "\n",
    "## Overview  \n",
    "Expected SARSA is a refinement of the standard **SARSA algorithm** that incorporates the expected value of the next Q-values instead of relying on a single sampled action. This approach **reduces variance** while maintaining the on-policy nature of SARSA, leading to more stable learning.\n",
    "\n",
    "## How Expected SARSA Works  \n",
    "At each time step \\( t \\), Expected SARSA updates the Q-value using the following rule:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\mathbb{E}_{a_{t+1} \\sim \\pi} Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q(s_t, a_t)$ is the estimated value of taking action $a_t$ in state $s_t$.\n",
    "- $\\alpha$ is the learning rate, controlling the step size of updates.\n",
    "- $r_{t+1}$ is the reward received after taking action $a_t$.\n",
    "- $\\gamma$ is the discount factor, weighting the importance of future rewards.\n",
    "- $\\mathbb{E}_{a_{t+1} \\sim \\pi} Q(s_{t+1}, a_{t+1})$ is the **expected Q-value** over all possible actions in state $s_{t+1}$, weighted by the action selection probabilities.\n",
    "\n",
    "## Key Differences from SARSA  \n",
    "- **Expectation Over Next Actions:** Instead of using a single action $a_{t+1}$, Expected SARSA **computes the weighted sum** of all possible Q-values based on the current policy.\n",
    "- **Lower Variance:** By averaging over possible future actions, Expected SARSA reduces fluctuations caused by **highly variable rewards** in stochastic environments.\n",
    "- **Smoother Convergence:** Learning is more stable because updates are based on a distribution rather than a single sampled action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSARSA(Policy):\n",
    "    \"\"\"\n",
    "    Implements the Expected SARSA reinforcement learning algorithm.\n",
    "\n",
    "    Expected SARSA is a variation of the SARSA algorithm that updates Q-values based on the \n",
    "    expected value of the next state's Q-values under the current policy, rather than using \n",
    "    the actual next action taken. This leads to a smoother learning process and more stable convergence.\n",
    "\n",
    "    Attributes:\n",
    "    - alpha (float): The learning rate, controlling the step size for Q-value updates.\n",
    "    - gamma (float): The discount factor, representing the importance of future rewards.\n",
    "    - epsilon (float): The probability of selecting a random action (exploration rate).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space_size: int, epsilon: float = 0.1, alpha: float = 0.1, gamma: float = 0.9):\n",
    "        \"\"\"\n",
    "        Initializes the Expected SARSA algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - action_space_size (int): The number of available actions.\n",
    "        - epsilon (float): The probability of selecting a random action (exploration rate).\n",
    "        - alpha (float): The learning rate for Q-value updates.\n",
    "        - gamma (float): The discount factor for future rewards.\n",
    "        \"\"\"\n",
    "        super().__init__(action_space_size, epsilon=epsilon)\n",
    "        self.alpha: float = alpha\n",
    "        self.gamma: float = gamma\n",
    "        self.epsilon: float = epsilon\n",
    "\n",
    "    def get_expected_q(self, state: int | Iterable) -> float:\n",
    "        \"\"\"\n",
    "        Computes the expected Q-value for a given state under the current policy.\n",
    "\n",
    "        This is done by calculating the weighted sum of Q-values, where the weights \n",
    "        correspond to the probability of selecting each action based on the ε-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        - state (int | Iterable): The state for which to compute the expected Q-value.\n",
    "\n",
    "        Returns:\n",
    "        - float: The expected Q-value for the given state.\n",
    "        \"\"\"\n",
    "        state = discretize_state(state)\n",
    "        policy_probs = np.ones(self.action_space_size) * (self.epsilon / self.action_space_size)\n",
    "        best_action = np.argmax(self.q_table[state])\n",
    "        policy_probs[best_action] += (1.0 - self.epsilon)\n",
    "        return np.dot(self.q_table[state], policy_probs)\n",
    "\n",
    "    def update_and_select_action(self, state: int | Iterable, action: int, reward: float, next_state: int | Iterable,done:bool) -> int:\n",
    "        \"\"\"\n",
    "        Updates the Q-table using the Expected SARSA update rule and selects the next action.\n",
    "\n",
    "        Unlike standard SARSA, this update rule uses the expected Q-value of the next state \n",
    "        rather than the Q-value of the actual next action.\n",
    "\n",
    "        Parameters:\n",
    "        - state (int | Iterable): The current state.\n",
    "        - action (int): The action taken.\n",
    "        - reward (float): The reward received for the action.\n",
    "        - next_state (int | Iterable): The next state after taking the action.\n",
    "\n",
    "        Returns:\n",
    "        - int: The next action to take based on the updated Q-values.\n",
    "        \"\"\"\n",
    "        state = discretize_state(state)\n",
    "        next_state = discretize_state(next_state)\n",
    "\n",
    "        expected_q = self.get_expected_q(next_state)\n",
    "\n",
    "        td_target = reward + self.gamma * expected_q\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.alpha * td_error\n",
    "\n",
    "        return self(next_state)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns a string representation of the Expected SARSA algorithm.\n",
    "\n",
    "        Returns:\n",
    "        - str: A formatted string showing the Expected SARSA parameters.\n",
    "        \"\"\"\n",
    "        return f\"ExpectedSARSA (ε={self.epsilon}, α={self.alpha}, γ={self.gamma})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning: Off-Policy Temporal Difference Learning\n",
    "\n",
    "## Overview  \n",
    "Q-Learning is a **model-free, off-policy** reinforcement learning algorithm that estimates the optimal **action-value function** by learning from **maximized future rewards** rather than the agent’s actual behavior. Unlike **SARSA**, which follows the current policy, Q-learning **optimizes independently** of the agent’s exploration, making it more aggressive in finding optimal strategies.\n",
    "\n",
    "## How Q-Learning Works  \n",
    "At each time step \\( t \\), Q-learning updates the Q-value using the following rule:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q(s_t, a_t)$ is the estimated value of taking action $a_t$ in state $s_t$.\n",
    "- $\\alpha$ is the learning rate, controlling the step size of updates.\n",
    "- $r_{t+1}$ is the reward received after taking action $a_t$.\n",
    "- $\\gamma$ is the discount factor, weighting the importance of future rewards.\n",
    "- $\\max_{a} Q(s_{t+1}, a)$ represents the **maximum estimated Q-value** of the next state, assuming the best possible action.\n",
    "\n",
    "## Key Characteristics  \n",
    "- **Off-Policy Learning:** Q-learning updates its Q-values using the **greedy** action selection in the update step, regardless of the agent’s actual behavior.\n",
    "- **More Optimistic Learning:** Since it assumes the best action will always be taken, it learns faster in deterministic environments but can be unstable in stochastic settings.\n",
    "- **Exploration via ε-Greedy:** While updates are based on **greedy action selection**, exploration can still be encouraged using an **ε-greedy** action selection strategy.\n",
    "\n",
    "## Q-Learning vs. SARSA vs. Expected SARSA  \n",
    "| Feature             | Q-Learning (Off-Policy) | SARSA (On-Policy) | Expected SARSA (On-Policy) |\n",
    "|---------------------|------------------|-----------------|------------------|\n",
    "| Policy Type        | Off-policy (greedy) | On-policy (follows its own updates) | On-policy (soft policy) |\n",
    "| Update Method      | Uses the max Q-value of the next state | Uses the Q-value of the next sampled action | Expected value over all actions |\n",
    "| Variance           | Moderate (depends on environment) | Higher (single action sample) | Lower (averages over actions) |\n",
    "| Learning Stability | Can be unstable but finds the optimal policy | More stable but may learn safer policies | More stable due to expectation calculation |\n",
    "| Exploration Handling | Assumes greedy action selection | Explicitly follows $\\epsilon$-greedy exploration | Naturally accounts for stochastic policies |\n",
    "\n",
    "Q-learning is widely used in **reinforcement learning applications** due to its ability to learn the **optimal policy** independently of the agent’s actual behavior. However, it can be unstable in **stochastic environments**, making techniques like **Double Q-Learning** and **Deep Q Networks (DQN)** essential for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!![SARSA](assets/QLearn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(Policy):\n",
    "    \"\"\"\n",
    "    Implements the Q-Learning reinforcement learning algorithm.\n",
    "\n",
    "    Q-Learning is an off-policy learning algorithm that updates the Q-values based on the \n",
    "    maximum future reward possible from the next state. It follows an epsilon-greedy policy \n",
    "    for action selection.\n",
    "\n",
    "    Attributes:\n",
    "    - alpha (float): The learning rate, controlling the step size for Q-value updates.\n",
    "    - gamma (float): The discount factor, representing the importance of future rewards.\n",
    "    - epsilon (float): The probability of selecting a random action (exploration rate).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space_size: int, epsilon: float, alpha: float = 0.1, gamma: float = 0.99):\n",
    "        \"\"\"\n",
    "        Initializes the Q-Learning algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - action_space_size (int): The number of available actions.\n",
    "        - epsilon (float): The probability of selecting a random action (exploration rate).\n",
    "        - alpha (float): The learning rate for Q-value updates.\n",
    "        - gamma (float): The discount factor for future rewards.\n",
    "        \"\"\"\n",
    "        super().__init__(action_space_size, epsilon=epsilon)\n",
    "        self.alpha: float = alpha\n",
    "        self.gamma: float = gamma\n",
    "        self.epsilon: float = epsilon\n",
    "\n",
    "    def update_and_select_action(self, state: int | Iterable, action: int, reward: float, next_state: int | Iterable,done:bool) -> int:\n",
    "        \"\"\"\n",
    "        Updates the Q-table using the Q-Learning update rule and selects the next action.\n",
    "\n",
    "        Unlike SARSA, Q-Learning uses the **maximum Q-value** from the next state to update \n",
    "        the current Q-value, making it an off-policy learning algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state.\n",
    "        - action (int): The action taken.\n",
    "        - reward (float): The reward received for the action.\n",
    "        - next_state: The next state after taking the action.\n",
    "\n",
    "        Returns:\n",
    "        - int: The next action to take based on the updated Q-values.\n",
    "        \"\"\"\n",
    "        state = discretize_state(state)\n",
    "        next_state = discretize_state(next_state)\n",
    "\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.alpha * td_error\n",
    "\n",
    "        return self(next_state)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns a string representation of the Q-Learning algorithm.\n",
    "\n",
    "        Returns:\n",
    "        - str: A formatted string showing the Q-Learning parameters.\n",
    "        \"\"\"\n",
    "        return f\"Q-Learning (ε={self.epsilon}, α={self.alpha}, γ={self.gamma})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, policy, num_episodes: int = 10000, max_steps: int = 1000,verbose = False) -> List[float]:\n",
    "    \"\"\"\n",
    "    Trains an RL agent using the given policy.\n",
    "\n",
    "    This function runs multiple episodes where the agent interacts with the environment, \n",
    "    updating its policy based on the rewards received. The training process follows the \n",
    "    epsilon-greedy exploration strategy.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The reinforcement learning environment (e.g., OpenAI Gym environment).\n",
    "    - policy: The policy object that determines action selection and updates Q-values.\n",
    "    - num_episodes (int, optional): The number of episodes to train the agent. Default is 10,000.\n",
    "    - max_steps (int, optional): The maximum number of steps per episode before termination. Default is 1,000.\n",
    "\n",
    "    Returns:\n",
    "    - List[float]: A list of total rewards obtained per episode.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        if episode % 100 == 0 and verbose:\n",
    "            print(f\"Episode {episode}/{num_episodes}\")\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        action = policy(state)\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            next_state, reward, done, truncated, _ = env.step(int(action))\n",
    "            next_action = policy.update_and_select_action(state, action, reward, next_state, done)\n",
    "            state, action = next_state, next_action\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "    print(\"Training done!\")\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 500\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "# Initialize the environment \n",
    "env = gym.make(\"CliffWalking-v0\",)\n",
    "\n",
    "\n",
    "# Initialize agents\n",
    "q_learning = QLearning(env.action_space.n,epsilon=0.1,alpha=0.1, gamma=0.9)\n",
    "sarsa = SARSA(env.action_space.n,epsilon=0.1,alpha=0.1, gamma=0.9)\n",
    "expected_sarsa = ExpectedSARSA(env.action_space.n,epsilon=0.1,alpha=0.1, gamma=0.9)\n",
    "\n",
    "data = {\n",
    "    str(q_learning): train(env, q_learning, num_episodes=NUM_EPISODES, max_steps=MAX_STEPS),\n",
    "    str(sarsa): train(env, sarsa, num_episodes=NUM_EPISODES, max_steps=MAX_STEPS),\n",
    "    str(expected_sarsa): train(env, expected_sarsa, num_episodes=NUM_EPISODES, max_steps=MAX_STEPS),\n",
    "}\n",
    "moving_average(data, window_size=50,show_original=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 5000\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "# Initialize the environment \n",
    "env = gym.make(\"CliffWalking-v0\",)\n",
    "\n",
    "\n",
    "# Initialize agents\n",
    "q_learning = QLearning(env.action_space.n,epsilon=0.01,alpha=0.01, gamma=0.9)\n",
    "sarsa = SARSA(env.action_space.n,epsilon=0.01,alpha=0.01, gamma=0.9)\n",
    "expected_sarsa = ExpectedSARSA(env.action_space.n,epsilon=0.01,alpha=0.01, gamma=0.9)\n",
    "\n",
    "data = {\n",
    "    str(q_learning): train(env, q_learning, num_episodes=NUM_EPISODES, max_steps=MAX_STEPS),\n",
    "    str(sarsa): train(env, sarsa, num_episodes=NUM_EPISODES, max_steps=MAX_STEPS),\n",
    "    str(expected_sarsa): train(env, expected_sarsa, num_episodes=NUM_EPISODES, max_steps=MAX_STEPS),\n",
    "}\n",
    "moving_average(data, window_size=50,show_original=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Policy vs. Off-Policy Learning\n",
    "\n",
    "Reinforcement learning (RL) algorithms can be broadly categorized into **on-policy** and **off-policy** methods based on how they learn from their experiences. The choice between these approaches has important implications for **policy safety, stability, and efficiency** in real-world applications.\n",
    "\n",
    "---\n",
    "\n",
    "## On-Policy Learning\n",
    "- **Definition:**  \n",
    "  On-policy algorithms learn the value of the policy being carried out by the agent. They update their estimates using actions that are generated by the **current** policy.\n",
    "\n",
    "- **Examples:**  \n",
    "  - **SARSA:** Updates Q-values based on the action actually taken by the current ε-greedy policy.\n",
    "  - **Policy Gradient Methods (e.g., REINFORCE, PPO):** Optimize policy parameters directly using trajectories generated from the current policy.\n",
    "\n",
    "- **Policy Safety:**  \n",
    "  On-policy methods tend to be more conservative because they explicitly consider the **exploration behavior** during learning. This makes them well-suited for applications where **taking risky actions can have serious consequences**.\n",
    "\n",
    "---\n",
    "\n",
    "## Off-Policy Learning\n",
    "- **Definition:**  \n",
    "  Off-policy algorithms learn the value of an optimal policy **independently** of the agent's current behavior. They can use data collected from **a different policy** to improve the learned policy.\n",
    "\n",
    "- **Examples:**  \n",
    "  - **Q-Learning:** Uses the maximum estimated Q-value for the next state, regardless of the action taken by the behavior policy.\n",
    "  - **Deep Q-Networks (DQN):** Extends Q-learning to high-dimensional state spaces using deep neural networks.\n",
    "  - **Experience Replay Buffers (used in DQN, SAC):** Store past transitions and reuse them for training, making learning more sample-efficient.\n",
    "\n",
    "- **Policy Safety:**  \n",
    "  Off-policy methods are typically more **aggressive** in seeking the optimal policy. They often **converge faster** but may be **unstable** in environments where exploration is risky or where the behavior policy differs significantly from the optimal policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates a trained RL policy in the given environment.\n",
    "\n",
    "    This function runs a single episode using the provided policy and returns the total \n",
    "    reward accumulated. The environment is rendered during the evaluation process.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The reinforcement learning environment (e.g., OpenAI Gym environment).\n",
    "    - policy: The trained policy object that selects actions.\n",
    "\n",
    "    Returns:\n",
    "    - float: The total reward obtained during the episode.\n",
    "    \"\"\"\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < 1000:\n",
    "        env.render()\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize different agents' approaches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "q_learning = QLearning(env.action_space.n,epsilon=0.05,alpha=0.1, gamma=0.9)\n",
    "train(env,q_learning,num_episodes=5000,verbose=True)\n",
    "env = gym.make(\"CliffWalking-v0\",render_mode=\"human\")\n",
    "evaluate(env,q_learning)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "sarsa = SARSA(env.action_space.n,epsilon=0.05,alpha=0.1, gamma=0.9)\n",
    "train(env,sarsa,num_episodes=5000,verbose=True)\n",
    "env = gym.make(\"CliffWalking-v0\",render_mode=\"human\")\n",
    "evaluate(env,sarsa)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Step TD: Bridging Monte Carlo and TD Learning\n",
    "\n",
    "N-Step Temporal Difference (TD) Learning is an **intermediate approach** between **one-step TD methods** (such as SARSA and Q-learning) and **Monte Carlo (MC) methods**. It generalizes TD learning by updating value estimates based on rewards accumulated over **multiple steps**, rather than a single step (as in TD(0)) or a full episode (as in Monte Carlo methods).\n",
    "\n",
    "## How N-Step TD Works  \n",
    "At each time step $ t $, the **n-step TD update rule** is:\n",
    "\n",
    "$$\n",
    "V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ G_t - V(s_t) \\right]\n",
    "$$\n",
    "\n",
    "where the **n-step return** $ G_t$ is calculated as:\n",
    "\n",
    "$$\n",
    "G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots + \\gamma^{n-1} r_{t+n} + \\gamma^n V(s_{t+n})\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ V(s_t) $ is the estimated value of state $ s_t $.\n",
    "- $ \\alpha $ is the learning rate, controlling the step size for updates.\n",
    "- $ r_{t+1}, r_{t+2}, \\dots, r_{t+n} $ are the rewards collected over the next $ n $ steps.\n",
    "- $ \\gamma $ is the discount factor, weighting the importance of future rewards.\n",
    "- $ V(s_{t+n}) $ is the estimated value of the state after $ n $ steps.\n",
    "\n",
    "## Key Characteristics  \n",
    "- **Balances Bias and Variance:**  \n",
    "  - **TD(0)** has **low bias but high variance** because it updates after every step.\n",
    "  - **Monte Carlo** has **high bias but low variance**, since it updates only at the end of an episode.\n",
    "  - **N-Step TD** provides a **tradeoff**, reducing variance while incorporating more future information.\n",
    "  \n",
    "- **Adaptive Credit Assignment:**  \n",
    "  - TD(0) only considers immediate rewards.\n",
    "  - Monte Carlo methods consider the full return after the episode.\n",
    "  - N-step TD assigns credit **partially**, considering rewards over multiple steps.\n",
    "\n",
    "- **Handles Partial Episodes:**  \n",
    "  - Unlike Monte Carlo, **N-step TD can update values before an episode ends**, making it suitable for **online learning**.\n",
    "\n",
    "## N-Step TD vs. Monte Carlo vs. TD(0)  \n",
    "| Feature             | N-Step TD | Monte Carlo | TD(0) |\n",
    "|---------------------|----------|-------------|--------|\n",
    "| Update Timing      | After \\( n \\) steps | After full episode | After each step |\n",
    "| Variance           | Moderate | High | Low |\n",
    "| Bias               | Moderate | Low | High |\n",
    "| Online Learning    | Yes | No | Yes |\n",
    "| Sample Efficiency  | Higher than MC | Low | Very High |\n",
    "| Stability          | More stable than TD(0) | Stable but high variance | Unstable in stochastic cases |\n",
    "\n",
    "## N-Step TD in Practice  \n",
    "N-step TD is widely used in **reinforcement learning applications** where full episode learning (Monte Carlo) is too slow, and **TD(0) updates are too noisy**. **Choosing the right value of \\( n \\)** depends on the problem—small values resemble **TD(0)**, while larger values approximate **Monte Carlo methods**.\n",
    "\n",
    "Techniques like **TD(λ)** use **eligibility traces** to dynamically adjust \\( n \\), allowing smooth interpolation between TD and Monte Carlo methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Backup_diagrams](assets/td_mc_backup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NstepSarsa](assets/NstepSASRA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NStepSARSA(Policy):\n",
    "    \"\"\"\n",
    "    Implements N-step SARSA without modifying the training loop.\n",
    "    All N-step logic (buffering, flushing, partial returns) is handled\n",
    "    inside update_and_select_action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 action_space_size: int,\n",
    "                 epsilon: float = 0.1, \n",
    "                 alpha: float = 0.1, \n",
    "                 gamma: float = 0.9,\n",
    "                 n: int = 5):\n",
    "        \"\"\"\n",
    "        Initializes the N-step SARSA algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - action_space_size (int): Number of available actions.\n",
    "        - epsilon (float): Probability of choosing a random action (exploration).\n",
    "        - alpha (float): Learning rate.\n",
    "        - gamma (float): Discount factor.\n",
    "        - n (int): Number of steps to look ahead for updates.\n",
    "        \"\"\"\n",
    "        super().__init__(action_space_size, epsilon=epsilon)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.n = n\n",
    "        \n",
    "        self.buffer = deque()\n",
    "\n",
    "    def update_and_select_action(\n",
    "        self,\n",
    "        state: int | Iterable,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_state: int | Iterable,\n",
    "        done: bool\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Choose the next action (epsilon-greedy), store this step's transition,\n",
    "        and perform an N-step SARSA update if possible. If the episode ends,\n",
    "        flush remaining transitions.\n",
    "\n",
    "        Parameters:\n",
    "        - state (int | Iterable): Current state.\n",
    "        - action (int): Action taken in `state`.\n",
    "        - reward (float): Reward received.\n",
    "        - next_state (int | Iterable): Next state after taking `action`.\n",
    "        - done (bool): Whether the episode has ended.\n",
    "\n",
    "        Returns:\n",
    "        - int: The next action for `next_state`, or 0 if the episode ended.\n",
    "        \"\"\"\n",
    "        s = discretize_state(state)\n",
    "        ns = discretize_state(next_state)\n",
    "\n",
    "      \n",
    "        if not done:\n",
    "            next_action = self(ns)\n",
    "        else:\n",
    "          \n",
    "            next_action = 0\n",
    "\n",
    "       \n",
    "        transition = (s, action, reward, done, ns, next_action)\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "      \n",
    "        if len(self.buffer) >= self.n:\n",
    "            self._update_earliest_transition()\n",
    "\n",
    "        \n",
    "        if done:\n",
    "            self._flush_buffer()\n",
    "\n",
    "       \n",
    "        return next_action\n",
    "\n",
    "    def _update_earliest_transition(self):\n",
    "        \"\"\"\n",
    "        Perform an N-step SARSA update for the oldest transition in the buffer.\n",
    "        Uses the next (n-th) transition to add gamma^n * Q(...) if the episode\n",
    "        has not ended by that step.\n",
    "        \"\"\"\n",
    "        n_seq = list(self.buffer)[:self.n]  # first n transitions\n",
    "        s_old, a_old, _, _, _, _ = n_seq[0]\n",
    "\n",
    "        G = 0.0\n",
    "        steps = 0\n",
    "        for i, (s_i, a_i, r_i_plus_1, done_i_plus_1, ns_i, na_i) in enumerate(n_seq):\n",
    "            G += (self.gamma ** i) * r_i_plus_1\n",
    "            steps += 1\n",
    "            if done_i_plus_1:\n",
    "                break\n",
    "\n",
    "        last_s, last_a, _, last_done, _, _ = n_seq[-1]\n",
    "        if (not last_done) and (steps == self.n):\n",
    "            #\n",
    "            _, _, _, _, nth_s, nth_a = n_seq[-1]\n",
    "            G += (self.gamma ** self.n) * self.q_table[nth_s][nth_a]\n",
    "\n",
    "    \n",
    "        td_error = G - self.q_table[s_old][a_old]\n",
    "        self.q_table[s_old][a_old] += self.alpha * td_error\n",
    "\n",
    "        self.buffer.popleft()\n",
    "\n",
    "    def _flush_buffer(self):\n",
    "        \"\"\"\n",
    "        Flush any leftover transitions in the buffer at episode end,\n",
    "        performing shorter updates as the episode has ended.\n",
    "        \"\"\"\n",
    "        while self.buffer:\n",
    "            self._update_earliest_transition()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"N-Step SARSA(n={self.n}, ε={self.epsilon}, \"\n",
    "                f\"α={self.alpha}, γ={self.gamma})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES =3000\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "env = gym.make(\"CliffWalking-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sarsa = SARSA(env.action_space.n,epsilon=0.1,alpha=0.01, gamma=0.9)\n",
    "n_step_sarsa_small = NStepSARSA(env.action_space.n,epsilon=0.1,alpha=0.01, gamma=0.9,n=3)\n",
    "n_step_sarsa_large = NStepSARSA(env.action_space.n,epsilon=0.1,alpha=0.01, gamma=0.9,n=12)\n",
    "mc_agent = MonteCarloCliffWalking(env, gamma=0.9, epsilon=0.1, episodes=NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    str(sarsa): train(env, sarsa, num_episodes=NUM_EPISODES, max_steps=MAX_STEPS),\n",
    "    str(n_step_sarsa_small): train(env, n_step_sarsa_small, num_episodes=NUM_EPISODES, max_steps=MAX_STEPS),\n",
    "    str(n_step_sarsa_large): train(env, n_step_sarsa_large, num_episodes=NUM_EPISODES, max_steps=MAX_STEPS),\n",
    "    str(mc_agent): mc_agent.train()\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average(data, window_size=50,show_original=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Reinforcement Learning Algorithms: Reliable and Standardized Protocols\n",
    "\n",
    "Evaluating reinforcement learning (RL) algorithms is crucial to understanding their generalization, robustness, and efficiency. A well-defined evaluation protocol ensures that RL models are not only optimized for a specific task but also perform reliably across different environments and conditions.\n",
    "\n",
    "To ensure consistency and scientific reproducibility, researchers follow standardized evaluation protocols, including benchmarking against established datasets, statistical significance testing, and multiple training runs.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Principles of RL Evaluation\n",
    "\n",
    "### 1. Performance Metrics\n",
    "The most commonly used metrics for RL evaluation include:  \n",
    "- **Total Reward (Return):** The cumulative reward obtained per episode.  \n",
    "- **Sample Efficiency:** The number of interactions required to reach optimal performance.  \n",
    "- **Learning Stability:** How consistently the agent improves over time.  \n",
    "- **Final Convergence:** Whether the algorithm stabilizes at an optimal policy.  \n",
    "- **Robustness:** The agent’s ability to generalize across different scenarios.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Reliable Experimental Design\n",
    "To ensure fair comparisons, RL algorithms should be evaluated under standardized conditions.  \n",
    "\n",
    "#### Multiple Training Runs\n",
    "Since RL training involves stochastic processes, single-run evaluations are not reliable. A standard practice is to conduct multiple independent training runs (e.g., 5 to 10 runs) and report:  \n",
    "- The **mean** and **standard deviation** of performance metrics.  \n",
    "- Confidence intervals to assess statistical significance.  \n",
    "\n",
    "#### Benchmarking on Standard Environments\n",
    "RL algorithms should be tested across diverse environments to assess their generalization ability. Commonly used benchmarks include:  \n",
    "- **OpenAI Gym** (e.g., CartPole, LunarLander, Atari games)  \n",
    "- **DeepMind Control Suite**  \n",
    "- **Mujoco (Robotics Simulations)**  \n",
    "- **ProcGen (Procedurally Generated Environments)**  \n",
    "\n",
    "Different environments **require vastly different training durations** due to their complexity.  \n",
    "- **Simple environments** like **CartPole** may require only **a few thousand episodes** to reach optimal performance.  \n",
    "- **More complex environments** such as **Atari games** often require between **100,000 to 1,000,000 episodes** to learn effective policies.  \n",
    "- **High-dimensional control tasks** (e.g., Mujoco, robotic manipulation) may require millions of steps due to **continuous action spaces and sparse rewards**.  \n",
    "\n",
    "Training duration should be carefully chosen based on the **difficulty of the task** and the **expected sample efficiency** of the RL algorithm.  \n",
    "\n",
    "#### Hyperparameter Sensitivity Analysis\n",
    "RL models often require extensive tuning. A robust evaluation must analyze:  \n",
    "- The sensitivity of the algorithm to **learning rate (α), discount factor (γ), and exploration rate (ε)**.  \n",
    "- Whether the model **converges reliably** across different settings.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Statistical Robustness and Reporting\n",
    "A rigorous RL evaluation protocol should:  \n",
    "- Report variance: RL algorithms can be highly unstable. Showing only the best-performing run is misleading.  \n",
    "- Plot learning curves with confidence intervals (e.g., shaded regions for standard deviation).  \n",
    "- Use statistical tests (e.g., t-tests, bootstrapping) to compare different RL algorithms.  \n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices for RL Evaluation\n",
    "- Train and evaluate on multiple environments to test generalization.  \n",
    "- Run experiments multiple times to reduce randomness.  \n",
    "- Use moving averages to smooth learning curves for better visualization.  \n",
    "- Report variance and confidence intervals, not just raw scores.  \n",
    "- Conduct hyperparameter tuning and analyze sensitivity.  \n",
    "- Compare against strong baselines (e.g., random policies, human performance, existing RL benchmarks).  \n",
    "- **Adjust training duration** based on the complexity of the environment:\n",
    "  - **Small-scale environments**: 10,000 – 100,000 episodes.\n",
    "  - **Atari-like environments**: 100,000 – 1,000,000 episodes.\n",
    "  - **Continuous control and robotics**: 1,000,000+ episodes.  \n",
    "\n",
    "By following these reliable and standardized evaluation protocols, we ensure that RL research remains reproducible, fair, and scientifically rigorous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_rl_training_runs(data_dict: Dict[str, List[List[float]]], window_size: int = 10) -> None:\n",
    "    \"\"\"\n",
    "    Plots the mean and standard deviation of episode rewards across multiple training runs.\n",
    "\n",
    "    This function visualizes the performance of different RL algorithms by computing the \n",
    "    mean and standard deviation of rewards over multiple training runs. A moving average \n",
    "    is applied to smooth the learning curves.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict (Dict[str, List[List[float]]]): A dictionary where keys are algorithm names and values \n",
    "      are 2D lists or arrays of shape (M, N), where:\n",
    "        - M is the number of training runs.\n",
    "        - N is the number of episodes per run.\n",
    "    - window_size (int, optional): The number of episodes for moving average smoothing. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays the plotted learning curves.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    for algo, rewards in data_dict.items():\n",
    "        rewards = np.array(rewards)  # Convert to numpy array (M, N)\n",
    "        rewards = np.clip(rewards, -500, None)\n",
    "        # Compute mean and standard deviation across runs (axis=0)\n",
    "        mean_rewards = np.mean(rewards, axis=0)\n",
    "        std_rewards = np.std(rewards, axis=0)\n",
    "\n",
    "        # Smooth with moving average\n",
    "        if window_size > 1:\n",
    "            mean_rewards = np.convolve(mean_rewards, np.ones(window_size) / window_size, mode='valid')\n",
    "            std_rewards = np.convolve(std_rewards, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "        # Plot mean curve\n",
    "        x_values = np.arange(len(mean_rewards))\n",
    "        plt.plot(x_values, mean_rewards, label=f\"{algo} (Mean)\")\n",
    "\n",
    "        # Plot shaded region for standard deviation\n",
    "        plt.fill_between(x_values, mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.2)\n",
    "\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Returns\")\n",
    "    plt.title(f\"Algorithm Performance Comparison with {window_size}-Episode Smoothing\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "NUM_RUNS = 5\n",
    "NUM_EPISODES = 1000\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "returns = {\n",
    "    'Optimal Policy': [[-13 for _ in range(NUM_EPISODES)]] # Optimal policy rewards for CliffWalking, -13 is the best possible reward\n",
    "}\n",
    "\n",
    "\n",
    "for _ in range(NUM_RUNS):\n",
    "    algo = QLearning(env.action_space.n,epsilon=0.01,alpha=0.01, gamma=0.9)\n",
    "    ret = returns.get(str(algo), [])\n",
    "    ret.append(train(env, algo,NUM_EPISODES,MAX_STEPS))\n",
    "    returns[str(algo)] = ret\n",
    "for _ in range(NUM_RUNS):\n",
    "    algo = SARSA(env.action_space.n,epsilon=0.3,alpha=0.1, gamma=0.9)\n",
    "    ret = returns.get(str(algo), [])\n",
    "    ret.append(train(env, algo,NUM_EPISODES,MAX_STEPS))\n",
    "    returns[str(algo)] = ret\n",
    "for _ in range(NUM_RUNS):\n",
    "    algo = ExpectedSARSA(env.action_space.n,epsilon=0.1,alpha=0.1, gamma=0.9)\n",
    "    ret = returns.get(str(algo), [])\n",
    "    ret.append(train(env, algo,NUM_EPISODES,MAX_STEPS))\n",
    "    returns[str(algo)] = ret\n",
    "for _ in range(NUM_RUNS):\n",
    "    algo = NStepSARSA(env.action_space.n,epsilon=0.1,alpha=0.1, gamma=0.9,n=4)\n",
    "    ret = returns.get(str(algo), [])\n",
    "    ret.append(train(env, algo,NUM_EPISODES,MAX_STEPS))\n",
    "    returns[str(algo)] = ret\n",
    "for _ in range(NUM_RUNS):\n",
    "    algo = MonteCarloCliffWalking(env, gamma=0.9, epsilon=0.1, episodes=NUM_EPISODES)\n",
    "    ret = returns.get(str(algo), [])\n",
    "    ret.append(algo.train())\n",
    "    returns[str(algo)] = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rl_training_runs(returns, window_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can try to train a tabular Q-Learning agent on the Pong environment using the RAM state representation.\n",
    "\n",
    "The environment is very high-dimensional, so it may take a long time to train. (4+ hours)\n",
    "\n",
    "You can also try other algorithms like SARSA, Expected SARSA, or N-Step SARSA.\n",
    "\n",
    "Decomment the following code to train the agent on the Pong environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"Pong-ram-v4\")\n",
    "# ql = QLearning(env.action_space.n,epsilon=0.1,alpha=0.1, gamma=0.9)\n",
    "# rewards = train(env, ql,100_000,1000,verbose=True)\n",
    "# moving_average({\"Q-Learning\": rewards}, window_size=1000, show_original=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
