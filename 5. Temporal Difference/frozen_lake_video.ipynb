{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Logo](assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/5.%20Temporal%20Difference/frozen_lake.ipynb)\n",
    "\n",
    "# Frozen Lake\n",
    "\n",
    "Frozen lake involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
    "\n",
    "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "Holes in the ice are distributed in set locations.\n",
    "The player makes moves until they reach the goal or fall in a hole.\n",
    "\n",
    "![Example image](assets/frozen_lake.png)\n",
    "\n",
    "This problem can be formulated with a finite, undiscounted MDP, where the states are the positions in the grid world, the actions are UP, DOWN, LEFT and RIGHT, and the reward is 1 for reaching the goal and 0 otherwise (even for falling in a hole). In this example, we use the `FrozenLake-v1` environment from the `Gymnasium` library to represent the problem, and use *Q-learning* to solve it.\n",
    "\n",
    "- Documentation for the Frozen Lake environment: https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import time\n",
    "from tqdm.notebook import trange\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_raw = gym.make('FrozenLake-v1', render_mode='rgb_array')  # creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-TABLE:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# initializing q-table\n",
    "action_space_size = env_raw.action_space.n\n",
    "observation_space_size = env_raw.observation_space.n\n",
    "\n",
    "q_table = np.zeros((observation_space_size, action_space_size))\n",
    "print(\"Q-TABLE:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "N_EPISODES = 10_000\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "\n",
    "ALPHA = 0.1  # learning rate\n",
    "GAMMA = 0.98  # discount rate\n",
    "\n",
    "EPSILON = 1  # exploration rate\n",
    "EPSILON_MIN = 0.001\n",
    "EPSILON_DECAY = (2 * EPSILON) / N_EPISODES\n",
    "\n",
    "LOG_RATE = N_EPISODES / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/jovyan/work/5. Temporal Difference/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/gymnasium/core.py:297: UserWarning: \u001b[33mWARN: env.is_vector_env to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_vector_env` for environment variables or `env.get_attr('is_vector_env')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# wrap environment\n",
    "rec_episodes = np.linspace(0, N_EPISODES-1, num=3, dtype=int)\n",
    "trigger = lambda t: t in rec_episodes\n",
    "env = RecordVideo(env_raw, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Q-learning combines ideas from both *Dynamic Programming* and *Monte Carlo* methods. Similarly to MC, Q-learning simulates episodes, and updates the\n",
    "value function according to the returns. However, there is an important difference in the update rule of these two methods: while MC uses only returns\n",
    "from the currently simulated episode, Q-learning utilizes *bootstrapping*, that is, it updates estimates based on other learned estimates, without\n",
    "waiting for a final outcome.\n",
    "\n",
    "The update rule for Q-learning looks like this:\n",
    "\n",
    "$$ Q_t(S_t,A_t) \\leftarrow Q_t(S_t,A_t) + \\alpha[R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q_t(S_t,A_t)] $$\n",
    "\n",
    "Where $\\alpha \\in (0;1]$ is a constant step-size parameter and $\\gamma \\in [0;1]$ is the discount rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bf0bd5086846fd900386e473b0652f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: XDG_RUNTIME_DIR not set in the environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 : avg=0.021\n",
      "Episode 2000 : avg=0.027\n",
      "Episode 3000 : avg=0.071\n",
      "Episode 4000 : avg=0.166\n",
      "Episode 5000 : avg=0.407\n",
      "Episode 6000 : avg=0.726\n",
      "Episode 7000 : avg=0.71\n",
      "Episode 8000 : avg=0.753\n",
      "Episode 9000 : avg=0.76\n",
      "Episode 10000 : avg=0.75\n"
     ]
    }
   ],
   "source": [
    "env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n",
    "sum_rewards = 0\n",
    "\n",
    "for episode in trange(N_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for step in range(MAX_STEPS_PER_EPISODE):\n",
    "        # epsilon-greedy action selection\n",
    "        if np.random.rand() > EPSILON:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # updating q-table\n",
    "        q_table[state, action] = q_table[state, action] * (1 - ALPHA) + \\\n",
    "            ALPHA * (reward + GAMMA * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        sum_rewards += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # updating epsilon\n",
    "    EPSILON = max(EPSILON - EPSILON_DECAY, EPSILON_MIN)\n",
    "\n",
    "    # logging the results\n",
    "    if (episode + 1) % LOG_RATE == 0:\n",
    "        print(f'Episode {episode + 1} : avg={sum_rewards / LOG_RATE}')\n",
    "        sum_rewards = 0\n",
    "\n",
    "# saving the q-table\n",
    "with open('q_table.bin', 'wb') as f:\n",
    "    pickle.dump(q_table, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-TABLE:\n",
      "[[0.46101416 0.33632382 0.33413143 0.3345217 ]\n",
      " [0.22000326 0.23727849 0.22087343 0.36044701]\n",
      " [0.24098357 0.24756546 0.24192639 0.29620273]\n",
      " [0.2294834  0.1878254  0.16768959 0.27830905]\n",
      " [0.49729241 0.31305279 0.30758757 0.22582035]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.10513979 0.08859962 0.32455693 0.08639079]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.29885718 0.30188415 0.27435842 0.57432221]\n",
      " [0.31804341 0.63423765 0.43284419 0.34495501]\n",
      " [0.61277732 0.29753411 0.12537382 0.30343518]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.45581398 0.52213311 0.72648682 0.49487922]\n",
      " [0.59350923 0.86996797 0.65386095 0.61464718]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"Q-TABLE:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8472258aae214016b1eb20146d9bc9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free...', autâ€¦"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "children = [widgets.Video.from_file(f'./videos/rl-video-episode-{episode}.mp4', autoplay=False, loop=False, width=500) for episode in rec_episodes]\n",
    "tab = widgets.Tab()\n",
    "tab.children = children\n",
    "tab.titles = tuple([f'Episode {episode+1}' for episode in rec_episodes])\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
