{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/5.%20Temporal%20Difference/frozen_lake.ipynb)\n",
    "\n",
    "# Frozen Lake\n",
    "\n",
    "Frozen lake involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
    "\n",
    "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "Holes in the ice are distributed in set locations.\n",
    "The player makes moves until they reach the goal or fall in a hole.\n",
    "\n",
    "<img src=\"assets/frozen_lake.gif\" width=\"400\"/>\n",
    "\n",
    "In Frozen Lake, the states are the positions in the grid world (integers 0-15), and the actions are UP, DOWN, LEFT and RIGHT (integers 0-3). The reward is 1 for reaching the goal and 0 otherwise (even for falling in a hole).\n",
    "\n",
    "This notebook uses *Q-learning* to approximate the optimal policy in the `FrozenLake-v1` Gymnasium environment.\n",
    "\n",
    "- This notebook is based on Chapter 6 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html\n",
    "- Documentation for the Frozen Lake environment: https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install gymnasium==0.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import time\n",
    "from tqdm.notebook import trange\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax function that breaks ties randomly\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_EPISODES = 10_000  # Number of training episodes\n",
    "MAX_STEPS_PER_EPISODE = 100  # Number of steps before truncation (there is no truncation in this env by default)\n",
    "EPSILON_MAX = 1  # Initial exploration\n",
    "EPSILON_MIN = 0.001  # Final exploration\n",
    "EPSILON_DECAY = 2 * EPSILON_MAX / N_EPISODES  # Exploration decay rate\n",
    "ALPHA = 0.1  # Learning rate\n",
    "GAMMA = 0.98  # Discount factor\n",
    "N_RECORDINGS = 3  # Number of episodes to record\n",
    "REC_EPISODES = np.linspace(0, N_EPISODES-1, num=N_RECORDINGS, dtype=int)  # Episodes to record\n",
    "LOG_FREQ = N_EPISODES / 10  # Progress log frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "base_env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "# Wrap environment to record videos throughout the learning process \n",
    "trigger = lambda ep: ep in REC_EPISODES\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "observation_space_size = env.observation_space.n\n",
    "action_space_size = env.action_space.n\n",
    "q_table_shape = observation_space_size, action_space_size\n",
    "q_table = np.zeros(q_table_shape)\n",
    "print(\"Q-TABLE: (shape =\", q_table.shape, \")\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Q-learning combines ideas from both *Dynamic Programming* and *Monte Carlo* methods. Similarly to MC, Q-learning simulates episodes, and updates the\n",
    "value function according to the returns. However, while MC uses only returns\n",
    "from the currently simulated episode, Q-learning updates estimates based on other learned estimates, without waiting for a final outcome. This property makes Q-learning a *bootstrapping* method, like DP.\n",
    "\n",
    "The update rule for Q-learning is:\n",
    "\n",
    "$$ Q_t(S_t,A_t) \\leftarrow Q_t(S_t,A_t) + \\alpha[R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q_t(S_t,A_t)] $$\n",
    "\n",
    "Where $\\alpha \\in (0;1]$ is a constant step-size parameter and $\\gamma \\in [0;1]$ is the discount rate.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement this algorithm! The block below only contains code necessary for logging the average reward at every `LOG_FREQ` iterations. The algorithm itself is up to you! Pseudocode for this algorithm is shown in the box below.\n",
    "\n",
    "<img src=\"assets/q-learning.png\" width=\"700\"/>\n",
    "\n",
    "*Pseudocode from page 131 of the Sutton & Barto book*\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- Use the array defined above: `q_table` corresponds to $Q$ in the pseudocode.\n",
    "- Instead of `np.argmax`, use the `argmax` function defined above!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-intialize environment\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n",
    "sum_rewards = 0\n",
    "\n",
    "# Training loop\n",
    "for episode in trange(N_EPISODES):\n",
    "    \n",
    "    ############## CODE HERE ###################\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # Log results\n",
    "    if (episode + 1) % LOG_FREQ == 0:\n",
    "        print(f'Episode {episode + 1} : avg={sum_rewards / LOG_FREQ}')\n",
    "        sum_rewards = 0\n",
    "\n",
    "# Save Q-table\n",
    "with open('q_table.bin', 'wb') as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print updated Q-table\n",
    "print(\"Q-TABLE:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "You can watch the videos recorded throughout the training process here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recordings\n",
    "children = [widgets.Video.from_file(f'./videos/rl-video-episode-{episode}.mp4', autoplay=False, loop=False, width=500) for episode in REC_EPISODES]\n",
    "tab = widgets.Tab()\n",
    "tab.children = children\n",
    "titles = tuple([f'Episode {episode + 1:,}' for episode in REC_EPISODES])\n",
    "for i in range(len(children)):\n",
    "    tab.set_title(i, titles[i])\n",
    "display(tab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
