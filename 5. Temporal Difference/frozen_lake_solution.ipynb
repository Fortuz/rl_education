{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/5.%20Temporal%20Difference/frozen_lake_solution.ipynb)\n",
    "\n",
    "# Frozen Lake (solution)\n",
    "\n",
    "Frozen lake involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
    "\n",
    "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "Holes in the ice are distributed in set locations.\n",
    "The player makes moves until they reach the goal or fall in a hole.\n",
    "\n",
    "<img src=\"assets/frozen_lake.gif\" width=\"500\"/>\n",
    "\n",
    "In Frozen Lake, the states are the positions in the grid world (integers 0-15), and the actions are UP, DOWN, LEFT and RIGHT (integers 0-3). The reward is 1 for reaching the goal and 0 otherwise (even for falling in a hole).\n",
    "\n",
    "This notebook uses *Q-learning* to find the optimal policy in the `FrozenLake-v1` Gymnasium environment.\n",
    "\n",
    "- This notebook is based on Chapter 6 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html\n",
    "- Documentation for the Frozen Lake environment: https://gymnasium.farama.org/environments/toy_text/frozen_lake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install gymnasium==0.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import time\n",
    "from tqdm.notebook import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argmax function that breaks ties randomly\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_EPISODES = 10_000 # Number of training episodes\n",
    "MAX_STEPS_PER_EPISODE = 100 # Number of episodes before truncation (there is no truncation in this env by default)\n",
    "EPSILON = 1  # Initial exploration\n",
    "EPSILON_MIN = 0.001  # Final exploration\n",
    "EPSILON_DECAY = (2 * EPSILON) / N_EPISODES  # Exploration decay rate\n",
    "ALPHA = 0.1  # Learning rate\n",
    "GAMMA = 0.98  # Discount factor\n",
    "N_RECORDINGS = 3  # Number of episodes to record\n",
    "REC_EPISODES = np.linspace(0, N_EPISODES-1, num=N_RECORDINGS, dtype=int)  # Episodes to record\n",
    "LOG_FREQ = N_EPISODES / 10  # Progress log frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "base_env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "# Wrap environment to record videos throughout the learning process \n",
    "trigger = lambda ep: ep in REC_EPISODES\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-TABLE:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Q-table\n",
    "action_space_size = env.action_space.n\n",
    "observation_space_size = env.observation_space.n\n",
    "q_table_shape = observation_space_size, action_space_size\n",
    "q_table = np.zeros(q_table_shape)\n",
    "print(\"Q-TABLE:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "Q-learning combines ideas from both *Dynamic Programming* and *Monte Carlo* methods. Similarly to MC, Q-learning simulates episodes, and updates the\n",
    "value function according to the returns. However, while MC uses only returns\n",
    "from the currently simulated episode, Q-learning updates estimates based on other learned estimates, without waiting for a final outcome. This property makes Q-learning a *bootstrapping* method, like DP.\n",
    "\n",
    "The update rule for Q-learning is:\n",
    "\n",
    "$$ Q_t(S_t,A_t) \\leftarrow Q_t(S_t,A_t) + \\alpha[R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q_t(S_t,A_t)] $$\n",
    "\n",
    "Where $\\alpha \\in (0;1]$ is a constant step-size parameter and $\\gamma \\in [0;1]$ is the discount rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa36178ee8ea4678a5f9bdd65ae181e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 : avg=0.018\n",
      "Episode 2000 : avg=0.032\n",
      "Episode 3000 : avg=0.065\n",
      "Episode 4000 : avg=0.152\n",
      "Episode 5000 : avg=0.416\n",
      "Episode 6000 : avg=0.714\n",
      "Episode 7000 : avg=0.729\n",
      "Episode 8000 : avg=0.767\n",
      "Episode 9000 : avg=0.745\n",
      "Episode 10000 : avg=0.707\n"
     ]
    }
   ],
   "source": [
    "# Re-intialize environment and Q-table\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n",
    "q_table = np.zeros(q_table_shape)\n",
    "sum_rewards = 0\n",
    "\n",
    "# Training loop\n",
    "for episode in trange(N_EPISODES):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() > EPSILON:\n",
    "            action = argmax(q_table[obs])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # Take step\n",
    "        new_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or step >= MAX_STEPS_PER_EPISODE  # Truncating manually\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[obs, action] += ALPHA * (reward + GAMMA * np.max(q_table[new_obs]) - q_table[obs, action])\n",
    "\n",
    "        # Store reward and new state\n",
    "        sum_rewards += reward\n",
    "        obs = new_obs\n",
    "        step += 1\n",
    "\n",
    "    # Decay epsilon\n",
    "    EPSILON = max(EPSILON - EPSILON_DECAY, EPSILON_MIN)\n",
    "\n",
    "    # Log results\n",
    "    if (episode + 1) % LOG_FREQ == 0:\n",
    "        print(f'Episode {episode + 1} : avg={sum_rewards / LOG_FREQ}')\n",
    "        sum_rewards = 0\n",
    "\n",
    "# Save Q-table\n",
    "with open('q_table.bin', 'wb') as f:\n",
    "    pickle.dump(q_table, f)\n",
    "\n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-TABLE:\n",
      "[[0.34798346 0.31604709 0.31920247 0.31920109]\n",
      " [0.27577851 0.20877358 0.18096888 0.30864357]\n",
      " [0.25478524 0.26154585 0.25434942 0.25457424]\n",
      " [0.1673691  0.16800298 0.19078848 0.25826826]\n",
      " [0.3744031  0.31658972 0.18656963 0.2815639 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11431091 0.07316903 0.24596349 0.09962938]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.31589313 0.28015899 0.27582565 0.40717594]\n",
      " [0.37909307 0.45413064 0.41051007 0.29902925]\n",
      " [0.48756108 0.31962601 0.29624658 0.26183056]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44713157 0.42553751 0.59194665 0.4338701 ]\n",
      " [0.59748442 0.75547702 0.68296786 0.66044644]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"Q-TABLE:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "You can watch the videos recorded throughout the training process here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45a156bf49044febce23877907fbf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Video(value=b'\\x00\\x00\\x00 ftypisom\\x00\\x00\\x02\\x00isomiso2avc1mp41\\x00\\x00\\x00\\x08free...', aut…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display recordings\n",
    "children = [widgets.Video.from_file(f'./videos/rl-video-episode-{episode}.mp4', autoplay=False, loop=False, width=500) for episode in REC_EPISODES]\n",
    "tab = widgets.Tab()\n",
    "tab.children = children\n",
    "titles = tuple([f'Episode {episode + 1:,}' for episode in REC_EPISODES])\n",
    "for i in range(len(children)):\n",
    "    tab.set_title(i, titles[i])\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
