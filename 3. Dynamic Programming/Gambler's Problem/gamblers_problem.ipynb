{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logo](../../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/3.%20Dynamic%20Programming/Gambler's%20Problem/gamblers_problem.ipynb)\n",
    "\n",
    "# Gambler's Problem\n",
    "\n",
    "A gambler has the opportunity to make bets on\n",
    "the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many\n",
    "dollars as he has staked on that flip; if it is tails, he loses his stake.\n",
    "\n",
    "<img src=\"assets/coinflip.jpg\" width=\"500\"/>\n",
    "\n",
    "The game ends\n",
    "when the gambler wins by reaching his goal of \\\\$100, or loses by running out of money.\n",
    "On each flip, the gambler must decide what portion of his capital to stake, in integer\n",
    "numbers of dollars. This problem can be formulated as an undiscounted, episodic, finite\n",
    "MDP. The state is the gamblerâ€™s capital, $ s \\in {1, 2, . . . , 99} $ and the actions\n",
    "are stakes, $ a \\in {0, 1, . . . , min(s, 100 - s)} $. The reward is zero on all transitions\n",
    "except those on which the gambler reaches his goal, when it is +1.\n",
    "\n",
    "Our goal is to find the optimal policy for this problem. We will implement a simple\n",
    "algorithm that uses *value iteration* to solve the Bellman equation for the state-value\n",
    "function.\n",
    "\n",
    "- This notebook is based on Chapter 4 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROB_HEADS = 0.4  # Probability that a coin comes up heads\n",
    "THETA = 1e-12  # Error treshold for the value iteration\n",
    "GAMMA = 1  # Discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_estimations = np.zeros(101)  # Value function\n",
    "policy = np.zeros(101, dtype=int)  # Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "In *value iteration*, we start with an arbitrary initial value function and then iteratively improve it until it converges to the optimal value function. At each iteration, we update the value of each state based on the Bellman optimality equation, which states that the value of a state is equal to the immediate reward plus the discounted value of the successor states, weighted by the probability of transitioning to those states under the optimal policy.\n",
    "The algorithm uses the following update rule:\n",
    "\n",
    "$$ v_{k+1}(s) = \\max_a \\sum_{s', r}p(s', r | s, a) [r + \\gamma v_k(s')] $$\n",
    "\n",
    "By repeatedly applying the Bellman optimality equation, the value function converges to the optimal value function. In practice, we stop when the magnitude of the greatest update, $\\delta$ falls below a sufficiently low treshold, $\\theta$.\n",
    "The code below implements value iteration for this problem, and plots out the value function after each iteration.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement value iteration! The block below only contains code necessary for plotting the state-value function at every iteration. The algorithm itself is up to you! Pseudocode for this algorithm is shown in the box below.\n",
    "\n",
    "<img src=\"assets/value_iteration.png\" width=\"700\"/>\n",
    "\n",
    "*Pseudocode from page 83 of the Sutton & Barto book*\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- Remember that terminal states are excluded from the update loop!\n",
    "- When comparing action values, you should ignore differences lower than the threshold ($\\theta$).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta = THETA\n",
    "i = 0\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "while delta >= THETA:\n",
    "\n",
    "    # TODO: Complete the algorithm!\n",
    "\n",
    "    # Add the resulting value funtion to the plot\n",
    "    plt.plot(np.arange(1, 100), v_estimations[1:100])\n",
    "    i += 1\n",
    "\n",
    "# Plot state-value functions\n",
    "plt.xlabel('Capital')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(alpha=0.8, linestyle=':', zorder=0)\n",
    "plt.title(f'State-value function after {i} sweeps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal policy\n",
    "The resulting policy can be seen below. The x axis corresponds to the state (the current capital), and the y axis shows the optimal amount of money to bet in a given state. As you can see, the graph has a peculiar, self-similar shape. Can you explain why? Think about how you would approach this problem, and keep the value of `PROB_HEADS` in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot policy\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(np.arange(101), policy)\n",
    "plt.xlabel('Capital')\n",
    "plt.ylabel('Stake')\n",
    "plt.title('Policy')\n",
    "plt.grid(alpha=0.8, linestyle=':', zorder=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
