{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fMOTafWANYU"
      },
      "source": [
        "![\"Logo\"](https://github.com/Fortuz/rl_education/blob/main/assets/logo.png?raw=1)\n",
        "\n",
        "Created by **Domonkos Nagy**\n",
        "\n",
        "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/10.%20Off-policy%20Control/atari_breakout.ipynb)\n",
        "\n",
        "# Atari Breakout\n",
        "\n",
        "Breakout is a famous Atari game. The dynamics are similar to pong: You move a paddle and hit the ball in a brick wall at the top of the screen. Your goal is to destroy the brick wall. You can try to break through the wall and let the ball wreak havoc on the other side, all on its own! You have five lives.\n",
        "\n",
        "<img src=\"https://github.com/Fortuz/rl_education/blob/main/10.%20Off-policy%20Control/assets/breakout.gif?raw=1\" width=\"300\"/>\n",
        "\n",
        "This environment runs the actual Atari game in an emulator. The observation space is `Box(0, 255, (210, 160, 3), np.uint8)`, meaning we have a 210x160 RGB image at every time step. There are 4 possible actions: NOOP, FIRE, RIGHT and LEFT, where FIRE is only used to start a new round after the ball has fallen down. You get rewards for destroying bricks in the wall, different color bricks yield different rewards. For a more detailed documentation, see the AtariAge page linked below.\n",
        "\n",
        "This notebook uses *Deep Q-Learning (DQN)* to train an agent to play Breakout. We will follow the original DQN paper (Human-level control through deep reinforcement learning, Mnih et. al.)\n",
        "closely with our implementation, and thus **it is highly recommended to read the paper before getting started with the notebook!** It is also recommended to run this notebook on Colab for much faster training times.\n",
        "\n",
        "- Read the DQN paper here: https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
        "- Documentation for the Atari Breakout environment: https://gymnasium.farama.org/environments/atari/breakout/\n",
        "- Description of the game: https://atariage.com/manual_html_page.php?SoftwareID=889"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sfeF9uqrEv1"
      },
      "outputs": [],
      "source": [
        "# Install dependencies if running in Colab\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gymnasium[atari,accept-rom-license]==0.29.0\n",
        "    !pip install gymnasium==0.29.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulI5R53UuU74"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import AtariPreprocessing, FrameStack, RecordVideo\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-MKAG5xANY1"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os-R-_UoANY3"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "These parameters are set up by default so that you can train a decent agent in about 3 hours using the free GPUs provided by Colab. This code block more or less mirrors page 10 of the paper, but there are some differences,\n",
        "for example we use epsiodes instead of overall time steps as the unit of measurement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNOdyzQMsMq5"
      },
      "outputs": [],
      "source": [
        "FRAME_SKIP = 4  # Repeat each action selected for this many frames\n",
        "N_EPISODES = 4_000 # Number of training episodes\n",
        "BATCH_SIZE = 32  # SGD batch size\n",
        "BUFFER_SIZE = 1_000_000 // FRAME_SKIP  # Size of the replay memory\n",
        "HISTORY_LENGTH = 4  # Number of frames given to the Q-network\n",
        "GAMMA = 0.99  # Discount factor\n",
        "OPTIM_FREQ = 4  # Optimize after this many action selections\n",
        "ALPHA = 0.00005  # Learning rate\n",
        "EPSILON = 1.0  # Initial exploration\n",
        "EPSILON_MIN = 0.01  # Final exploration\n",
        "EPSILON_DECAY = 4 * (EPSILON - EPSILON_MIN) / (3 * N_EPISODES)  # Exploration decay rate\n",
        "MIN_REPLAY_SIZE = 50_000 // FRAME_SKIP  # Minimum size of the replay memory\n",
        "NOOP_MAX = 30  # Maximum number of NOOP actions take by the agent at the start of each episode\n",
        "TARGET_UPDATE_FREQ = 10_000  # Update the target network with the online network's weights this frequently\n",
        "SAVE_FREQ = N_EPISODES // 50  # Model saving frequency\n",
        "N_RECORDINGS = 5  # Number of episodes to record\n",
        "REC_EPISODES = np.linspace(0, N_EPISODES-1, num=N_RECORDINGS, dtype=int)  # Episodes to record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd5NhNGrANY5"
      },
      "source": [
        "## Nature CNN\n",
        "\n",
        "This class defines the network architecture described in the paper, commonly called the \"Nature CNN\".\n",
        "The `act` method runs a forward pass on the state it receives as an argument, and returns the argmax\n",
        "of the resulting action values.\n",
        "\n",
        "<img src=\"https://github.com/Fortuz/rl_education/blob/main/10.%20Off-policy%20Control/assets/cnn.png?raw=1\" width=\"700\"/>\n",
        "\n",
        "*Figure from the DQN paper. Note that for Breakout specifically, we only need 4 output nodes.*\n",
        "\n",
        "***\n",
        "\n",
        "### **Your Task**\n",
        "\n",
        "Implement the network architecture! You can find all the details in the paper. The `act` method is already completed, you only have to implement the forward pass.\n",
        "\n",
        "#### **Hints:**\n",
        "\n",
        "- You can use the constructor to initialize your modules.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-Hoh1a3u-Ln"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, env, device):\n",
        "        super(Net, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Return the action with the highest Q-value for the input state\n",
        "    def act(self, state):\n",
        "        with torch.no_grad():\n",
        "            state_t = torch.as_tensor(np.array(state), dtype=torch.float32, device=self.device)\n",
        "            q_values = self(state_t.unsqueeze(0))\n",
        "        return torch.argmax(q_values[0]).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrlosPsvANY8"
      },
      "source": [
        "## Replay Memory\n",
        "\n",
        "The replay memory stores transitions, i. e. $(S, A, R, S')$ tuples. In practice, we also need to store whether the transition terminated the episode, so our tuples have 5 elements.\n",
        "Upon initialization, a random policy is used to fill the buffer to its minimal size. The `append` method stores a transition (5-element tuple) in the memory, while the `sample`\n",
        "method returns `batch_size` transitions sampled randomly from the buffer as a tuple of 5 PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHtOOsKZokRV"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory():\n",
        "    def __init__(self, env, min_size, max_size, device):\n",
        "        self.device = device\n",
        "        self.transitions = [None] * max_size\n",
        "\n",
        "        self.idx = 0\n",
        "        self.min_size = min_size\n",
        "        self.max_size = max_size\n",
        "        self.full = False\n",
        "        self._initialize(env)\n",
        "\n",
        "    # Run random policy for min_size steps to fill up the buffer\n",
        "    def _initialize(self, env):\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        for _ in trange(self.min_size):\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "            transition = (state, action, reward, new_state, terminated)\n",
        "            self.append(transition)\n",
        "            state = new_state\n",
        "\n",
        "            if terminated or truncated:\n",
        "                state, _ = env.reset()\n",
        "\n",
        "    # Append a transition to the buffer\n",
        "    def append(self, transition):\n",
        "        self.transitions[self.idx] = transition\n",
        "\n",
        "        self.idx = (self.idx + 1) % self.max_size\n",
        "        self.full = self.full or self.idx == 0\n",
        "\n",
        "    # Sample batch_size random transitions from the buffer\n",
        "    def sample(self, batch_size):\n",
        "        size = self.max_size if self.full else self.idx\n",
        "        indices = np.random.randint(0, high=size, size=(batch_size,))\n",
        "\n",
        "        states = np.array([self.transitions[i][0] for i in indices])\n",
        "        actions = np.array([[self.transitions[i][1]] for i in indices])\n",
        "        rewards = np.array([[self.transitions[i][2]] for i in indices])\n",
        "        new_states = np.array([self.transitions[i][3] for i in indices])\n",
        "        terminateds = np.array([[self.transitions[i][4]] for i in indices])\n",
        "\n",
        "        states_t = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
        "        actions_t = torch.as_tensor(actions, dtype=torch.int64, device=self.device)\n",
        "        rewards_t = torch.as_tensor(rewards, dtype=torch.float32, device=self.device)\n",
        "        new_states_t = torch.as_tensor(new_states, dtype=torch.float32, device=self.device)\n",
        "        terminateds_t = torch.as_tensor(terminateds, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        return states_t, actions_t, rewards_t, new_states_t, terminateds_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AOpJ5ULANY-"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "The `AtariPreprocessing` and `FrameStack` wrappers implement the preprocessing pipeline described in the paper. Using these wrappers, we resize each frame to 84x84 pixels and apply a grayscale filter,\n",
        "as well as repeat each action `FRAME_SKIP` (default: 4) times. In addition, we stack the last `HISTORY_LENGTH` (default: 4) frames, so the state\n",
        "is represented by the last 4 observations the agent experienced. Other preprocessing steps (described in the paper) are also applied.\n",
        "Below is a comparison of an observation before and after the preprocessing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol35Kqdgujux"
      },
      "outputs": [],
      "source": [
        "# Initialize environment with preprocessing\n",
        "def initialize_env(recording=False, show_comparison=False):\n",
        "    # Create base environment\n",
        "    env = gym.make('ALE/Breakout-v5', render_mode='rgb_array',\n",
        "               frameskip=1, repeat_action_probability=0)\n",
        "    env.metadata['render_fps'] = 60\n",
        "\n",
        "    # Show state before preprocessing\n",
        "    if show_comparison:\n",
        "        plt.figure(figsize=(11, 6))\n",
        "        plt.suptitle(\"The state before vs. after preprocessing\", fontsize=\"x-large\")\n",
        "        env.reset()\n",
        "        obs = env.step(1)[0]\n",
        "        ax = plt.subplot(121)\n",
        "        ax.tick_params(left=False, labelleft=False,\n",
        "                labelbottom=False, bottom=False)\n",
        "        ax.imshow(obs, cmap='gray')\n",
        "\n",
        "    # Apply recording wrapper\n",
        "    if recording:\n",
        "        trigger = lambda ep: ep in REC_EPISODES\n",
        "        env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n",
        "\n",
        "    # Apply preprocessing wrappers\n",
        "    env = AtariPreprocessing(env, noop_max=NOOP_MAX,\n",
        "                                  frame_skip=FRAME_SKIP, scale_obs=True)\n",
        "    env = FrameStack(env, HISTORY_LENGTH)\n",
        "\n",
        "    # Show state after preprocessing\n",
        "    if show_comparison:\n",
        "        env.reset()\n",
        "        env.step(1)\n",
        "        env.step(2)\n",
        "        env.step(3)\n",
        "        obs = env.step(3)[0]\n",
        "\n",
        "        for i in range(4):\n",
        "            ax = plt.subplot(243 + i + (2 if i > 1 else 0))\n",
        "            ax.imshow(obs[i], cmap='gray')\n",
        "            ax.tick_params(left=False, labelleft=False,\n",
        "                    labelbottom=False, bottom=False)\n",
        "        plt.show()\n",
        "        env.reset()\n",
        "\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZobHmcpdS1BU"
      },
      "outputs": [],
      "source": [
        "# Initialize replay memory\n",
        "env = initialize_env(show_comparison=True)\n",
        "replay_memory = ReplayMemory(env, MIN_REPLAY_SIZE, BUFFER_SIZE, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTcHt3IcwUDE"
      },
      "outputs": [],
      "source": [
        "# Initialize Q-networks\n",
        "online_net = Net(env, device).to(device)\n",
        "target_net = Net(env, device).to(device)\n",
        "\n",
        "# Initalize and copy weights\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        m.bias.data.fill_(0.0)\n",
        "\n",
        "    if type(m) == nn.Conv2d:\n",
        "        torch.nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "\n",
        "online_net.apply(init_weights)\n",
        "target_net.load_state_dict(online_net.state_dict())\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.Adam(online_net.parameters(), lr=ALPHA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXKEHCJBD3oC"
      },
      "source": [
        "## Training\n",
        "\n",
        "The training loop is quite similar to tabular Q-learning, the main difference is the way the updates are handled: instead of updating the Q-values every time based\n",
        "on the current transition, we store the transition in the reward buffer, and sample a batch of transitions every `OPTIM_FREQ` (default: 4) steps.\n",
        "\n",
        "***\n",
        "\n",
        "### **Your Task**\n",
        "\n",
        "Complete the training loop! Pseudocode is shown below.\n",
        "\n",
        "<img src=\"assets/dqn.png\" width=\"700\"/>\n",
        "\n",
        "*Pseudocode from \"Human-level control through deep reinforcement\n",
        "learning\" (Mnih et. al., 2015)*\n",
        "\n",
        "#### **Hints:**\n",
        "\n",
        "- It is important to keep track of your remaining lives and store the event of losing a life in the replay buffer. The easiest way to do this is to set terminated to `True` in the replay buffer when losing a life, even if the episode was not actually terminated. `env.ale.lives()` returns your current number of lives at any point in time.\n",
        "- You should only take an optimization step every `OPTIM_FREQ` (default: 4) steps.\n",
        "- $C$ in the pseudocode corresponds to the `TARGET_UPDATE_FREQ` parameter.\n",
        "- To keep track of the average loss and reward, append every loss to the `loss_buffer` and every cumulative episode reward to the `rew_buffer`.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDAiVYRXxmW0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Initialize environment\n",
        "env = initialize_env(recording=True)\n",
        "\n",
        "# Initialize log data\n",
        "rew_buffer = deque([0.], maxlen=80)\n",
        "loss_buffer = deque([0.], maxlen=80)\n",
        "n_steps = 0\n",
        "online_net.train()\n",
        "\n",
        "# Training loop\n",
        "for episode in (t := trange(N_EPISODES)):\n",
        "    # Set tqdm description\n",
        "    t.set_description(f\"Avg. loss: {np.mean(loss_buffer):.4f} | Avg. reward: {np.mean(rew_buffer):.2f}\")\n",
        "    t.refresh()\n",
        "\n",
        "    # TODO: Implement the algorithm\n",
        "\n",
        "    # Update epsilon\n",
        "    EPSILON = max(EPSILON - EPSILON_DECAY, EPSILON_MIN)\n",
        "\n",
        "    # Save model\n",
        "    if (episode + 1) % SAVE_FREQ == 0:\n",
        "        model_scripted = torch.jit.script(online_net)  # Export to TorchScript\n",
        "        model_scripted.save('model_scripted.pt')  # Save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LImPODT4D3oD"
      },
      "source": [
        "## Results\n",
        "\n",
        "You can watch the videos recorded throughout the training process here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl6dA1JFniFg"
      },
      "outputs": [],
      "source": [
        "# Display recordings\n",
        "children = [widgets.Video.from_file(f'./videos/rl-video-episode-{episode}.mp4', autoplay=False, loop=False, width=500) for episode in REC_EPISODES]\n",
        "tab = widgets.Tab()\n",
        "tab.children = children\n",
        "titles = tuple([f'Episode {episode + 1}' for episode in REC_EPISODES])\n",
        "for i in range(len(children)):\n",
        "    tab.set_title(i, titles[i])\n",
        "display(tab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz57-_FND3oD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}