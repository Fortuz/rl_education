{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b3355f-db24-460b-a3ee-f20d1db76181",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/4.%20Monte%20Carlo/blackjack.ipynb)\n",
    "\n",
    "# Blackjack\n",
    "\n",
    "Blackjack is a card game where the goal is to beat the dealer by obtaining cards that sum to closer to 21 (without going over 21) than the dealers cards.\n",
    "The game starts with the dealer having one face up and one face down card, while the player has two face up cards. All cards are drawn from an infinite deck (i.e. with replacement).\n",
    "\n",
    "<img src=\"assets/blackjack.gif\" width=\"500\"/>\n",
    "\n",
    "The card values are:\n",
    "\n",
    "- Face cards (Jack, Queen, King) have a point value of 10.\n",
    "- Aces can either count as 11 (called a ‘usable ace’) or 1.\n",
    "- Numerical cards (2-9) have a value equal to their number.\n",
    "\n",
    "The player has the sum of cards held. The player can request additional cards (*hit*) until they decide to stop (*stick*) or exceed 21 (*bust*, immediate loss).\n",
    "After the player sticks, the dealer reveals their facedown card, and draws cards until their sum is 17 or greater. If the dealer goes bust, the player wins.\n",
    "If neither the player nor the dealer busts, the outcome (win, lose, draw) is decided by whose sum is closer to 21. If the player has 21 immediately (an ace and a 10-card),\n",
    "it is called a *natural*. The player then wins unless the dealer also has a natural, in which case the game is a draw.\n",
    "\n",
    "We are going to use the `Blackjack-v1` environment of the `Gymnasium` library. The states are 3-component tuples, where the components are:\n",
    "\n",
    "1. The player's current sum (0 - 31)\n",
    "2. The dealer's face up card (1 - 10, where 1 is Ace)\n",
    "3. Whether the player has a usable ace (0 for no, 1 for yes)\n",
    "\n",
    "An action is either 0 (stick) or 1 (hit). The reward is +1 for winning the game, -1 for losing, and 0 for a draw.\n",
    "\n",
    "This example uses an *Off-policy Monte Carlo* algorithm to approximate the optimal policy.\n",
    "\n",
    "- This notebook is based on Chapter 5 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html\n",
    "- Documentation for the Blackjack environment: https://gymnasium.farama.org/environments/toy_text/blackjack/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71245af3-6764-484f-9813-fae48b9db8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install gymnasium==0.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92556a-8ee0-48ba-ad86-0d9ad56c8287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm.notebook import trange\n",
    "from matplotlib.patches import Patch\n",
    "import ipywidgets as widgets\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.print_figure_kwargs = {'pad_inches': .3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031787f-156e-4c3e-bcd1-ba8772c7f1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N_EPISODES = 4_000_000  # Number of training episodes\n",
    "EPSILON = 1  # Initial exploration\n",
    "EPSILON_MIN = 0.1  # Final exploration\n",
    "EPSILON_DECAY = EPSILON / (N_EPISODES / 2)  # Exploration decay rate\n",
    "GAMMA = 0.99  # Discount factor\n",
    "N_RECORDINGS = 3  # Number of episodes to record\n",
    "REC_EPISODES = np.linspace(0, N_EPISODES-1, num=N_RECORDINGS, dtype=int)  # Episodes to record\n",
    "LOG_FREQ = N_EPISODES / 10  # Progress log frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ae8ae-f5f7-454d-bc6b-258423e4bfd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create environment\n",
    "base_env = gym.make('Blackjack-v1', sab=True, render_mode='rgb_array')  # 'sab' for Sutton & Barto's version\n",
    "# Wrap environment to record videos throughout the learning process \n",
    "trigger = lambda ep: ep in REC_EPISODES\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb929640-f059-4dff-b57c-ad3bd1b390a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "observation_space_shape = tuple([dim.n for dim in env.observation_space])\n",
    "q_table_shape = observation_space_shape + (action_space_size, )\n",
    "\n",
    "# Create Q-table and weight matrix\n",
    "q_table = np.zeros(q_table_shape)\n",
    "weights = np.zeros(q_table_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5361a698-3ac2-4351-beb5-ec13a0267cb9",
   "metadata": {},
   "source": [
    "## Off-policy Monte Carlo Control\n",
    "\n",
    "*Monte Carlo* (MC) methods are iterative processes for finding the optimal policy and state-value function, similarly to Value Iteration. However, instead\n",
    "of doing sweeps of the complete state set, MC algorithms only simulate episodes. *Off-policy* means that the policy used to simulate episodes (*behaviour policy*) is different\n",
    "from the policy learned (*target policy*): in this case, the target policy is a deterministic policy $\\pi$, while the behaviour policy $b$ is an $\\varepsilon$-greedy policy, that\n",
    "uses $\\pi$'s state-value function to determine the best action for each state.\n",
    "\n",
    "Let's consider the *On-policy* version of this method first, that is, where the policy used to generate the episodes, $b$ is identical to the policy being evaulated, $\\pi$. We simulate an episode using the policy, and save the resulting trajectory: $S_0, A_0, R_1, S_1, A_1, ..., S_{T-1}, A_{T-1}, R_T$ ($T$ is the terminal time step).\n",
    "Looping backwards through the trajectory, we calculate the returns for each step, and update our value function so that the value of each state-action pair will be the\n",
    "average of the returns observed after visiting that state and taking that action. After that, we update the policy according to the new action-value function. As long as the policy is $\\varepsilon$-greedy, this method will converge to the best $\\varepsilon$-soft policy (a policy is $\\varepsilon$-soft if $\\pi(a|s) > \\frac{\\varepsilon}{|\\mathcal{A}(S_t)|}$ for all $a, s$).\n",
    "\n",
    "In contrast, the Off-policy method uses different behaviour and target policies. In this case, when updating the action-value function, we have to take into account\n",
    "the difference between $\\pi(a|s)$ and $b(a|s)$, since we use data generated by using $b$ to better estimate the action-value function of $\\pi$. We can account for this difference\n",
    "by calculating the *importance-sampling ratio*:\n",
    "\n",
    "$$ \\rho_{t:T-1} := \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)} $$\n",
    "\n",
    "The state-action value is then determined by the weighted average of all previous returns:\n",
    "\n",
    "$$ Q(s,a) := \\frac{\\sum_{t\\in\\mathcal{T}(s,a)} \\rho_{t:T(t)-1} G_t}{\\sum_{t\\in\\mathcal{T}(s,a)} \\rho_{t:T(t)-1}} $$\n",
    "\n",
    "Where $T(t)$ is the terminal time step of the episode where $t$ is sampled from, and $\\mathcal{T}(s,a)$ denotes the set of all time steps where action $a$ was selected in state $s$ (in any episode).\n",
    "\n",
    "Weighing the returns with the importance-sampling ratio lets us update the action-value function of $\\pi$ using only data generated with $b$. Using this method we are not limited\n",
    "to just $\\varepsilon$-soft methods: while the behaviour policy is more exploratory, the target policy can be deterministic.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement this algorithm! The block below only contains code necessary for logging the average reward at every `LOG_FREQ` iterations. The algorithm itself is up to you! Pseudocode for this algorithm is shown in the box below.\n",
    "\n",
    "<img src=\"assets/monte_carlo.png\" width=\"700\"/>\n",
    "\n",
    "*Pseudocode from page 111 of the Sutton & Barto book*\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- This pseudocode updates the $Q$-values (weighted averages) incrementally, similarly to the method described in the $k$-armed bandit chapter.\n",
    "- It is recommended to use an $\\varepsilon$-greedy policy as $b$.\n",
    "- Use the arrays defined above: `q_table` corresponds to $Q$, and `weights` corresponds to $C$ in the pseudocode.\n",
    "- When recording a trajectory, in addition to the state-action-reward sequence, you should store $b(A_t|S_t)$ at each step as well.\n",
    "- Since we need the tie-breaking of argmax to be consistent, it is better to use `np.argmax` this time instead of the `argmax` we defined in an earlier notebook.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbce15-2766-4f5a-8c6a-27875bd0cc41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Re-intialize environment, weights and Q-table\n",
    "env = RecordVideo(base_env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n",
    "q_table = np.zeros(q_table_shape)\n",
    "weights = np.zeros(q_table_shape)\n",
    "sum_rewards = 0\n",
    "\n",
    "# Training loop\n",
    "for episode in trange(N_EPISODES):\n",
    "\n",
    "    # TODO: Complete the algorithm!\n",
    "    \n",
    "    # Log results\n",
    "    if (episode + 1) % LOG_FREQ == 0:\n",
    "        print(f'Episode {episode + 1} : avg={sum_rewards / LOG_FREQ}')\n",
    "        sum_rewards = 0\n",
    "\n",
    "# Save Q-table\n",
    "with open('q_table.bin', 'wb') as f:\n",
    "    pickle.dump(q_table, f)\n",
    "    \n",
    "# Close environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4773e7-dc4f-4726-ac5c-527ccc93413e",
   "metadata": {},
   "source": [
    "## Final Value Function and Policy\n",
    "\n",
    "The plots for the final state-value function and policy are shown below. The player sum below 12 (or above 21) is irrelevant, so it is not shown here:\n",
    "in the with-usable-ace case, a sum less than 12 is not even possible, and in the no-usable-ace case, hitting is trivially better than sticking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6ab6fd-38bb-4f92-bb7d-d7b649d85a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get value functions\n",
    "state_value_no_usable_ace = np.max(q_table[12:22, 1:, 0, :], axis=-1)\n",
    "state_value_with_usable_ace = np.max(q_table[12:22, 1:, 1, :], axis=-1)\n",
    "\n",
    "x_tick_labels = ['A'] + list(range(2, 11))  # 'Ace' instead of 1\n",
    "\n",
    "x, y = np.meshgrid(np.arange(state_value_no_usable_ace.shape[0]),\n",
    "                   np.arange(state_value_no_usable_ace.shape[1]))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6), subplot_kw={'projection': '3d'})\n",
    "ax1, ax2 = axs\n",
    "\n",
    "# Options for both plots\n",
    "plt.setp(axs, xticks=np.arange(10),\n",
    "         xticklabels=x_tick_labels,\n",
    "         yticks=np.arange(10), yticklabels=np.arange(12, 22),\n",
    "         xlabel='Dealer showing', ylabel='Player sum', zlabel='State value')\n",
    "\n",
    "# 'No usable ace' plot\n",
    "ax1.plot_surface(x, y, state_value_no_usable_ace, cmap='magma')\n",
    "ax1.view_init(20, -40)\n",
    "ax1.set_title('No usable ace', y=1)\n",
    "\n",
    "# 'With usable ace' plot\n",
    "ax2.plot_surface(x, y, state_value_with_usable_ace, cmap='magma')\n",
    "ax2.view_init(20, -40)\n",
    "ax2.set_title('With usable ace', y=1)\n",
    "\n",
    "fig.suptitle('State-value function', fontsize=20, y=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5381ad-8465-4afd-b0d1-12dc4976108b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get policies\n",
    "policy_no_usable_aces = np.argmax(q_table[12:22, 1:, 0, :], axis=-1)\n",
    "policy_with_usable_aces = np.argmax(q_table[12:22, 1:, 1, :], axis=-1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1, ax2 = axs\n",
    "\n",
    "# 'No usable ace' plot\n",
    "sns.heatmap(np.flip(policy_no_usable_aces, axis=0), cmap='coolwarm',\n",
    "            linecolor='white', linewidths=0.05, square=True,\n",
    "            yticklabels=np.arange(21, 11, -1),\n",
    "            xticklabels=x_tick_labels,\n",
    "            cbar=False, ax=ax1)\n",
    "ax1.set_title('No usable ace')\n",
    "\n",
    "# 'With usable ace' plot\n",
    "sns.heatmap(np.flip(policy_with_usable_aces, axis=0), cmap='coolwarm',\n",
    "            linecolor='white', linewidths=0.05, square=True,\n",
    "            yticklabels=np.arange(21, 11, -1),\n",
    "            xticklabels=x_tick_labels,\n",
    "            cbar=False, ax=ax2)\n",
    "ax2.set_title('With usable ace')\n",
    "\n",
    "# Options for both plots\n",
    "plt.setp(axs, xlabel='Dealer showing', ylabel='Player sum')\n",
    "\n",
    "legend_elements = [\n",
    "        Patch(facecolor=\"#3b4cc0\", edgecolor=\"white\", label=\"Stick\"),\n",
    "        Patch(facecolor=\"#b50526\", edgecolor=\"white\", label=\"Hit\"),\n",
    "    ]\n",
    "\n",
    "fig.suptitle('Policy', fontsize=20)\n",
    "plt.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51b988-6f04-472a-b3a1-fd4afd700e1a",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "You can watch the videos recorded throughout the training process here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bee16a-53ed-4a4f-94c1-8d76426205c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display recordings\n",
    "children = [widgets.Video.from_file(f'./videos/rl-video-episode-{episode}.mp4', autoplay=False, loop=False, width=500) for episode in REC_EPISODES]\n",
    "tab = widgets.Tab()\n",
    "tab.children = children\n",
    "titles = tuple([f'Episode {episode + 1:,}' for episode in REC_EPISODES])\n",
    "for i in range(len(children)):\n",
    "    tab.set_title(i, titles[i])\n",
    "display(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777af19-bf16-4459-86aa-73909bba5ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
