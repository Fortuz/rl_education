{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "# Policy Network Definition\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Log standard deviation for continuous actions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.fc3(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "# Value Network Definition\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_size=128):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# TRPO Step (Using Conjugate Gradient and Line Search)\n",
    "def trpo_step(policy, states, actions, advantages, old_log_probs, max_kl=0.01):\n",
    "    # Compute policy gradient\n",
    "    mean, std = policy(states)\n",
    "    dist = MultivariateNormal(mean, torch.diag(std**2))\n",
    "    log_probs = dist.log_prob(actions)\n",
    "    loss = (log_probs.exp() / old_log_probs.exp() * advantages).mean()\n",
    "    \n",
    "    grads = torch.autograd.grad(loss, policy.parameters(), retain_graph=True)\n",
    "    flat_grads = torch.cat([grad.view(-1) for grad in grads])\n",
    "    \n",
    "    def fisher_vector_product(vector):\n",
    "        kl = (old_log_probs - log_probs).mean()\n",
    "        grads = torch.autograd.grad(kl, policy.parameters(), create_graph=True)\n",
    "        flat_grads = torch.cat([grad.view(-1) for grad in grads])\n",
    "        return flat_grads @ vector\n",
    "    \n",
    "    step_dir = conjugate_gradient(fisher_vector_product, -flat_grads)\n",
    "    max_step = torch.sqrt(2 * max_kl / (step_dir @ fisher_vector_product(step_dir))) * step_dir\n",
    "    \n",
    "    # Line search for best step size\n",
    "    new_params = get_flat_params_from(policy) + max_step\n",
    "    set_flat_params_to(policy, new_params)\n",
    "\n",
    "# Train TRPO on Mujoco Environment\n",
    "def train_trpo(env_name='LunarLanderContinuous-v2', episodes=1000):\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    policy = PolicyNetwork(state_dim, action_dim)\n",
    "    value_net = ValueNetwork(state_dim)\n",
    "    optimizer = optim.Adam(value_net.parameters(), lr=0.01)\n",
    "    \n",
    "    reward_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_rewards = []\n",
    "        log_probs = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        \n",
    "        for _ in range(200):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            mean, std = policy(state_tensor)\n",
    "            dist = MultivariateNormal(mean, torch.diag(std**2))\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            next_state, reward, done, _ = env.step(action.numpy())\n",
    "            \n",
    "            states.append(state_tensor)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            episode_rewards.append(reward)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Compute advantages\n",
    "        returns = compute_returns(episode_rewards)\n",
    "        advantages = returns - value_net(torch.stack(states)).squeeze()\n",
    "        \n",
    "        # Update value function\n",
    "        value_loss = nn.functional.mse_loss(value_net(torch.stack(states)).squeeze(), returns)\n",
    "        optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Perform TRPO step\n",
    "        trpo_step(policy, torch.stack(states), torch.stack(actions), advantages, torch.stack(log_probs))\n",
    "        reward_history.append(sum(episode_rewards))\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {sum(episode_rewards)}\")\n",
    "    \n",
    "    env.close()\n",
    "    return reward_history\n",
    "\n",
    "# Run training and plot results\n",
    "reward_history = train_trpo()\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"TRPO Training Performance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "# Policy Network Definition\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Log standard deviation for continuous actions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.fc3(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "# Value Network Definition\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_size=128):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# PPO Training Loop\n",
    "def train_ppo(env_name='LunarLanderContinuous-v2', episodes=1000, clip_epsilon=0.2, gamma=0.99):\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    policy = PolicyNetwork(state_dim, action_dim)\n",
    "    value_net = ValueNetwork(state_dim)\n",
    "    policy_optimizer = optim.Adam(policy.parameters(), lr=0.0003)\n",
    "    value_optimizer = optim.Adam(value_net.parameters(), lr=0.001)\n",
    "    \n",
    "    reward_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_rewards = []\n",
    "        log_probs = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        values = []\n",
    "        \n",
    "        for _ in range(200):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            mean, std = policy(state_tensor)\n",
    "            dist = MultivariateNormal(mean, torch.diag(std**2))\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            value = value_net(state_tensor)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action.numpy())\n",
    "            \n",
    "            states.append(state_tensor)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Compute advantages\n",
    "        returns = compute_returns(episode_rewards, gamma)\n",
    "        advantages = returns - torch.stack(values).squeeze()\n",
    "        \n",
    "        # Compute policy loss with PPO clipping\n",
    "        mean, std = policy(torch.stack(states))\n",
    "        new_dist = MultivariateNormal(mean, torch.diag(std**2))\n",
    "        new_log_probs = new_dist.log_prob(torch.stack(actions))\n",
    "        ratio = torch.exp(new_log_probs - torch.stack(log_probs))\n",
    "        \n",
    "        surrogate1 = ratio * advantages\n",
    "        surrogate2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "        \n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        # Update value function\n",
    "        value_loss = nn.functional.mse_loss(torch.stack(values).squeeze(), returns)\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "        \n",
    "        reward_history.append(sum(episode_rewards))\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Reward: {sum(episode_rewards)}\")\n",
    "    \n",
    "    env.close()\n",
    "    return reward_history\n",
    "\n",
    "# Run training and plot results\n",
    "reward_history = train_ppo()\n",
    "plt.plot(reward_history)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"PPO Training Performance\")\n",
    "plt.show()e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchrl\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchrl.envs import GymEnv\n",
    "from torchrl.data import TensorDictReplayBuffer\n",
    "from torchrl.modules import ProbabilisticActor, ValueOperator\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.trainers import make_trainer\n",
    "\n",
    "# Define environment\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "env = GymEnv(env_name)\n",
    "state_dim = env.observation_spec.shape[0]\n",
    "action_dim = env.action_spec.shape[0]\n",
    "\n",
    "# Define policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.fc3(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "# Define value network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialize networks\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "value_net = ValueNetwork(state_dim)\n",
    "\n",
    "# Create actor and critic operators\n",
    "actor = ProbabilisticActor(policy, action_spec=env.action_spec, distribution_class=torch.distributions.Normal)\n",
    "critic = ValueOperator(value_net)\n",
    "\n",
    "# Define PPO Loss\n",
    "ppo_loss = ClipPPOLoss(actor, critic, clip_epsilon=0.2, entropy_bonus=True)\n",
    "\n",
    "# Set up optimizers\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=3e-4)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Set up replay buffer\n",
    "buffer = TensorDictReplayBuffer(size=100000)\n",
    "\n",
    "# Define trainer\n",
    "trainer = make_trainer(\n",
    "    loss_module=ppo_loss,\n",
    "    env=env,\n",
    "    buffer=buffer,\n",
    "    optimizer=[policy_optimizer, value_optimizer],\n",
    "    batch_size=64,\n",
    "    max_epochs=1000,\n",
    ")\n",
    "\n",
    "# Train PPO model\n",
    "trainer.train()\n",
    "\n",
    "# Plot results\n",
    "plt.plot(trainer.episode_rewards)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"PPO Training Performance with TorchRL\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
