{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848daf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71323ea5-f77e-42cd-8b5e-c1ff89e2c6b2",
   "metadata": {},
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**, **Balázs Nagy** and **Zoltán Barta**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/1.%20K-armed%20Bandit/homework.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714920cc",
   "metadata": {},
   "source": [
    "## Homework 1 (1 Point): Extending the $\\varepsilon$-Greedy Action Selection Strategy to an Associative Bandit Problem\n",
    "\n",
    "### Problem Description\n",
    "\n",
    "The **Associative Bandit Problem (ABP)** is an extension of the classic **multi-armed bandit problem**, where instead of a single set of actions, we have multiple **K-armed bandit problems**, each associated with a different state. The system provides a state at each time step, and the strategy must learn an optimal action selection policy for each state.\n",
    "\n",
    "### Key Features:\n",
    "1. **Multiple States**: The problem consists of different states (e.g., colors such as Red, Blue, Yellow), where each state has its own stationary reward distribution.\n",
    "2. **State-Dependent Rewards**: The reward distributions differ across states, meaning an action may yield different rewards depending on the current state.\n",
    "3. **Learning Over Time**: The strategy must adapt and learn the best action for each state through experience.\n",
    "\n",
    "### Task:\n",
    "\n",
    "Extend the **$\\varepsilon$-greedy action selection strategy** to handle the **Associative Bandit Problem** by incorporating state-awareness into the decision-making process. Specifically:\n",
    "\n",
    "- Modify the **action-value estimates** to be **state-dependent**, meaning each state maintains its own estimates for each action.\n",
    "- Implement the **$\\varepsilon$-greedy strategy**:\n",
    "  - With probability **$1 - \\varepsilon$**, select the action with the highest estimated reward for the given state.\n",
    "  - With probability **$\\varepsilon$**, explore by choosing an action randomly.\n",
    "- Update the action-value estimates separately for each state-action pair.\n",
    "\n",
    "### Implementation Considerations:\n",
    "- The agent receives the **index of the current state** at each timestep.\n",
    "- The learning process should ensure convergence to the optimal policy for each state.\n",
    "- Track and update the rewards for each **(state, action) pair** independently.\n",
    "\n",
    "The goal is to develop an **adaptive $\\varepsilon$-greedy strategy** that can efficiently learn the best action for each state while balancing exploration and exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06156ef-5366-4790-af7c-a5b9d31278c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0528443-6a12-4109-b9cd-a42fc724919e",
   "metadata": {},
   "source": [
    "Numpy's `np.argmax` will choose the smallest index in case there are multiple\n",
    "maximal values. This function breaks these ties randomly instead, which is\n",
    "desirable in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48f7a0-8634-4980-8503-1d1567b5033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# Argmax function that breaks ties randomly\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c7d13",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We test our strategies by trying them out in multiple runs, and then averaging out the received reward at each time step. After that, we plot the results to\n",
    "compare the strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "class KArmedBandit:\n",
    "    def __init__(self, K, mean=0, std_dev=1):\n",
    "        \"\"\"\n",
    "        Initializes the K-armed bandit with normally distributed action values.\n",
    "        \n",
    "        Parameters:\n",
    "        - K (int): Number of arms.\n",
    "        - mean (float): Mean of the normal distribution for optimal action values.\n",
    "        - std_dev (float): Standard deviation for the optimal action values.\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.optimal_action_values = np.random.normal(loc=mean, scale=std_dev, size=K)  # True action values\n",
    "        self.best_action = np.argmax(self.optimal_action_values)  # Best action index\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        \"\"\"\n",
    "        Returns a stochastic reward from a normal distribution centered at the true action value.\n",
    "        \n",
    "        Parameters:\n",
    "        - action (int): The index of the chosen action.\n",
    "\n",
    "        Returns:\n",
    "        - reward (float): The observed reward for the selected action.\n",
    "        \"\"\"\n",
    "        return np.random.normal(loc=self.optimal_action_values[action], scale=1.0)\n",
    "        # Reduce spread\n",
    "\n",
    "    def get_optimal_action(self):\n",
    "        \"\"\"\n",
    "        Returns the index of the optimal action (the arm with the highest expected reward).\n",
    "        \"\"\"\n",
    "        return self.best_action\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the bandit by re-generating the optimal action values.\n",
    "        \"\"\"\n",
    "        self.optimal_action_values = np.random.normal(loc=0, scale=1, size=self.K)\n",
    "        self.best_action = np.argmax(self.optimal_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246809db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "class AssociativeKArmedBandit:\n",
    "    def __init__(self, K, number_of_possible_states, mean=0, std_dev=0.5):\n",
    "        \"\"\"\n",
    "        Initializes the K-armed bandit with normally distributed action values.\n",
    "        \n",
    "        Parameters:\n",
    "        - K (int): Number of arms.\n",
    "        - number_of_possible_states (int): Number of different states.\n",
    "        - mean (float): Mean of the normal distribution for action values.\n",
    "        - std_dev (float): Standard deviation for the action values.\n",
    "        \"\"\"\n",
    "        self.number_of_possible_states = number_of_possible_states\n",
    "        self.bandits = [KArmedBandit(K, mean, std_dev) for _ in range(number_of_possible_states)]\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns a stochastic reward from a normal distribution centered at the true action value.\n",
    "        \n",
    "        Parameters:\n",
    "        - state (int): The current state.\n",
    "        - action (int): The index of the chosen action.\n",
    "\n",
    "        Returns:\n",
    "        - reward (float): The observed reward for the selected action.\n",
    "        \"\"\"\n",
    "        return self.bandits[state].get_reward(action)  \n",
    "\n",
    "    def get_optimal_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns the index of the optimal action (the arm with the highest expected reward).\n",
    "        \n",
    "        Parameters:\n",
    "        - state (int): The current state.\n",
    "\n",
    "        Returns:\n",
    "        - (int): Index of the optimal action for the given state.\n",
    "        \"\"\"\n",
    "        return self.bandits[state].get_optimal_action()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets all bandits by re-generating their optimal action values.\n",
    "        \"\"\"\n",
    "        for bandit in self.bandits:\n",
    "            bandit.reset()  # <-- Fixed: Removed the incorrect syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "def plotResults(strategies, rewards, best_action_choices):\n",
    "  fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "\n",
    "  for strategy, reward in zip(strategies, rewards):\n",
    "      ax1.plot(reward, label=f\"{strategy.name}\", zorder=2)\n",
    "  ax1.set_xlabel('Steps')\n",
    "  ax1.set_ylabel('Average reward')\n",
    "  ax1.grid(alpha=0.8, linestyle=':', zorder=0)\n",
    "  ax1.set_title('Average reward of strategies')\n",
    "  ax1.legend()\n",
    "\n",
    "  for strategy, choices in zip(strategies, best_action_choices):\n",
    "      ax2.plot(choices, label=f\"{strategy.name}\")\n",
    "  ax2.set_xlabel('Steps')\n",
    "  ax2.set_ylabel('% Optimal action')\n",
    "  ax2.grid(alpha=0.8, linestyle=':', zorder=0)\n",
    "  ax2.set_title('% Optimal action choices of strategies')\n",
    "  ax2.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "def simulate_associative(strategies, K, number_of_states, bandit_mean =0,bandit_std = 1,runs=2000, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Simulates the associative K-armed bandit problem with different strategies.\n",
    "\n",
    "    Parameters:\n",
    "    - strategies (list): A list of strategies to evaluate.\n",
    "    - K (int): Number of arms.\n",
    "    - number_of_states (int): Number of possible states.\n",
    "    - runs (int): Number of independent runs.\n",
    "    - n_steps (int): Number of time steps per run.\n",
    "\n",
    "    Returns:\n",
    "    - mean_rewards (np.array): Average rewards per time step for each strategy.\n",
    "    - mean_best_action_choices (np.array): Average probability of selecting the best action.\n",
    "    \"\"\"\n",
    "    rewards = np.zeros((len(strategies), runs, n_steps))\n",
    "    best_action_choices = np.zeros(rewards.shape)\n",
    "\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        print(f\"Evaluating strategy {i + 1}/{len(strategies)}...\")\n",
    "\n",
    "        for r in range(runs):\n",
    "            # Initialize an associative K-armed bandit instance\n",
    "            bandit = AssociativeKArmedBandit(K, number_of_states, mean=bandit_mean, std_dev=bandit_std)\n",
    "            bandit.reset()\n",
    "            \n",
    "            strategy.reset()  # Reset strategy state\n",
    "\n",
    "            for t in range(n_steps):\n",
    "                state = np.random.randint(0, number_of_states)  # Randomly select a state\n",
    "                action = strategy.act(state)  # Strategy selects an action based on the state\n",
    "                reward = bandit.get_reward(state, action)  # Get the reward for the selected action in the given state\n",
    "                \n",
    "                rewards[i, r, t] = reward\n",
    "               \n",
    "                strategy.update(state, action, reward)  # Update strategy based on state\n",
    "\n",
    "                best_action = bandit.get_optimal_action(state)  # Get the best action for the current state\n",
    "                if action == best_action:\n",
    "                    best_action_choices[i, r, t] = 1  # Track if the best action was chosen\n",
    "\n",
    "\n",
    "    # Compute mean rewards and best action selection frequency\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "    mean_best_action_choices = best_action_choices.mean(axis=1)\n",
    "\n",
    "    return mean_rewards, mean_best_action_choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66818fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssociativeEpsilonGreedy:\n",
    "    \n",
    "    def __init__(self, num_states, num_actions, epsilon=0.1):\n",
    "        ############## CODE HERE ###################\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        \n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns a formatted string representing the strategy name.\n",
    "        \"\"\"\n",
    "        return f\"AssociativeEpsilonGreedy(epsilon={self.epsilon}, alpha={self.alpha})\"\n",
    "\n",
    "    def act(self, state):\n",
    "        ############## CODE HERE ###################\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "    \n",
    "    def update(self, state, action, reward):\n",
    "        ############## CODE HERE ###################\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "    \n",
    "    def reset(self):\n",
    "        ############## CODE HERE ###################\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1dd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10  # Number of actions\n",
    "number_of_states = 5  # Number of possible states\n",
    "# List of strategies to test\n",
    "strategies = [AssociativeEpsilonGreedy(number_of_states,K, epsilon=0.1),\n",
    "              AssociativeEpsilonGreedy(number_of_states,K, epsilon=0.01),\n",
    "              AssociativeEpsilonGreedy(number_of_states,K, epsilon=0.0)]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate_associative(strategies, K=K, number_of_states=number_of_states,runs=200, n_steps=10_000)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07ebbb9edfac425baf9a2dbb7d34c67a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "08766d7e7ba9483e889112dc5d026a10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e5ecb38705794f79a3a1e5e6a308e4eb",
       "style": "IPY_MODEL_9687d120372c482cb74b71c7826d0cc6",
       "value": "100%"
      }
     },
     "0af20d17977d4ef3bc8b423bb4f994dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "14742ccc27e140a894ca7581a8837a8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1a4843195585449ca0cf41a8532ce173": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1e0e04f407834c56a3da5d5e777b2285": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1e3d3c8b492a42de9eed9aec4d252529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2b9c79b37c5242d3a9da596fbec0ea22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "317dc691dcac4d9681d5a0c2713f1db1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4805e0f1568c4584a5fa7db25627e6a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0af20d17977d4ef3bc8b423bb4f994dd",
       "max": 2000,
       "style": "IPY_MODEL_1a4843195585449ca0cf41a8532ce173",
       "value": 2000
      }
     },
     "548e34805d784045ab2d6b810edc9ebf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_590dd8bd235343ca98c7715a0ded63fb",
       "max": 2000,
       "style": "IPY_MODEL_dcb0315e8f8e496ea3d3e80a0f6c6d4f",
       "value": 2000
      }
     },
     "590dd8bd235343ca98c7715a0ded63fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5ecda2fcb2684cef9d35f7062dd2af53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ab2a3b1a4944bdfbf95ba0dbe11cd12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b8afc7c172f410d937031881e6bf8cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6d27ff26fef344a6882007ad47030213": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75513d022da1452a952aff7f074f9964": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "755413d3284f4720a61398776719798e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75c9a46c4e3b4a429e09a4a05ddec642": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7b11c9fb1e2f4cc9b1fa70edd23a3301": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c78ffad4cdc042f3afe5efc727eabb31",
       "style": "IPY_MODEL_2b9c79b37c5242d3a9da596fbec0ea22",
       "value": "100%"
      }
     },
     "7ccb1685b35e419e9d43abfe152bd306": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8952aaa7bb334b1ab795d51432c1fe9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7ccb1685b35e419e9d43abfe152bd306",
       "style": "IPY_MODEL_e8d730fe69314cd78fddf187d4662416",
       "value": " 2000/2000 [01:04&lt;00:00, 31.99it/s]"
      }
     },
     "8db846b5eb7043999faa9bbb9cfa82b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9687d120372c482cb74b71c7826d0cc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a7760f48fe6c4720a61c8cf8573c2634": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a7e8cca8ac1646cfa2f2202f07f0f135": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_08766d7e7ba9483e889112dc5d026a10",
        "IPY_MODEL_c2762225efb04bc0af5daf619bd9989f",
        "IPY_MODEL_8952aaa7bb334b1ab795d51432c1fe9a"
       ],
       "layout": "IPY_MODEL_b9ad02240b1b4daca607fb7de9765cb7"
      }
     },
     "ae082241625f42b49f7d0ea17aabf42e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_6b8afc7c172f410d937031881e6bf8cf",
       "max": 2000,
       "style": "IPY_MODEL_1e3d3c8b492a42de9eed9aec4d252529",
       "value": 2000
      }
     },
     "b12033f10b0c41bd91a85f257d33034a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b491177b51504242b2150455e40391e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5ecda2fcb2684cef9d35f7062dd2af53",
       "style": "IPY_MODEL_07ebbb9edfac425baf9a2dbb7d34c67a",
       "value": " 2000/2000 [01:50&lt;00:00, 18.25it/s]"
      }
     },
     "b7eacbdc3f1049d282a40f13db58a1db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c855a5a97f354c72ada6fc3d6907fd85",
        "IPY_MODEL_548e34805d784045ab2d6b810edc9ebf",
        "IPY_MODEL_cdf8f62184534aa39a6fa121d64ec6c0"
       ],
       "layout": "IPY_MODEL_b12033f10b0c41bd91a85f257d33034a"
      }
     },
     "b9ad02240b1b4daca607fb7de9765cb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c2762225efb04bc0af5daf619bd9989f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_dddb73e1746e4faa8caa1f1c092e7094",
       "max": 2000,
       "style": "IPY_MODEL_8db846b5eb7043999faa9bbb9cfa82b8",
       "value": 2000
      }
     },
     "c78ffad4cdc042f3afe5efc727eabb31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c855a5a97f354c72ada6fc3d6907fd85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_317dc691dcac4d9681d5a0c2713f1db1",
       "style": "IPY_MODEL_75513d022da1452a952aff7f074f9964",
       "value": "100%"
      }
     },
     "cdf8f62184534aa39a6fa121d64ec6c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_755413d3284f4720a61398776719798e",
       "style": "IPY_MODEL_6ab2a3b1a4944bdfbf95ba0dbe11cd12",
       "value": " 2000/2000 [01:06&lt;00:00, 28.54it/s]"
      }
     },
     "d85d7d1fcfd14f778606850604811de6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7b11c9fb1e2f4cc9b1fa70edd23a3301",
        "IPY_MODEL_4805e0f1568c4584a5fa7db25627e6a6",
        "IPY_MODEL_dd6de22286e741abb1d66494a2b2192f"
       ],
       "layout": "IPY_MODEL_6d27ff26fef344a6882007ad47030213"
      }
     },
     "dcb0315e8f8e496ea3d3e80a0f6c6d4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dd6de22286e741abb1d66494a2b2192f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1e0e04f407834c56a3da5d5e777b2285",
       "style": "IPY_MODEL_14742ccc27e140a894ca7581a8837a8e",
       "value": " 2000/2000 [01:33&lt;00:00, 22.06it/s]"
      }
     },
     "dddb73e1746e4faa8caa1f1c092e7094": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e5ecb38705794f79a3a1e5e6a308e4eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e8d730fe69314cd78fddf187d4662416": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eaa0458b4e3a4ff2b49022e1620b53c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fb4e5dc2536e4f8eb9c70fd4cc3b311b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a7760f48fe6c4720a61c8cf8573c2634",
       "style": "IPY_MODEL_75c9a46c4e3b4a429e09a4a05ddec642",
       "value": "100%"
      }
     },
     "fc1e38ec9e2d42b29633452a708016af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fb4e5dc2536e4f8eb9c70fd4cc3b311b",
        "IPY_MODEL_ae082241625f42b49f7d0ea17aabf42e",
        "IPY_MODEL_b491177b51504242b2150455e40391e8"
       ],
       "layout": "IPY_MODEL_eaa0458b4e3a4ff2b49022e1620b53c4"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
