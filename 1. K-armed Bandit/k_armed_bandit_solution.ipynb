{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71323ea5-f77e-42cd-8b5e-c1ff89e2c6b2",
   "metadata": {},
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**, **Balázs Nagy** and **Zoltán Barta**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/1.%20K-armed%20Bandit/k_armed_bandit_solution.ipynb)\n",
    "\n",
    "# $k$-armed Bandit (solution)\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among\n",
    "$k$ different options, or actions. After each choice, you receive a numerical reward\n",
    "from a stationary probability distribution that depends on the action you selected. Your\n",
    "objective is to maximize the expected total reward over some time period, for example,\n",
    "over 1000 action selections, or time steps.\n",
    "\n",
    "This is called the $k$-armed bandit problem. You can visualize this problem as having to\n",
    "choose between $k$ slot machines (also known as one-armed bandits) at each time step,\n",
    "each of which has a different probability distribution for rewards - that is where the name comes from.\n",
    "\n",
    "<img src=\"assets/k_armed_bandit.png\" width=\"500\"/>\n",
    "\n",
    "The $k$-armed bandit problem illustrates an important problem in reinforcement\n",
    "learning: **exploration vs. exploitation**. At each time step $t$, the agent has to make a decision:\n",
    "take the action with the highest expected reward according to its current knowledge of the environment, \n",
    "or choose a different action to get a better estimation of the value of that action. The former is called an\n",
    "*exploitation* step, because it exploits the current knowledge of the agent in order to obtain a high reward.\n",
    "The latter is called an *exploration* step, since it involves trying out an action in order to have a better\n",
    "estimation of its value, thereby exploring the environment.\n",
    "\n",
    "This notebook introduces a few common strategies to tackle this problem and puts them to the test by simulating\n",
    "multiple test runs, and comparing the results.\n",
    "\n",
    "- This notebook is based on Chapter 2 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06156ef-5366-4790-af7c-a5b9d31278c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184eca01-435a-4c1e-a58d-4374e4b75123",
   "metadata": {},
   "source": [
    "## Strategy setup\n",
    "\n",
    "The `Strategy` base class is used to implement startegies for action selection. An action is selected by the `act` method, and then the `update` method is used\n",
    "to update the inner state after receiving a reward for the selected action. After an episode (a \"run\" consisting of $n$ steps, 1000 for example) is over, the `reset` method is called to reset the inner state of the class. The `name` propery is used get a name for the strategy in a visual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1ff81-334f-4c0b-bf3d-eade83e9c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "class Strategy(ABC):\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k  # Number of actions\n",
    "         \n",
    "        self.rewards_history = {i: [] for i in range(self.k)}  # Store observed rewards per action\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def act(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, action, reward):\n",
    "        self.rewards_history[action].append(reward)  # Update rewards history\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        self.rewards_history = {i: [] for i in range(self.k)}  # Reset reward history\n",
    "        pass\n",
    "\n",
    "    def plot_estimated_distributions(self):\n",
    "        \"\"\"\n",
    "        Plots the estimated reward distributions for each action\n",
    "        based on the rewards observed during training.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        for action, rewards in self.rewards_history.items():\n",
    "            if rewards:\n",
    "                sns.kdeplot(rewards, label=f\"Action {action+1}\", fill=True, alpha=0.5)\n",
    "        \n",
    "        plt.xlabel(\"Estimated Reward\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.title(f\"Estimated Reward Distributions - {self.name}\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0528443-6a12-4109-b9cd-a42fc724919e",
   "metadata": {},
   "source": [
    "Numpy's `np.argmax` will choose the smallest index in case there are multiple\n",
    "maximal values. This function breaks these ties randomly instead, which is\n",
    "desirable in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48f7a0-8634-4980-8503-1d1567b5033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# Argmax function that breaks ties randomly\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c7d13",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We test our strategies by trying them out in multiple runs, and then averaging out the received reward at each time step. After that, we plot the results to\n",
    "compare the strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit:\n",
    "    def __init__(self, K, mean=0, std_dev=1):\n",
    "        \"\"\"\n",
    "        Initializes the K-armed bandit with normally distributed action values.\n",
    "        \n",
    "        Parameters:\n",
    "        - K (int): Number of arms.\n",
    "        - mean (float): Mean of the normal distribution for optimal action values.\n",
    "        - std_dev (float): Standard deviation for the optimal action values.\n",
    "        \"\"\"\n",
    "        self.K = K\n",
    "        self.optimal_action_values = np.random.normal(loc=mean, scale=std_dev, size=K)  # True action values\n",
    "        self.best_action = np.argmax(self.optimal_action_values)  # Best action index\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        \"\"\"\n",
    "        Returns a stochastic reward from a normal distribution centered at the true action value.\n",
    "        \n",
    "        Parameters:\n",
    "        - action (int): The index of the chosen action.\n",
    "\n",
    "        Returns:\n",
    "        - reward (float): The observed reward for the selected action.\n",
    "        \"\"\"\n",
    "        return np.random.normal(loc=self.optimal_action_values[action], scale=1.0)\n",
    "        # Reduce spread\n",
    "\n",
    "    def get_optimal_action(self):\n",
    "        \"\"\"\n",
    "        Returns the index of the optimal action (the arm with the highest expected reward).\n",
    "        \"\"\"\n",
    "        return self.best_action\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the bandit by re-generating the optimal action values.\n",
    "        \"\"\"\n",
    "        self.optimal_action_values = np.random.normal(loc=0, scale=1, size=self.K)\n",
    "        self.best_action = np.argmax(self.optimal_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bandit_distributions(bandit, num_samples=10000):\n",
    "    \"\"\"\n",
    "    Plots the reward distributions for all K actions in a K-armed bandit using a violin plot.\n",
    "\n",
    "    Parameters:\n",
    "    - bandit (KArmedBandit): An instance of the KArmedBandit class.\n",
    "    - num_samples (int): Number of reward samples to generate for each action.\n",
    "    \"\"\"\n",
    "    K = bandit.K  # Number of actions\n",
    "    rewards = {action: [bandit.get_reward(action) for _ in range(num_samples)] for action in range(K)}\n",
    "\n",
    "    # Convert to data format suitable for seaborn\n",
    "    reward_data = []\n",
    "    action_labels = []\n",
    "    \n",
    "    for action, reward_list in rewards.items():\n",
    "        reward_data.extend(reward_list)\n",
    "        action_labels.extend([action + 1] * num_samples)  # Convert 0-indexed to 1-indexed for display\n",
    "\n",
    "    # Create violin plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.violinplot(x=action_labels, y=reward_data, inner=None, color=\"lightblue\", linewidth=1.5)\n",
    "\n",
    "    # Add scatter points for true action values\n",
    "    plt.scatter(range(0, K ), bandit.optimal_action_values, color='blue', s=50)\n",
    "\n",
    "    # Formatting\n",
    "    plt.axhline(0, linestyle='dotted', color='black', linewidth=1)  # Dashed line at 0\n",
    "    plt.xlabel(\"Actions\")\n",
    "    plt.ylabel(\"Expected Reward\")\n",
    "    plt.title(\"Action Reward Distributions in K-Armed Bandit\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78397eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 10-armed bandit\n",
    "bandit = KArmedBandit(K=10)\n",
    "\n",
    "# Plot the full action reward distributions\n",
    "plot_bandit_distributions(bandit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffca1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "def simulate(strategies, K, runs=2000, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Simulates the K-armed bandit problem with different strategies.\n",
    "\n",
    "    Parameters:\n",
    "    - strategies (list): A list of strategies to evaluate.\n",
    "    - K (int): Number of arms.\n",
    "    - runs (int): Number of independent runs.\n",
    "    - n_steps (int): Number of time steps per run.\n",
    "\n",
    "    Returns:\n",
    "    - mean_rewards (np.array): Average rewards per time step for each strategy.\n",
    "    - mean_best_action_choices (np.array): Average probability of selecting the best action.\n",
    "    \"\"\"\n",
    "    rewards = np.zeros((len(strategies), runs, n_steps))\n",
    "    best_action_choices = np.zeros(rewards.shape)\n",
    "\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        print(f\"Evaluating strategy {i + 1}/{len(strategies)}...\")\n",
    "\n",
    "        for r in trange(runs):\n",
    "            # Initialize a new K-armed bandit instance\n",
    "            bandit = KArmedBandit(K)\n",
    "            best_action = bandit.get_optimal_action()  # Get the best possible action\n",
    "\n",
    "            strategy.reset()  # Reset strategy state\n",
    "\n",
    "            for t in range(n_steps):\n",
    "                action = strategy.act()  # Strategy selects an action\n",
    "                reward = bandit.get_reward(action)  # Bandit returns a reward\n",
    "                \n",
    "                rewards[i, r, t] = reward\n",
    "                strategy.update(action, reward)  # Update strategy\n",
    "\n",
    "                if action == best_action:\n",
    "                    best_action_choices[i, r, t] = 1  # Track if best action was chosen\n",
    "\n",
    "            time.sleep(0.00001)  # For tqdm (progress bar smoothness)\n",
    "\n",
    "    # Compute mean rewards and best action selection frequency\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "    mean_best_action_choices = best_action_choices.mean(axis=1)\n",
    "\n",
    "    return mean_rewards, mean_best_action_choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a337157",
   "metadata": {},
   "source": [
    "To examine the results a plot function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "def plotResults(strategies, rewards, best_action_choices):\n",
    "  fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "\n",
    "  for strategy, reward in zip(strategies, rewards):\n",
    "      ax1.plot(reward, label=f\"{strategy.name}\", zorder=2)\n",
    "  ax1.set_xlabel('Steps')\n",
    "  ax1.set_ylabel('Average reward')\n",
    "  ax1.grid(alpha=0.8, linestyle=':', zorder=0)\n",
    "  ax1.set_title('Average reward of strategies')\n",
    "  ax1.legend()\n",
    "\n",
    "  for strategy, choices in zip(strategies, best_action_choices):\n",
    "      ax2.plot(choices, label=f\"{strategy.name}\")\n",
    "  ax2.set_xlabel('Steps')\n",
    "  ax2.set_ylabel('% Optimal action')\n",
    "  ax2.grid(alpha=0.8, linestyle=':', zorder=0)\n",
    "  ax2.set_title('% Optimal action choices of strategies')\n",
    "  ax2.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79096804-a20e-499d-abdc-4a187ab3ac01",
   "metadata": {},
   "source": [
    "## $\\varepsilon$-greedy Action Selection\n",
    "\n",
    "With this method, the agent will select a random action with an $\\varepsilon$ probability ($0 \\le \\varepsilon \\le 1$), and act greedily (select the best action according to its knowledge) otherwise. The action values are calculated using the *sample-averages* method: the value of an action is the average of all the rewards received after taking that action.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement the $\\varepsilon$-greedy action selection method! You should complete all methods of the class below, and\n",
    "you can add new class methods and constructor arguments as needed. Pseudocode for this algorithm is shown in the box below.\n",
    "\n",
    "<img src=\"assets/epsilon_greedy.png\" width=\"700\"/>\n",
    "\n",
    "*Pseudocode from page 32 of the Sutton & Barto book*\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- Use the argmax function defined above!\n",
    "- There is a simple way to incremetally calculate the average reward for a given action: $Q_{n+1} = Q_n + \\frac{1}{n}\\left[R_n-Q_n\\right]$ (See page 31 of the Sutton & Barto book for details)\n",
    "- To test your implementation, instantiate this class in the `strategies` list at the bottom of the notebook and run the simulation.\n",
    "- Try experimenting with different values of $\\varepsilon$!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EpsilonGreedy(Strategy):\n",
    "    def __init__(self, k, epsilon=0, initial=0):\n",
    "        \"\"\"\n",
    "        Initializes the epsilon-greedy strategy.\n",
    "\n",
    "        Parameters:\n",
    "        - k (int): Number of actions (arms).\n",
    "        - epsilon (float): Probability of exploring a random action.\n",
    "        - initial (float): Initial value for action-value estimates.\n",
    "        \"\"\"\n",
    "        super().__init__(k)  # Initialize parent Strategy class\n",
    "        self.epsilon = epsilon  # Probability of exploration (random action selection)\n",
    "        self.initial = initial  # Initial estimate for action values\n",
    "        self.q_estimations = np.zeros(self.k) + self.initial  # Estimated values of each action\n",
    "        self.selections = np.zeros(self.k)  # Number of times each action is selected\n",
    "        self.indices = np.arange(self.k)  # List of possible action indices\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns a formatted string representing the strategy name.\n",
    "        \"\"\"\n",
    "        name_str = \"Greedy\" if self.epsilon == 0 else f\"$\\\\varepsilon$-greedy, $\\\\varepsilon = {self.epsilon}$\"\n",
    "        if self.initial != 0:\n",
    "            name_str += f\", init: {self.initial}\"  # Include initial value if non-zero\n",
    "        return name_str\n",
    "\n",
    "    def act(self):\n",
    "        \"\"\"\n",
    "        Selects an action based on the epsilon-greedy policy.\n",
    "\n",
    "        Returns:\n",
    "        - (int) Index of the selected action.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:  # With probability epsilon, explore\n",
    "            return np.random.choice(self.indices)\n",
    "        else:  # Otherwise, exploit (choose the best known action)\n",
    "            return np.argmax(self.q_estimations)  \n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Updates the action-value estimate using sample averaging.\n",
    "\n",
    "        Parameters:\n",
    "        - action (int): Index of the chosen action.\n",
    "        - reward (float): Reward received from the environment.\n",
    "        \"\"\"\n",
    "        super().update(action, reward)  # Call parent class update method if needed\n",
    "        self.selections[action] += 1  # Increment action selection count\n",
    "        # Update the action-value estimate using incremental mean formula\n",
    "        self.q_estimations[action] += (reward - self.q_estimations[action]) / self.selections[action]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the internal state of the strategy for a new experiment.\n",
    "        \"\"\"\n",
    "        super().reset()  # Call parent class reset method if needed\n",
    "        self.q_estimations = np.zeros(self.k) + self.initial  # Reset action-value estimates\n",
    "        self.selections = np.zeros(self.k)  # Reset action selection counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089671d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "def plot_and_save(strategy, step,save_path = \"bandit_training.png\"):\n",
    "    \"\"\"\n",
    "    Creates and updates the figure, then saves it to disk.\n",
    "\n",
    "    Parameters:\n",
    "    - strategy (EpsilonGreedy): The current strategy with reward history.\n",
    "    - step (int): Current training step.\n",
    "    \"\"\"\n",
    "    K = strategy.k\n",
    "\n",
    "    # Create new figure inside the function\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    # Prepare data\n",
    "    reward_data = []\n",
    "    action_labels = []\n",
    "\n",
    "    for action, reward_list in strategy.rewards_history.items():\n",
    "        reward_data.extend(reward_list)\n",
    "        action_labels.extend([action+1] * len(reward_list))\n",
    "\n",
    "    # Create violin plot\n",
    "    sns.violinplot(x=action_labels, y=reward_data, inner=None, density_norm=\"width\", color=\"lightcoral\", linewidth=1.5, ax=ax)\n",
    "\n",
    "    # Scatter plot for estimated means\n",
    "    ax.scatter(range(K), strategy.q_estimations, color='red', s=50, zorder=3, label=\"Estimated Means\")\n",
    "\n",
    "    ax.axhline(0, linestyle='dotted', color='black', linewidth=1)\n",
    "    ax.set_xlabel(\"Actions\")\n",
    "    ax.set_ylabel(\"Estimated Reward\")\n",
    "    ax.set_title(f\"Training Step: {step} - Estimated Reward Distributions\")\n",
    "    ax.legend([\"Estimated Means\"])\n",
    "\n",
    "    # Save the figure to the same file (overwrite each time)\n",
    "    fig.savefig(save_path, format=\"png\", dpi=150)\n",
    "    plt.close(fig)  # Close figure to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55938e",
   "metadata": {},
   "source": [
    "Visualize, how the $\\varepsilon$-greedy Action Selection updates it's estimations of the reward distributions of the K-Armed bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadba576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bandit and strategy\n",
    "K = 10  # Number of arms\n",
    "bandit = KArmedBandit(K)\n",
    "strategy = EpsilonGreedy(k=K, epsilon=0.2)\n",
    "\n",
    "# Training loop with live plot updates\n",
    "n_steps = 5000\n",
    "plot_interval = n_steps // 50\n",
    "\n",
    "# Plot initial bandit distributions before training\n",
    "plot_bandit_distributions(bandit)\n",
    "\n",
    "for step in range(n_steps+1):\n",
    "    action = strategy.act()\n",
    "    reward = bandit.get_reward(action)\n",
    "    strategy.update(action, reward)\n",
    "\n",
    "    # Update plot every 'plot_interval' steps\n",
    "    if step % plot_interval == 0:\n",
    "        plot_and_save(strategy, step)\n",
    "        time.sleep(0.07)  # Small delay for better visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e4fcb",
   "metadata": {},
   "source": [
    "Test of $\\varepsilon$-greedy Action Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bee753",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # Number of actions\n",
    "\n",
    "# List of strategies to test\n",
    "strategies = [EpsilonGreedy(K), EpsilonGreedy(K, epsilon=0.1), EpsilonGreedy(K, epsilon=0.01)]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=2000)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a7400-b006-4b32-a3b4-dff353377a61",
   "metadata": {},
   "source": [
    "## Upper-Confidence-Bound (UCB) Action Selection\n",
    "\n",
    "The UCB action selection method offers a way to select an action by taking both the estimated value, as well as the accuracy of those estimates into account.\n",
    "It uses the following formula:\n",
    "\n",
    "$$ A_t := \\underset{a}{\\arg\\max} \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln(t)}{N_t(a)}} \\right] $$\n",
    "\n",
    "Where $Q_t(a)$ denotes the value of action $a$ (calculated using the *sample-averages* method), $N_t(a)$ denotes the number of times that action $a$ has\n",
    "been selected prior to time $t$, and the number $c > 0$ controls\n",
    "the degree of exploration. If $N_t(a) = 0$, then $a$ is considered to be a maximizing action.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement the UCB action selection method! You should complete all methods of the class below, and\n",
    "you can add new class methods and constructor arguments as needed.\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- You can utilize the same incremental average calulation as with the $\\varepsilon$-greedy method.\n",
    "- Compare UCB to $\\varepsilon$-greedy by adding an instance of this class to the `strategies` list below.\n",
    "- Try experimenting with different values of $c$!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f904719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class UCB(Strategy):\n",
    "    \"\"\"\n",
    "    Implements the Upper Confidence Bound (UCB) action selection strategy.\n",
    "    This method balances exploration and exploitation by selecting actions \n",
    "    with the highest estimated value plus an uncertainty bonus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, c=1, initial=0):\n",
    "        \"\"\"\n",
    "        Initializes the UCB strategy.\n",
    "\n",
    "        Parameters:\n",
    "        - k (int): Number of actions (arms).\n",
    "        - c (float): Exploration degree (higher c encourages more exploration).\n",
    "        - initial (float): Initial estimate for action values.\n",
    "        \"\"\"\n",
    "        super().__init__(k)  # Initialize parent Strategy class\n",
    "        self.c = c  # Degree of exploration (controls uncertainty bonus)\n",
    "        self.initial = initial  # Initial action-value estimates\n",
    "        self.q_estimations = np.zeros(self.k) + self.initial  # Estimated action values\n",
    "        self.selections = np.zeros(self.k)  # Number of times each action was selected\n",
    "        self.t = 0  # Time step counter\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns a formatted string representing the UCB strategy name.\n",
    "        \"\"\"\n",
    "        return f\"UCB, $c = {self.c}$\"\n",
    "\n",
    "    def act(self):\n",
    "        \"\"\"\n",
    "        Selects an action using the UCB formula.\n",
    "\n",
    "        Returns:\n",
    "        - (int) Index of the selected action.\n",
    "        \"\"\"\n",
    "        # Calculate the UCB upper confidence bound for each action:\n",
    "        # UCB = Q(a) + c * sqrt( log(t) / N(a) )\n",
    "        # - Q(a): Estimated value of action a\n",
    "        # - c: Degree of exploration (hyperparameter)\n",
    "        # - t: Current time step\n",
    "        # - N(a): Number of times action a has been selected\n",
    "\n",
    "        UCB_estimations = self.q_estimations + self.c * np.sqrt(\n",
    "            np.log(self.t + 1) / (self.selections + 1e-5)\n",
    "        )  # Small epsilon to avoid division by zero\n",
    "\n",
    "        return np.argmax(UCB_estimations)  # Select the action with highest UCB value\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Updates action value estimates using sample averaging.\n",
    "\n",
    "        Parameters:\n",
    "        - action (int): Index of the chosen action.\n",
    "        - reward (float): Reward received from the environment.\n",
    "        \"\"\"\n",
    "        super().update(action, reward)\n",
    "        self.t += 1  # Increment time step\n",
    "        self.selections[action] += 1  # Increment selection count for the action\n",
    "        \n",
    "        # Update the estimated action value using incremental mean:\n",
    "        # Q(a) = Q(a) + (1/N(a)) * (R - Q(a))\n",
    "        self.q_estimations[action] += (reward - self.q_estimations[action]) / self.selections[action]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the internal state of the UCB strategy for a new experiment.\n",
    "        \"\"\"\n",
    "        super().reset()\n",
    "        self.t = 0  # Reset time step counter\n",
    "        self.q_estimations = np.zeros(self.k) + self.initial  # Reset action-value estimates\n",
    "        self.selections = np.zeros(self.k)  # Reset action selection counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab366d3",
   "metadata": {},
   "source": [
    "Test of UCB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bfc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # Number of actions\n",
    "\n",
    "# List of strategies to test\n",
    "strategies = [UCB(K)]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=200)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c4cb1-b701-4a1e-840f-10ab0f3ccee1",
   "metadata": {},
   "source": [
    "## Gradient Bandit Algorithms\n",
    "\n",
    "Instead of estimating action values, this method learns a numerical *preference*, denoted $H_t(a)$ for each action. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Action probabilites are determined using the *soft-max* function:\n",
    "\n",
    "$$ \\pi_t(a) := \\Pr\\{A_t = a\\} := \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} $$\n",
    "\n",
    "Here we have also introduced a useful new notation, $\\pi_t(a)$, for the probability of\n",
    "taking action $a$ at time $t$. Note that this function defines a probability distribution over the set of all actions. On each step, after selecting action $A_t$ and receiving the reward $R_t$, the\n",
    "action preferences are updated by:\n",
    "\n",
    "$$ H_{t+1}(a) := H_t(a) + \\alpha(R_t - \\bar{R}_t)(\\mathbb{1}_{a=A_t} - \\pi_t(a)) $$\n",
    "\n",
    "Where $\\alpha > 0$ is a step-size parameter, and $\\bar{R}_t \\in \\mathbb{R}$ is the average of all the rewards up\n",
    "through and including time $t$. \n",
    "The $\\bar{R}_t$ term serves as a\n",
    "baseline with which the reward is compared.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement the Gradient action selection method! You should complete all methods of the class below, and\n",
    "you can add new class methods and constructor arguments as needed.\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- You can incrementally calculate the baseline value similarly to the action-value averages for the methods above.\n",
    "- The action preferences can be updated with a single line using an array which contains 1 at index $A_t$ and 0 elsewhere.\n",
    "- Instantiate this class in the list below as well and see how the three methods compare!\n",
    "- Try experimenting with different values of $\\alpha$!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Gradient(Strategy):\n",
    "    \"\"\"\n",
    "    Implements the Gradient Bandit Algorithm.\n",
    "    This method uses policy gradient updates to optimize action selection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Gradient Bandit strategy.\n",
    "\n",
    "        Parameters:\n",
    "        - k (int): Number of actions (arms).\n",
    "        - alpha (float): Step-size parameter for updating action preferences.\n",
    "        \"\"\"\n",
    "        super().__init__(k)  # Initialize parent Strategy class\n",
    "        self.alpha = alpha  # Step-size parameter (learning rate)\n",
    "        self.preferences = np.zeros(self.k)  # Action preference values (H(a))\n",
    "        self.indices = np.arange(self.k)  # List of possible action indices\n",
    "        self.baseline = 0  # Average reward (used as a baseline)\n",
    "        self.t = 0  # Time step counter\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns a formatted string representing the strategy name.\n",
    "        \"\"\"\n",
    "        return f\"Gradient, $\\\\alpha = {self.alpha}$\"\n",
    "\n",
    "    def act(self):\n",
    "        \"\"\"\n",
    "        Selects an action based on the softmax probabilities.\n",
    "\n",
    "        Returns:\n",
    "        - (int) Index of the selected action.\n",
    "        \"\"\"\n",
    "        # Compute softmax probabilities for all actions\n",
    "        action_probabilities = self.softmax(self.preferences)\n",
    "        \n",
    "        # Select an action according to the softmax distribution\n",
    "        return np.random.choice(self.indices, p=action_probabilities)\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Updates action preferences using policy gradient.\n",
    "\n",
    "        Parameters:\n",
    "        - action (int): Index of the chosen action.\n",
    "        - reward (float): Reward received from the environment.\n",
    "        \"\"\"\n",
    "        super().update(action, reward)  # Call parent class update method if needed\n",
    "        self.t += 1  # Increment time step\n",
    "\n",
    "        # Update the baseline reward using an incremental mean\n",
    "        self.baseline += (reward - self.baseline) / self.t\n",
    "\n",
    "        # Create a one-hot vector indicating the chosen action\n",
    "        one_hot = np.zeros(self.k)\n",
    "        one_hot[action] = 1\n",
    "\n",
    "        # Update action preferences using gradient ascent\n",
    "        # H(a) = H(a) + α * (R - baseline) * (I(a) - π(a))\n",
    "        self.preferences += self.alpha * (reward - self.baseline) * (one_hot - self.softmax(self.preferences))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the internal state of the Gradient Bandit strategy.\n",
    "        \"\"\"\n",
    "        super().reset()\n",
    "        self.t = 0  # Reset time step counter\n",
    "        self.baseline = 0  # Reset baseline reward estimate\n",
    "        self.preferences = np.zeros(self.k)  # Reset action preferences\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax probability distribution.\n",
    "\n",
    "        Parameters:\n",
    "        - x (np.array): Input preferences.\n",
    "\n",
    "        Returns:\n",
    "        - (np.array): Probability distribution over actions.\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))  # Numerical stability trick\n",
    "        return exp_x / np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29255ed2",
   "metadata": {},
   "source": [
    "Test of Gradient Bandit Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da57d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # Number of actions\n",
    "\n",
    "# List of strategies to test\n",
    "strategies = [Gradient(K,alpha=0.1)]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=200)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f32e2",
   "metadata": {},
   "source": [
    "## Comprehensive test\n",
    "\n",
    "In this final section let's run a longer comprehensive test with more actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53774a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5  # Number of actions\n",
    "\n",
    "# List of strategies to test\n",
    "strategies = [\n",
    "        EpsilonGreedy(K),\n",
    "        EpsilonGreedy(K, epsilon=0.1),\n",
    "        UCB(K, c=2),\n",
    "        Gradient(K)\n",
    "    ]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate(strategies, K=K, runs=2000, n_steps=500)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22172201",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Extend the $\\varepsilon$-greedy Action Selection Strategy to and Associative Bandit Problem. The Associative Bandit Problem consists  of multiple K-armed bandit problem. For each step the ABP have a given state (e.g a color: Red, Blue, Yellow). Each state have a different stationary distribution of rewards, which the strategy should handle. The strategy gets the index of the state of the ABP in each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246809db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssociativeKArmedBandit:\n",
    "    def __init__(self, K, number_of_possible_states,mean=0, std_dev=1):\n",
    "        \"\"\"\n",
    "        Initializes the K-armed bandit with normally distributed action values.\n",
    "        \n",
    "        Parameters:\n",
    "        - K (int): Number of arms.\n",
    "        - mean (float): Mean of the normal distribution for optimal action values.\n",
    "        - std_dev (float): Standard deviation for the optimal action values.\n",
    "        \"\"\"\n",
    "        self.number_of_possible_states = number_of_possible_states\n",
    "        self.bandits = [KArmedBandit(K, mean, std_dev) for _ in range(number_of_possible_states)]\n",
    "\n",
    "    def get_reward(self, state,action):\n",
    "        \"\"\"\n",
    "        Returns a stochastic reward from a normal distribution centered at the true action value.\n",
    "        \n",
    "        Parameters:\n",
    "        - action (int): The index of the chosen action.\n",
    "\n",
    "        Returns:\n",
    "        - reward (float): The observed reward for the selected action.\n",
    "        \"\"\"\n",
    "        self.bandits[state].get_reward(action)\n",
    "        # Reduce spread\n",
    "\n",
    "    def get_optimal_action(self,state):\n",
    "        \"\"\"\n",
    "        Returns the index of the optimal action (the arm with the highest expected reward).\n",
    "        \"\"\"\n",
    "        return self.bandits[state].get_optimal_action()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the bandit by re-generating the optimal action values.\n",
    "        \"\"\"\n",
    "        for bandit in self.bandits:\n",
    "            bandit.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_associative(strategies, K, number_of_states, runs=2000, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Simulates the associative K-armed bandit problem with different strategies.\n",
    "\n",
    "    Parameters:\n",
    "    - strategies (list): A list of strategies to evaluate.\n",
    "    - K (int): Number of arms.\n",
    "    - number_of_states (int): Number of possible states.\n",
    "    - runs (int): Number of independent runs.\n",
    "    - n_steps (int): Number of time steps per run.\n",
    "\n",
    "    Returns:\n",
    "    - mean_rewards (np.array): Average rewards per time step for each strategy.\n",
    "    - mean_best_action_choices (np.array): Average probability of selecting the best action.\n",
    "    \"\"\"\n",
    "    rewards = np.zeros((len(strategies), runs, n_steps))\n",
    "    best_action_choices = np.zeros(rewards.shape)\n",
    "\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        print(f\"Evaluating strategy {i + 1}/{len(strategies)}...\")\n",
    "\n",
    "        for r in range(runs):\n",
    "            # Initialize an associative K-armed bandit instance\n",
    "            bandit = AssociativeKArmedBandit(K, number_of_states)\n",
    "            bandit.reset()\n",
    "            \n",
    "            strategy.reset()  # Reset strategy state\n",
    "\n",
    "            for t in range(n_steps):\n",
    "                state = np.random.randint(0, number_of_states)  # Randomly select a state\n",
    "                action = strategy.act(state)  # Strategy selects an action based on the state\n",
    "                reward = bandit.get_reward(state, action)  # Get the reward for the selected action in the given state\n",
    "                \n",
    "                rewards[i, r, t] = reward\n",
    "                strategy.update(state, action, reward)  # Update strategy based on state\n",
    "\n",
    "                best_action = bandit.get_optimal_action(state)  # Get the best action for the current state\n",
    "                if action == best_action:\n",
    "                    best_action_choices[i, r, t] = 1  # Track if the best action was chosen\n",
    "\n",
    "\n",
    "    # Compute mean rewards and best action selection frequency\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "    mean_best_action_choices = best_action_choices.mean(axis=1)\n",
    "\n",
    "    return mean_rewards, mean_best_action_choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66818fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssociativeEpsilonGreedy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        ############## CODE HERE ###################\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        \n",
    "\n",
    "    def act(self,state):\n",
    "        ############## CODE HERE ###################\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "    \n",
    "    def update(self,state, action, reward):\n",
    "        ############## CODE HERE ###################\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "    \n",
    "    def reset(self):\n",
    "        ############## CODE HERE ###################\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1dd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10  # Number of actions\n",
    "number_of_states = 5  # Number of possible states\n",
    "# List of strategies to test\n",
    "strategies = [AssociativeEpsilonGreedy()]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate_associative(strategies, K=K, number_of_states=number_of_states,runs=2000, n_steps=1000)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07ebbb9edfac425baf9a2dbb7d34c67a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "08766d7e7ba9483e889112dc5d026a10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e5ecb38705794f79a3a1e5e6a308e4eb",
       "style": "IPY_MODEL_9687d120372c482cb74b71c7826d0cc6",
       "value": "100%"
      }
     },
     "0af20d17977d4ef3bc8b423bb4f994dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "14742ccc27e140a894ca7581a8837a8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1a4843195585449ca0cf41a8532ce173": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1e0e04f407834c56a3da5d5e777b2285": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1e3d3c8b492a42de9eed9aec4d252529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2b9c79b37c5242d3a9da596fbec0ea22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "317dc691dcac4d9681d5a0c2713f1db1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4805e0f1568c4584a5fa7db25627e6a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0af20d17977d4ef3bc8b423bb4f994dd",
       "max": 2000,
       "style": "IPY_MODEL_1a4843195585449ca0cf41a8532ce173",
       "value": 2000
      }
     },
     "548e34805d784045ab2d6b810edc9ebf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_590dd8bd235343ca98c7715a0ded63fb",
       "max": 2000,
       "style": "IPY_MODEL_dcb0315e8f8e496ea3d3e80a0f6c6d4f",
       "value": 2000
      }
     },
     "590dd8bd235343ca98c7715a0ded63fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5ecda2fcb2684cef9d35f7062dd2af53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ab2a3b1a4944bdfbf95ba0dbe11cd12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b8afc7c172f410d937031881e6bf8cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6d27ff26fef344a6882007ad47030213": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75513d022da1452a952aff7f074f9964": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "755413d3284f4720a61398776719798e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75c9a46c4e3b4a429e09a4a05ddec642": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7b11c9fb1e2f4cc9b1fa70edd23a3301": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c78ffad4cdc042f3afe5efc727eabb31",
       "style": "IPY_MODEL_2b9c79b37c5242d3a9da596fbec0ea22",
       "value": "100%"
      }
     },
     "7ccb1685b35e419e9d43abfe152bd306": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8952aaa7bb334b1ab795d51432c1fe9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7ccb1685b35e419e9d43abfe152bd306",
       "style": "IPY_MODEL_e8d730fe69314cd78fddf187d4662416",
       "value": " 2000/2000 [01:04&lt;00:00, 31.99it/s]"
      }
     },
     "8db846b5eb7043999faa9bbb9cfa82b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9687d120372c482cb74b71c7826d0cc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a7760f48fe6c4720a61c8cf8573c2634": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a7e8cca8ac1646cfa2f2202f07f0f135": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_08766d7e7ba9483e889112dc5d026a10",
        "IPY_MODEL_c2762225efb04bc0af5daf619bd9989f",
        "IPY_MODEL_8952aaa7bb334b1ab795d51432c1fe9a"
       ],
       "layout": "IPY_MODEL_b9ad02240b1b4daca607fb7de9765cb7"
      }
     },
     "ae082241625f42b49f7d0ea17aabf42e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_6b8afc7c172f410d937031881e6bf8cf",
       "max": 2000,
       "style": "IPY_MODEL_1e3d3c8b492a42de9eed9aec4d252529",
       "value": 2000
      }
     },
     "b12033f10b0c41bd91a85f257d33034a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b491177b51504242b2150455e40391e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5ecda2fcb2684cef9d35f7062dd2af53",
       "style": "IPY_MODEL_07ebbb9edfac425baf9a2dbb7d34c67a",
       "value": " 2000/2000 [01:50&lt;00:00, 18.25it/s]"
      }
     },
     "b7eacbdc3f1049d282a40f13db58a1db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c855a5a97f354c72ada6fc3d6907fd85",
        "IPY_MODEL_548e34805d784045ab2d6b810edc9ebf",
        "IPY_MODEL_cdf8f62184534aa39a6fa121d64ec6c0"
       ],
       "layout": "IPY_MODEL_b12033f10b0c41bd91a85f257d33034a"
      }
     },
     "b9ad02240b1b4daca607fb7de9765cb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c2762225efb04bc0af5daf619bd9989f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_dddb73e1746e4faa8caa1f1c092e7094",
       "max": 2000,
       "style": "IPY_MODEL_8db846b5eb7043999faa9bbb9cfa82b8",
       "value": 2000
      }
     },
     "c78ffad4cdc042f3afe5efc727eabb31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c855a5a97f354c72ada6fc3d6907fd85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_317dc691dcac4d9681d5a0c2713f1db1",
       "style": "IPY_MODEL_75513d022da1452a952aff7f074f9964",
       "value": "100%"
      }
     },
     "cdf8f62184534aa39a6fa121d64ec6c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_755413d3284f4720a61398776719798e",
       "style": "IPY_MODEL_6ab2a3b1a4944bdfbf95ba0dbe11cd12",
       "value": " 2000/2000 [01:06&lt;00:00, 28.54it/s]"
      }
     },
     "d85d7d1fcfd14f778606850604811de6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7b11c9fb1e2f4cc9b1fa70edd23a3301",
        "IPY_MODEL_4805e0f1568c4584a5fa7db25627e6a6",
        "IPY_MODEL_dd6de22286e741abb1d66494a2b2192f"
       ],
       "layout": "IPY_MODEL_6d27ff26fef344a6882007ad47030213"
      }
     },
     "dcb0315e8f8e496ea3d3e80a0f6c6d4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dd6de22286e741abb1d66494a2b2192f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1e0e04f407834c56a3da5d5e777b2285",
       "style": "IPY_MODEL_14742ccc27e140a894ca7581a8837a8e",
       "value": " 2000/2000 [01:33&lt;00:00, 22.06it/s]"
      }
     },
     "dddb73e1746e4faa8caa1f1c092e7094": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e5ecb38705794f79a3a1e5e6a308e4eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e8d730fe69314cd78fddf187d4662416": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eaa0458b4e3a4ff2b49022e1620b53c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fb4e5dc2536e4f8eb9c70fd4cc3b311b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a7760f48fe6c4720a61c8cf8573c2634",
       "style": "IPY_MODEL_75c9a46c4e3b4a429e09a4a05ddec642",
       "value": "100%"
      }
     },
     "fc1e38ec9e2d42b29633452a708016af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fb4e5dc2536e4f8eb9c70fd4cc3b311b",
        "IPY_MODEL_ae082241625f42b49f7d0ea17aabf42e",
        "IPY_MODEL_b491177b51504242b2150455e40391e8"
       ],
       "layout": "IPY_MODEL_eaa0458b4e3a4ff2b49022e1620b53c4"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
