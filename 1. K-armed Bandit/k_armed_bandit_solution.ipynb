{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71323ea5-f77e-42cd-8b5e-c1ff89e2c6b2",
   "metadata": {},
   "source": [
    "![Logo](../assets/logo.png)\n",
    "\n",
    "Made by **Domonkos Nagy**\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/Fortuz/rl_education/blob/main/1.%20K-armed%20Bandit/k_armed_bandit_solution.ipynb)\n",
    "\n",
    "# $k$-armed Bandit (solution)\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among\n",
    "$k$ different options, or actions. After each choice, you receive a numerical reward\n",
    "from a stationary probability distribution that depends on the action you selected. Your\n",
    "objective is to maximize the expected total reward over some time period, for example,\n",
    "over 1000 action selections, or time steps.\n",
    "\n",
    "This is called the $k$-armed bandit problem. You can visualize this problem as having to\n",
    "choose between $k$ slot machines (also known as one-armed bandits) at each time step,\n",
    "each of which has a different probability distribution for rewards - that is where the name comes from.\n",
    "\n",
    "<img src=\"assets/k_armed_bandit.png\" width=\"500\"/>\n",
    "\n",
    "The $k$-armed bandit problem illustrates an important problem in reinforcement\n",
    "learning: **exploration vs. exploitation**. At each time step $t$, the agent has to make a decision:\n",
    "take the action with the highest expected reward according to its current knowledge of the environment, \n",
    "or choose a different action to get a better estimation of the value of that action. The former is called an\n",
    "*exploitation* step, because it exploits the current knowledge of the agent in order to obtain a high reward.\n",
    "The latter is called an *exploration* step, since it involves trying out an action in order to have a better\n",
    "estimation of its value, thereby exploring the environment.\n",
    "\n",
    "This notebook introduces a few common strategies to tackle this problem and puts them to the test by simulating\n",
    "multiple test runs, and comparing the results.\n",
    "\n",
    "- This notebook is based on Chapter 2 of the book *Reinforcement Learning: An Introduction (2nd ed.)* by R. Sutton & A. Barto, available at http://incompleteideas.net/book/the-book-2nd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06156ef-5366-4790-af7c-a5b9d31278c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184eca01-435a-4c1e-a58d-4374e4b75123",
   "metadata": {},
   "source": [
    "## Strategy setup\n",
    "\n",
    "The `Strategy` base class is used to implement startegies for action selection. An action is selected by the `act` method, and then the `update` method is used\n",
    "to update the inner state after receiving a reward for the selected action. After an episode (a \"run\" consisting of $n$ steps, 1000 for example) is over, the `reset` method is called to reset the inner state of the class. The `name` propery is used get a name for the strategy in a visual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f1ff81-334f-4c0b-bf3d-eade83e9c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "class Strategy(ABC):\n",
    "\n",
    "    def __init__(self, k):\n",
    "        self.k = k  # Number of actions\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def act(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self, action, reward):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0528443-6a12-4109-b9cd-a42fc724919e",
   "metadata": {},
   "source": [
    "Numpy's `np.argmax` will choose the smallest index in case there are multiple\n",
    "maximal values. This function breaks these ties randomly instead, which is\n",
    "desirable in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b48f7a0-8634-4980-8503-1d1567b5033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# Argmax function that breaks ties randomly\n",
    "def argmax(arr):\n",
    "    arr_max = np.max(arr)\n",
    "    return np.random.choice(np.where(arr == arr_max)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c7d13",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We test our strategies by trying them out in multiple runs, and then averaging out the received reward at each time step. After that, we plot the results to\n",
    "compare the strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffca1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "# Action with some probability distribution\n",
    "class Action:\n",
    "    def __init__(self, distribution_prob=[1, 0, 0]):\n",
    "        self.mean = np.random.randn()\n",
    "        # By default, the distribution is Normal\n",
    "        self.distribution = np.random.choice([\"normal\", \"constant\", \"uniform\"], p=distribution_prob)\n",
    "\n",
    "    def get_reward(self):\n",
    "        if self.distribution == \"normal\":\n",
    "            return np.random.randn() + self.mean\n",
    "        elif self.distribution == \"uniform\":\n",
    "            return np.random.rand() + self.mean\n",
    "        else:\n",
    "            return self.mean\n",
    "\n",
    "\n",
    "# Simulating the k-armed bandit problem\n",
    "def simulate(strategies, K, runs=2000, n_steps=1000):\n",
    "    rewards = np.zeros((len(strategies), runs, n_steps))\n",
    "    best_action_choices = np.zeros(rewards.shape)\n",
    "\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        print(f\"Evaluating strategy {i + 1}/{len(strategies)}...\")\n",
    "\n",
    "        for r in trange(runs):\n",
    "            # Setting up actions with random probabilty distributions\n",
    "            actions = [Action() for _ in range(K)]\n",
    "            best_action = np.argmax([action.mean for action in actions])\n",
    "\n",
    "            strategy.reset()\n",
    "\n",
    "            for t in range(n_steps):\n",
    "                action = strategy.act()\n",
    "\n",
    "                reward = actions[action].get_reward()\n",
    "                rewards[i, r, t] = reward\n",
    "                strategy.update(action, reward)\n",
    "\n",
    "                if action == best_action:\n",
    "                    best_action_choices[i, r, t] = 1\n",
    "                    \n",
    "            time.sleep(.00001)  # For tqdm\n",
    "            \n",
    "    # Average of rewards received over all runs\n",
    "    mean_rewards = rewards.mean(axis=1)\n",
    "    # Average of number of best action choices over all runs\n",
    "    mean_best_action_choices = best_action_choices.mean(axis=1)\n",
    "\n",
    "    return mean_rewards, mean_best_action_choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a337157",
   "metadata": {},
   "source": [
    "To examine the results a plot function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "def plotResults(strategies, rewards, best_action_choices):\n",
    "  fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "\n",
    "  for strategy, reward in zip(strategies, rewards):\n",
    "      ax1.plot(reward, label=f\"{strategy.name}\", zorder=2)\n",
    "  ax1.set_xlabel('Steps')\n",
    "  ax1.set_ylabel('Average reward')\n",
    "  ax1.grid(alpha=0.8, linestyle=':', zorder=0)\n",
    "  ax1.set_title('Average reward of strategies')\n",
    "  ax1.legend()\n",
    "\n",
    "  for strategy, choices in zip(strategies, best_action_choices):\n",
    "      ax2.plot(choices, label=f\"{strategy.name}\")\n",
    "  ax2.set_xlabel('Steps')\n",
    "  ax2.set_ylabel('% Optimal action')\n",
    "  ax2.grid(alpha=0.8, linestyle=':', zorder=0)\n",
    "  ax2.set_title('% Optimal action choices of strategies')\n",
    "  ax2.legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79096804-a20e-499d-abdc-4a187ab3ac01",
   "metadata": {},
   "source": [
    "## $\\varepsilon$-greedy Action Selection\n",
    "\n",
    "With this method, the agent will select a random action with an $\\varepsilon$ probability ($0 \\le \\varepsilon \\le 1$), and act greedily (select the best action according to its knowledge) otherwise. The action values are calculated using the *sample-averages* method: the value of an action is the average of all the rewards received after taking that action.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement the $\\varepsilon$-greedy action selection method! You should complete all methods of the class below, and\n",
    "you can add new class methods and constructor arguments as needed. Pseudocode for this algorithm is shown in the box below.\n",
    "\n",
    "<img src=\"assets/epsilon_greedy.png\" width=\"700\"/>\n",
    "\n",
    "*Pseudocode from page 32 of the Sutton & Barto book*\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- Use the argmax function defined above!\n",
    "- There is a simple way to incremetally calculate the average reward for a given action: $Q_{n+1} = Q_n + \\frac{1}{n}\\left[R_n-Q_n\\right]$ (See page 31 of the Sutton & Barto book for details)\n",
    "- To test your implementation, instantiate this class in the `strategies` list at the bottom of the notebook and run the simulation.\n",
    "- Try experimenting with different values of $\\varepsilon$!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy(Strategy):\n",
    "\n",
    "    def __init__(self, k, epsilon=0, initial=0):\n",
    "        super().__init__(k)\n",
    "        self.epsilon = epsilon  # Probability of exploration\n",
    "        self.initial = initial  # Initial action values\n",
    "        self.q_estimations = np.zeros(self.k) + self.initial  # Action value estimations\n",
    "        self.selections = np.zeros(self.k)  # Number of selection of each action\n",
    "        self.indices = np.arange(self.k)  # List of possible actions\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        name_str = \"\"\n",
    "        if (self.epsilon == 0):\n",
    "            name_str = \"Greedy\"\n",
    "        else:\n",
    "            name_str = f\"$\\\\varepsilon$-greedy, $\\\\varepsilon = {self.epsilon}$\"\n",
    "        if (self.initial != 0):\n",
    "            name_str += f\", init: {self.initial}\"\n",
    "        return name_str\n",
    "\n",
    "    def act(self):\n",
    "        ############## CODE HERE ###################\n",
    "        # Explore with probability epsilon\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.indices)\n",
    "        # Exploit otherwise\n",
    "        else:\n",
    "            return argmax(self.q_estimations)\n",
    "        ############################################\n",
    "\n",
    "    # Sample-averages method for calculating action values\n",
    "    def update(self, action, reward):\n",
    "        ############## CODE HERE ###################\n",
    "        self.selections[action] += 1\n",
    "        # An incremental way of calculating the average\n",
    "        # See pg. 31 of the Sutton-Barto book for more details\n",
    "        self.q_estimations[action] += (reward - self.q_estimations[action]) / self.selections[action]\n",
    "        ############################################\n",
    "\n",
    "    # Reset inner state\n",
    "    def reset(self):\n",
    "        self.q_estimations = np.zeros(self.k) + self.initial\n",
    "        self.selections = np.zeros(self.k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e4fcb",
   "metadata": {},
   "source": [
    "Test of $\\varepsilon$-greedy Action Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bee753",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # Number of actions\n",
    "\n",
    "# List of strategies to test\n",
    "strategies = [EpsilonGreedy(K)]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=200)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a7400-b006-4b32-a3b4-dff353377a61",
   "metadata": {},
   "source": [
    "## Upper-Confidence-Bound (UCB) Action Selection\n",
    "\n",
    "The UCB action selection method offers a way to select an action by taking both the estimated value, as well as the accuracy of those estimates into account.\n",
    "It uses the following formula:\n",
    "\n",
    "$$ A_t := \\underset{a}{\\arg\\max} \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln(t)}{N_t(a)}} \\right] $$\n",
    "\n",
    "Where $Q_t(a)$ denotes the value of action $a$ (calculated using the *sample-averages* method), $N_t(a)$ denotes the number of times that action $a$ has\n",
    "been selected prior to time $t$, and the number $c > 0$ controls\n",
    "the degree of exploration. If $N_t(a) = 0$, then $a$ is considered to be a maximizing action.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement the UCB action selection method! You should complete all methods of the class below, and\n",
    "you can add new class methods and constructor arguments as needed.\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- You can utilize the same incremental average calulation as with the $\\varepsilon$-greedy method.\n",
    "- Compare UCB to $\\varepsilon$-greedy by adding an instance of this class to the `strategies` list below.\n",
    "- Try experimenting with different values of $c$!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f904719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB(Strategy):\n",
    "\n",
    "    def __init__(self, k, c=1, initial=0):\n",
    "        super().__init__(k)\n",
    "        self.c = c  # Degree of exploration\n",
    "        self.initial = initial  # Initial action values\n",
    "        self.q_estimations = np.zeros(self.k) + self.initial  # Action value estimations\n",
    "        self.selections = np.zeros(self.k)  # Number of selections of each action\n",
    "        self.t = 0  # Time step\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f\"UCB, $c = {self.c}$\"\n",
    "\n",
    "    def act(self):\n",
    "        ############## CODE HERE ###################\n",
    "        # Calculate UCB estimations, take argmax\n",
    "        UCB_estimations = self.q_estimations + self.c * np.sqrt(np.log(self.t + 1) / (self.selections + 1e-5))\n",
    "        return argmax(UCB_estimations)\n",
    "        ############################################\n",
    "\n",
    "    # Sample-averages method for calculating action values\n",
    "    def update(self, action, reward):\n",
    "        ############## CODE HERE ###################\n",
    "        self.t += 1\n",
    "        self.selections[action] += 1\n",
    "        # An incremental way of calculating the average\n",
    "        # See pg. 31 of the Sutton-Barto book for more details\n",
    "        self.q_estimations[action] += (reward - self.q_estimations[action]) / self.selections[action]\n",
    "        ############################################\n",
    "\n",
    "    # Reset inner state\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.q_estimations = np.zeros(self.k) + self.initial\n",
    "        self.selections = np.zeros(self.k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab366d3",
   "metadata": {},
   "source": [
    "Test of UCB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bfc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # Number of actions\n",
    "\n",
    "# List of strategies to test\n",
    "strategies = [UCB(K)]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=200)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c4cb1-b701-4a1e-840f-10ab0f3ccee1",
   "metadata": {},
   "source": [
    "## Gradient Bandit Algorithms\n",
    "\n",
    "Instead of estimating action values, this method learns a numerical *preference*, denoted $H_t(a)$ for each action. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Action probabilites are determined using the *soft-max* function:\n",
    "\n",
    "$$ \\pi_t(a) := \\Pr\\{A_t = a\\} := \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} $$\n",
    "\n",
    "Here we have also introduced a useful new notation, $\\pi_t(a)$, for the probability of\n",
    "taking action $a$ at time $t$. Note that this function defines a probability distribution over the set of all actions. On each step, after selecting action $A_t$ and receiving the reward $R_t$, the\n",
    "action preferences are updated by:\n",
    "\n",
    "$$ H_{t+1}(a) := H_t(a) + \\alpha(R_t - \\bar{R}_t)(\\mathbb{1}_{a=A_t} - \\pi_t(a)) $$\n",
    "\n",
    "Where $\\alpha > 0$ is a step-size parameter, and $\\bar{R}_t \\in \\mathbb{R}$ is the average of all the rewards up\n",
    "through and including time $t$. \n",
    "The $\\bar{R}_t$ term serves as a\n",
    "baseline with which the reward is compared.\n",
    "\n",
    "***\n",
    "\n",
    "### **Your Task**\n",
    "\n",
    "Implement the Gradient action selection method! You should complete all methods of the class below, and\n",
    "you can add new class methods and constructor arguments as needed.\n",
    "\n",
    "#### **Hints:**\n",
    "\n",
    "- You can incrementally calculate the baseline value similarly to the action-value averages for the methods above.\n",
    "- The action preferences can be updated with a single line using an array which contains 1 at index $A_t$ and 0 elsewhere.\n",
    "- Instantiate this class in the list below as well and see how the three methods compare!\n",
    "- Try experimenting with different values of $\\alpha$!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient(Strategy):\n",
    "\n",
    "    def __init__(self, k, alpha=0.1):\n",
    "        super().__init__(k)\n",
    "        self.alpha = alpha  # Step-size parameter\n",
    "        self.preferences = np.zeros(self.k)  # Action preferences\n",
    "        self.indices = np.arange(self.k)  # List of possible actions\n",
    "        self.baseline = 0  # Average reward\n",
    "        self.t = 0  # Time step\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return f\"Gradient, $\\\\alpha = {self.alpha}$\"\n",
    "\n",
    "    def act(self):\n",
    "        ############## CODE HERE ###################\n",
    "        # Select action from the softmax distribution\n",
    "        return np.random.choice(self.indices, p=self.softmax(self.preferences))\n",
    "        ############################################\n",
    "\n",
    "    def update(self, action, reward):\n",
    "        ############## CODE HERE ###################\n",
    "        self.t += 1\n",
    "        # Similar incremental calculation as the strategies above\n",
    "        self.baseline += (reward - self.baseline) / self.t\n",
    "        one_hot = np.zeros(self.k)\n",
    "        one_hot[action] = 1\n",
    "        self.preferences += self.alpha * (reward - self.baseline) * (one_hot - self.softmax(self.preferences))\n",
    "        ############################################\n",
    "\n",
    "    # Reset inner state\n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "        self.baseline = 0\n",
    "        self.preferences = np.zeros(self.k)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29255ed2",
   "metadata": {},
   "source": [
    "Test of Gradient Bandit Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da57d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # Number of actions\n",
    "\n",
    "# List of strategies to test\n",
    "strategies = [EpsilonGreedy(K)]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=200)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f32e2",
   "metadata": {},
   "source": [
    "## Comprehensive test\n",
    "\n",
    "In this final section let's run a longer comprehensive test with more actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53774a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5  # Number of actions\n",
    "\n",
    "# List of strategies to test\n",
    "strategies = [\n",
    "        EpsilonGreedy(K),\n",
    "        EpsilonGreedy(K, epsilon=0.1),\n",
    "        UCB(K, c=2),\n",
    "        Gradient(K)\n",
    "    ]\n",
    "\n",
    "# Evaluate strategies\n",
    "rewards, best_action_choices = simulate(strategies, K=K, runs=2000, n_steps=500)\n",
    "\n",
    "plotResults(strategies, rewards, best_action_choices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07ebbb9edfac425baf9a2dbb7d34c67a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "08766d7e7ba9483e889112dc5d026a10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e5ecb38705794f79a3a1e5e6a308e4eb",
       "style": "IPY_MODEL_9687d120372c482cb74b71c7826d0cc6",
       "value": "100%"
      }
     },
     "0af20d17977d4ef3bc8b423bb4f994dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "14742ccc27e140a894ca7581a8837a8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1a4843195585449ca0cf41a8532ce173": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1e0e04f407834c56a3da5d5e777b2285": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1e3d3c8b492a42de9eed9aec4d252529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2b9c79b37c5242d3a9da596fbec0ea22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "317dc691dcac4d9681d5a0c2713f1db1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4805e0f1568c4584a5fa7db25627e6a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_0af20d17977d4ef3bc8b423bb4f994dd",
       "max": 2000,
       "style": "IPY_MODEL_1a4843195585449ca0cf41a8532ce173",
       "value": 2000
      }
     },
     "548e34805d784045ab2d6b810edc9ebf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_590dd8bd235343ca98c7715a0ded63fb",
       "max": 2000,
       "style": "IPY_MODEL_dcb0315e8f8e496ea3d3e80a0f6c6d4f",
       "value": 2000
      }
     },
     "590dd8bd235343ca98c7715a0ded63fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5ecda2fcb2684cef9d35f7062dd2af53": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ab2a3b1a4944bdfbf95ba0dbe11cd12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b8afc7c172f410d937031881e6bf8cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6d27ff26fef344a6882007ad47030213": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75513d022da1452a952aff7f074f9964": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "755413d3284f4720a61398776719798e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75c9a46c4e3b4a429e09a4a05ddec642": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7b11c9fb1e2f4cc9b1fa70edd23a3301": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c78ffad4cdc042f3afe5efc727eabb31",
       "style": "IPY_MODEL_2b9c79b37c5242d3a9da596fbec0ea22",
       "value": "100%"
      }
     },
     "7ccb1685b35e419e9d43abfe152bd306": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8952aaa7bb334b1ab795d51432c1fe9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7ccb1685b35e419e9d43abfe152bd306",
       "style": "IPY_MODEL_e8d730fe69314cd78fddf187d4662416",
       "value": " 2000/2000 [01:04&lt;00:00, 31.99it/s]"
      }
     },
     "8db846b5eb7043999faa9bbb9cfa82b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9687d120372c482cb74b71c7826d0cc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a7760f48fe6c4720a61c8cf8573c2634": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a7e8cca8ac1646cfa2f2202f07f0f135": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_08766d7e7ba9483e889112dc5d026a10",
        "IPY_MODEL_c2762225efb04bc0af5daf619bd9989f",
        "IPY_MODEL_8952aaa7bb334b1ab795d51432c1fe9a"
       ],
       "layout": "IPY_MODEL_b9ad02240b1b4daca607fb7de9765cb7"
      }
     },
     "ae082241625f42b49f7d0ea17aabf42e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_6b8afc7c172f410d937031881e6bf8cf",
       "max": 2000,
       "style": "IPY_MODEL_1e3d3c8b492a42de9eed9aec4d252529",
       "value": 2000
      }
     },
     "b12033f10b0c41bd91a85f257d33034a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b491177b51504242b2150455e40391e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5ecda2fcb2684cef9d35f7062dd2af53",
       "style": "IPY_MODEL_07ebbb9edfac425baf9a2dbb7d34c67a",
       "value": " 2000/2000 [01:50&lt;00:00, 18.25it/s]"
      }
     },
     "b7eacbdc3f1049d282a40f13db58a1db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c855a5a97f354c72ada6fc3d6907fd85",
        "IPY_MODEL_548e34805d784045ab2d6b810edc9ebf",
        "IPY_MODEL_cdf8f62184534aa39a6fa121d64ec6c0"
       ],
       "layout": "IPY_MODEL_b12033f10b0c41bd91a85f257d33034a"
      }
     },
     "b9ad02240b1b4daca607fb7de9765cb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c2762225efb04bc0af5daf619bd9989f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_dddb73e1746e4faa8caa1f1c092e7094",
       "max": 2000,
       "style": "IPY_MODEL_8db846b5eb7043999faa9bbb9cfa82b8",
       "value": 2000
      }
     },
     "c78ffad4cdc042f3afe5efc727eabb31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c855a5a97f354c72ada6fc3d6907fd85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_317dc691dcac4d9681d5a0c2713f1db1",
       "style": "IPY_MODEL_75513d022da1452a952aff7f074f9964",
       "value": "100%"
      }
     },
     "cdf8f62184534aa39a6fa121d64ec6c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_755413d3284f4720a61398776719798e",
       "style": "IPY_MODEL_6ab2a3b1a4944bdfbf95ba0dbe11cd12",
       "value": " 2000/2000 [01:06&lt;00:00, 28.54it/s]"
      }
     },
     "d85d7d1fcfd14f778606850604811de6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7b11c9fb1e2f4cc9b1fa70edd23a3301",
        "IPY_MODEL_4805e0f1568c4584a5fa7db25627e6a6",
        "IPY_MODEL_dd6de22286e741abb1d66494a2b2192f"
       ],
       "layout": "IPY_MODEL_6d27ff26fef344a6882007ad47030213"
      }
     },
     "dcb0315e8f8e496ea3d3e80a0f6c6d4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "dd6de22286e741abb1d66494a2b2192f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1e0e04f407834c56a3da5d5e777b2285",
       "style": "IPY_MODEL_14742ccc27e140a894ca7581a8837a8e",
       "value": " 2000/2000 [01:33&lt;00:00, 22.06it/s]"
      }
     },
     "dddb73e1746e4faa8caa1f1c092e7094": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e5ecb38705794f79a3a1e5e6a308e4eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e8d730fe69314cd78fddf187d4662416": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "eaa0458b4e3a4ff2b49022e1620b53c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fb4e5dc2536e4f8eb9c70fd4cc3b311b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a7760f48fe6c4720a61c8cf8573c2634",
       "style": "IPY_MODEL_75c9a46c4e3b4a429e09a4a05ddec642",
       "value": "100%"
      }
     },
     "fc1e38ec9e2d42b29633452a708016af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fb4e5dc2536e4f8eb9c70fd4cc3b311b",
        "IPY_MODEL_ae082241625f42b49f7d0ea17aabf42e",
        "IPY_MODEL_b491177b51504242b2150455e40391e8"
       ],
       "layout": "IPY_MODEL_eaa0458b4e3a4ff2b49022e1620b53c4"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
