import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
import pickle
from tqdm.notebook import trange
from matplotlib.patches import Patch
from IPython import display
get_ipython().run_line_magic("config", " InlineBackend.print_figure_kwargs = {'pad_inches': .3}")


env = gym.make('Blackjack-v1', sab=True, render_mode='rgb_array')  # sab for Sutton & Barto's version


action_space_size = env.action_space.n
state_space_shape = tuple([dim.n for dim in env.observation_space])
q_table_shape = state_space_shape + (action_space_size, )

q_table = np.zeros(q_table_shape)
weights = np.zeros(q_table_shape)


# hyperparameters
N_EPISODES = 10_000_000
EPSILON = 1
EPSILON_DECAY = EPSILON / (N_EPISODES / 2)
EPSILON_MIN = 0.1
GAMMA = 1
LOG_RATE = N_EPISODES / 10


sum_rewards = 0

for episode in trange(N_EPISODES):
    state = env.reset()[0]

    done = False
    trajectory = []
    returns = 0
    weight = 1

    while not done:
        best_action = np.argmax(q_table[state])
        prob_choice = 1 - (EPSILON / 2)

        if np.random.rand() > EPSILON:
            action = best_action
        else:
            action = env.action_space.sample()
            if action != best_action:
                prob_choice = EPSILON / 2

        new_state, reward, done, truncated, info = env.step(action)
        trajectory.insert(0, (state, action, reward, prob_choice))

        sum_rewards += reward
        state = new_state

    for step in trajectory:
        state, action, reward, prob_choice = step

        # updating the returns
        returns = reward + GAMMA * returns
        ind = state + (action, )

        # updating the q-table
        weights[ind] += weight
        q_table[ind] += (weight / weights[ind]) * (returns - q_table[ind])

        # since the target policy is deterministic,
        # this means that the importance-sampling ratio 0
        if action != np.argmax(q_table[state]):
            break

        weight /= prob_choice

    EPSILON = max(EPSILON - EPSILON_DECAY, EPSILON_MIN)

    # logging the results
    if (episode + 1) % LOG_RATE == 0:
        print(f'Episode {episode + 1} : avg={sum_rewards / LOG_RATE}')
        sum_rewards = 0

# saving the q-table
with open('q_table.bin', 'wb') as f:
    pickle.dump(q_table, f)


# getting the value functions
state_value_no_usable_ace = np.max(q_table[12:22, 1:, 0, :], axis=-1)
state_value_with_usable_ace = np.max(q_table[12:22, 1:, 1, :], axis=-1)

x_tick_labels = ['A'] + list(range(2, 11))  # 'Ace' instead of 1

x, y = np.meshgrid(np.arange(state_value_no_usable_ace.shape[0]),
                   np.arange(state_value_no_usable_ace.shape[1]))

fig, axs = plt.subplots(1, 2, figsize=(12, 6), subplot_kw={'projection': '3d'})
ax1, ax2 = axs

# options for both plots
plt.setp(axs, xticks=np.arange(10),
         xticklabels=x_tick_labels,
         yticks=np.arange(10), yticklabels=np.arange(12, 22),
         xlabel='Dealer showing', ylabel='Player sum', zlabel='Value')

# 'no usable ace' plot
ax1.plot_surface(x, y, state_value_no_usable_ace, cmap='magma')
ax1.view_init(20, -40)
ax1.set_title('No usable ace', y=1)

# 'with usable ace' plot
ax2.plot_surface(x, y, state_value_with_usable_ace, cmap='magma')
ax2.view_init(20, -40)
ax2.set_title('With usable ace', y=1)

fig.suptitle('State-value function', fontsize=20, y=0.9)
plt.show()


# getting the policies
policy_no_usable_aces = np.argmax(q_table[12:22, 1:, 0, :], axis=-1)
policy_with_usable_aces = np.argmax(q_table[12:22, 1:, 1, :], axis=-1)

fig, axs = plt.subplots(1, 2, figsize=(10, 5))
ax1, ax2 = axs

# 'no usable ace' plot
sns.heatmap(np.flip(policy_no_usable_aces, axis=0), cmap='coolwarm',
            linecolor='white', linewidths=0.05, square=True,
            yticklabels=np.arange(21, 11, -1),
            xticklabels=x_tick_labels,
            cbar=False, ax=ax1)
ax1.set_title('No usable ace')

# 'with usable ace' plot
sns.heatmap(np.flip(policy_with_usable_aces, axis=0), cmap='coolwarm',
            linecolor='white', linewidths=0.05, square=True,
            yticklabels=np.arange(21, 11, -1),
            xticklabels=x_tick_labels,
            cbar=False, ax=ax2)
ax2.set_title('With usable ace')

# options for both plots
plt.setp(axs, xlabel='Dealer showing', ylabel='Player sum')

legend_elements = [
        Patch(facecolor="#3b4cc0", edgecolor="white", label="Stick"),
        Patch(facecolor="#b50526", edgecolor="white", label="Hit"),
    ]

fig.suptitle('Policy', fontsize=20)
plt.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))
plt.show()


env = gym.make('Blackjack-v1', sab=True, render_mode='rgb_array')

# loading the q-table
with open('q_table.bin', 'rb') as f:
    q_table = pickle.load(f)

plt.tick_params(left=False, right=False, labelleft=False,
                labelbottom=False, bottom=False)

env.reset()
img = plt.imshow(env.render())
for episode in range(3):
    plt.title(f"Episode #{episode+1}")

    state = env.reset()[0]

    img.set_data(env.render())
    display.display(plt.gcf())
    display.clear_output(wait=True)
    time.sleep(1)

    done = False

    while not done:
        action = np.argmax(q_table[state])  # greedy policy
        new_state, reward, done, truncated, info = env.step(action)

        img.set_data(env.render())
        display.display(plt.gcf())
        display.clear_output(wait=True)

        if done:
            if reward == 1:
                plt.title("You win!")
            elif reward == -1:
                plt.title("You lose!")
            else:
                plt.title("It's a draw!")

            display.display(plt.gcf())
            display.clear_output(wait=True)

        time.sleep(1)

        state = new_state

env.close()



