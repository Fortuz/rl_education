import numpy as np
import gymnasium as gym
import time
from tqdm.notebook import trange
from IPython import display
import matplotlib.pyplot as plt
import pickle


env = gym.make('FrozenLake-v1', render_mode='rgb_array')  # creating the environment


# initializing q-table
action_space_size = env.action_space.n
state_space_size = env.observation_space.n

q_table = np.zeros((state_space_size, action_space_size))
print("Q-TABLE:")
print(q_table)


# hyperparameters
N_EPISODES = 10_000
MAX_STEPS_PER_EPISODE = 100

ALPHA = 0.1  # learning rate
GAMMA = 0.99  # discount rate

EPSILON = 1  # exploration rate
EPSILON_MAX = 1
EPSILON_MIN = 0.001
EPSILON_DECAY = (2 * EPSILON) / N_EPISODES

LOG_RATE = N_EPISODES / 10


sum_rewards = 0

for episode in trange(N_EPISODES):
    state, _ = env.reset()

    done = False
    rewards_current_episode = 0

    for step in range(MAX_STEPS_PER_EPISODE):
        # epsilon-greedy action selection
        if np.random.rand() > EPSILON:
            action = np.argmax(q_table[state,:])
        else:
            action = env.action_space.sample()

        new_state, reward, done, truncated, info = env.step(action)

        # updating q-table
        q_table[state, action] = q_table[state, action] * (1 - ALPHA) + \
            ALPHA * (reward + GAMMA * np.max(q_table[new_state, :]))

        state = new_state
        sum_rewards += reward

        if done:
            break

    # updating epsilon
    EPSILON = max(EPSILON - EPSILON_DECAY, EPSILON_MIN)

    # logging the results
    if (episode + 1) % LOG_RATE == 0:
        print(f'Episode {episode + 1} : avg={sum_rewards / LOG_RATE}')
        sum_rewards = 0

# saving the q-table
with open('q_table.bin', 'wb') as f:
    pickle.dump(q_table, f)


# Print updated Q-table
print("Q-TABLE:")
print(q_table)


env = gym.make('FrozenLake-v1', render_mode='rgb_array')

# loading the q-table
with open('q_table.bin', 'rb') as f:
    q_table = pickle.load(f)

plt.tick_params(left=False, right=False, labelleft=False,
                labelbottom=False, bottom=False)

env.reset()
img = plt.imshow(env.render())

for episode in range(3):
    state, _ = env.reset()
    done = False

    plt.title(f"Episode #{episode + 1}")
    for step in range(MAX_STEPS_PER_EPISODE):
        action = np.argmax(q_table[state, :])  # greedy action selection

        new_state, reward, done, truncated, info = env.step(action)

        img.set_data(env.render())
        display.display(plt.gcf())
        display.clear_output(wait=True)

        if done:
            plt.title("You " + ("win!" if reward == 1 else "lose!"))

            display.display(plt.gcf())
            display.clear_output(wait=True)
            time.sleep(1)
            break

        state = new_state

env.close()



